{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from torch import nn\n",
    "import datasets\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "from itertools import chain\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "sys.path.append('..')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load/Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/cryptography/hazmat/backends/openssl/x509.py:14: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = 'gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Holder:\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq_len = 4096\n",
    "target_seq_len = 4096\n",
    "\n",
    "num_mem_tokens = 10\n",
    "input_size = 1024\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "args = Holder\n",
    "args.target_seq_len = target_seq_len\n",
    "args.input_seq_len = input_seq_len\n",
    "args.num_mem_tokens = num_mem_tokens\n",
    "args.input_size = input_size\n",
    "args.input_prefix = ''\n",
    "args.block_size = None\n",
    "args.task_name = 'wikitext-2-v1'\n",
    "\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/home/bulatov/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ab2469c629400aabaa1a6c4dafbcaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/bulatov/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-389b922bfc5fe729.arrow\n",
      "Loading cached processed dataset at /home/bulatov/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-6067a66e735cfbb1.arrow\n",
      "Loading cached processed dataset at /home/bulatov/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-941845a5470f2db7.arrow\n",
      "Loading cached processed dataset at /home/bulatov/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-c59c474634171e07.arrow\n",
      "Loading cached processed dataset at /home/bulatov/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-414595b9a286c906.arrow\n"
     ]
    }
   ],
   "source": [
    "\n",
    "raw_datasets = datasets.load_dataset('wikitext', args.task_name)\n",
    "column_names = raw_datasets[\"train\"].column_names\n",
    "text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[text_column_name])\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=column_names,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "block_size = args.input_size \n",
    "if args.num_mem_tokens is not None:\n",
    "    block_size -= 2 * args.num_mem_tokens\n",
    "history_size = args.input_seq_len - block_size\n",
    "\n",
    "def group_texts(examples, block_size, history_size=None):\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "\n",
    "    if history_size is None:\n",
    "        result = {\n",
    "            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "    else:\n",
    "        result = {\n",
    "            k: [t[max({0, i - history_size}) : i + block_size] for i in range(0, total_length, block_size)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "id_pad_value = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "def collate_fn(batch):\n",
    "    input_ids = [torch.tensor(b['input_ids'][::-1]) for b in batch]\n",
    "    labels = [torch.tensor(b['labels'][::-1]) for b in batch]\n",
    "    attention_mask = [torch.tensor(b['attention_mask'][::-1]) for b in batch]\n",
    "    input_ids = pad_sequence(input_ids, padding_value=id_pad_value).T.flip(1)\n",
    "    labels = pad_sequence(labels, padding_value=-100).T.flip(1)\n",
    "    attention_mask = pad_sequence(attention_mask, padding_value=0).T.flip(1)\n",
    "\n",
    "    collated = {'input_ids': input_ids,\n",
    "                'labels': labels, \n",
    "                'attention_mask': attention_mask}\n",
    "\n",
    "    if input_ids.shape[1] != block_size:\n",
    "        labels_mask = torch.ones_like(input_ids, dtype=bool)\n",
    "        labels_mask[:, :-block_size] = False\n",
    "        collated['labels_mask'] = labels_mask\n",
    "\n",
    "    return collated\n",
    "\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"].map(lambda x: group_texts(x, block_size, history_size), \n",
    "                                        batched=True, desc=f\"Grouping train in chunks of {block_size} and history {history_size}\")\n",
    "valid_dataset = tokenized_datasets[\"validation\"].map(lambda x: group_texts(x, block_size), \n",
    "                                        batched=True, desc=f\"Grouping valid in chunks of {block_size}\")\n",
    "\n",
    "\n",
    "# shuffle train data each epoch (one loop over train_dataset)\n",
    "# train_sampler = DistributedSampler(train_dataset, rank=hvd.rank(), num_replicas=hvd.size(), shuffle=True,\n",
    "#                                     drop_last=False, seed=args.seed)\n",
    "# per_worker_batch_size = args.batch_size * args.gradient_accumulation_steps\n",
    "# global_batch_size = per_worker_batch_size * hvd.size()\n",
    "\n",
    "# train_sampler = RandomSampler(train_dataset)\n",
    "kwargs = {'pin_memory': True}#, 'num_workers': args.data_n_workers}\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4016])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = iter(train_dataloader)\n",
    "batch = next(gen)\n",
    "batch = next(gen)\n",
    "batch.pop('labels_mask')\n",
    "batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4016])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = batch['input_ids']\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_size=1, output_size=21):\n",
    "        super().__init__()\n",
    "        self.weight = torch.nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.weight(X[:, :1].float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 21])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = Transformer()\n",
    "out = t(X[:, :512])\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryTransformer(nn.Module):\n",
    "   def __init__(self, model, num_mem_tokens):\n",
    "      super().__init__()\n",
    "      self.model = model\n",
    "      self.memory = torch.randn(num_mem_tokens)\n",
    "      self.num_mem_tokens = num_mem_tokens\n",
    "\n",
    "   def forward(self, X, memory=None):\n",
    "      bs = X.shape[0]\n",
    "      if memory is None:\n",
    "         memory = torch.stack([self.memory] * bs)\n",
    "      inp = torch.cat((memory, X, memory), dim=1)\n",
    "      full_out = self.model.forward(inp)\n",
    "      out = full_out[:, self.num_mem_tokens:-self.num_mem_tokens]\n",
    "      memory = full_out[:, -self.num_mem_tokens:]\n",
    "      return out, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = MemoryTransformer(t, num_mem_tokens=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1]), torch.Size([2, 10]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out, memory = mt(X[:, :492])\n",
    "out.shape, memory.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1]), torch.Size([2, 10]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out, memory = mt(X[:, :492], memory)\n",
    "out.shape, memory.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([2, 80]),\n",
       " torch.Size([2, 492]),\n",
       " torch.Size([2, 492]),\n",
       " torch.Size([2, 492]),\n",
       " torch.Size([2, 492]),\n",
       " torch.Size([2, 492]),\n",
       " torch.Size([2, 492]),\n",
       " torch.Size([2, 492]),\n",
       " torch.Size([2, 492])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in X_segments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentMemoryTransformer(MemoryTransformer):\n",
    "    def __init__(self, model, num_mem_tokens, segment_size):\n",
    "        super().__init__(model, num_mem_tokens)\n",
    "        self.segment_size = segment_size\n",
    "\n",
    "    def segment(self, X):\n",
    "        split_inds = (list(range(X.shape[1], 0, -self.segment_size)) + [0])[::-1]\n",
    "        segments = [X[:, start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "        return segments\n",
    "    \n",
    "    def forward(self, X):\n",
    "        segments = self.segment(X, segment_size)\n",
    "        outputs = []\n",
    "        \n",
    "        memory = None\n",
    "        for seg in segments:\n",
    "            out, memory = super().forward(seg, memory)\n",
    "            outputs.append(out)\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmt = RecurrentMemoryTransformer(t, num_mem_tokens=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmt_out = rmt(X_segments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rmt_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/rmt_demo_pretrain.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu11/home/bulatov/bulatov/RMT_light/framework/notebooks/rmt_demo_pretrain.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mtry\u001b[39;00m: \n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu11/home/bulatov/bulatov/RMT_light/framework/notebooks/rmt_demo_pretrain.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     out \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbatch)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu11/home/bulatov/bulatov/RMT_light/framework/notebooks/rmt_demo_pretrain.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu11/home/bulatov/bulatov/RMT_light/framework/notebooks/rmt_demo_pretrain.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mError: Input size too large!\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    out = model(**batch)\n",
    "except IndexError:\n",
    "    print('Error: Input size too large!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add RMT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling_rmt.language_modeling import RMTDecoderLMHeadMultiSeg as RMTWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmt_config = {'num_mem_tokens': 10, \n",
    "              'max_n_segments': 8,\n",
    "              'tokenizer': tokenizer,\n",
    "              'input_size': 1024, \n",
    "            }\n",
    "\n",
    "rmt = RMTWrapper(model, **rmt_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = rmt(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 10.841404914855957\n",
      "loss_0 4.384130001068115\n",
      "loss_1 10.640803337097168\n",
      "loss_2 11.2599458694458\n",
      "loss_3 11.477508544921875\n",
      "loss_4 10.841404914855957\n"
     ]
    }
   ],
   "source": [
    "for k in out:\n",
    "    if 'loss' in k:\n",
    "        print(k, out[k].item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Success! \n",
    "Let's teach the model to use memory."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hvdenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
