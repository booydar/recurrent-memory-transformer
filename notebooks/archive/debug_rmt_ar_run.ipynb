{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10_000_000\n",
    "x = list(range(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6425110"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "libgomp: Invalid value for environment variable OMP_NUM_THREADS\n",
      "/home/jovyan/envs/accel_rmt/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-04 12:53:47,908] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-04 12:53:49,086 - root - INFO - CUDA_VISIBLE_DEVICES: 0,1,2,3,4,5,6,7\n",
      "2023-09-04 12:53:49,088 - root - INFO - CUDA DEVICE COUNT: 8\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import math\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "\n",
    "# from dotenv import load_dotenv\n",
    "import torch\n",
    "import numpy as np\n",
    "import datasets\n",
    "import transformers\n",
    "from torch.utils.data import DataLoader\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "from lm_experiments_tools.trainer_accelerate import TrainerAccelerate as Trainer, TrainerAccelerateArgs as TrainerArgs\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import accelerate\n",
    "\n",
    "# load_dotenv()\n",
    "\n",
    "logger_fmt = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "logging.basicConfig(format=logger_fmt, level=logging.INFO)\n",
    "logger = logging.getLogger('')\n",
    "\n",
    "\n",
    "# if CUDA_VISIBLE_DEVICES is not set make all gpus visible\n",
    "if os.environ.get('CUDA_VISIBLE_DEVICES', None) is None:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = ','.join([str(i) for i in range(torch.cuda.device_count())])\n",
    "\n",
    "logger.info(f\"CUDA_VISIBLE_DEVICES: {os.environ['CUDA_VISIBLE_DEVICES']}\")\n",
    "# first call to torch.cuda.device_count() sets visible gpus, following calls will not change the result\n",
    "logger.info(f\"CUDA DEVICE COUNT: {torch.cuda.device_count()}\")\n",
    "\n",
    "# import transformers  # noqa: E402\n",
    "from transformers import AutoConfig, AutoTokenizer, HfArgumentParser  # noqa: E402\n",
    "\n",
    "from lm_experiments_tools.utils import get_cls_by_name, get_optimizer, prepare_run  # noqa: E402\n",
    "import lm_experiments_tools.optimizers as optimizers  # noqa: E402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test(torch.nn.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(model, torch.nn.Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_SYMBOLS = 16\n",
    "def generate_pairs(key_size, value_size, num_pairs):\n",
    "    keys = torch.randint(0, NUM_SYMBOLS, (num_pairs * 2, key_size))\n",
    "    keys[:, 0] = torch.randint(1, NUM_SYMBOLS, (num_pairs * 2, ))\n",
    "    \n",
    "    unique = keys.unique(dim=0)\n",
    "    delta_pairs = num_pairs - unique.shape[0]\n",
    "    if delta_pairs > 0:\n",
    "        print('got unique')\n",
    "        return generate_pairs(key_size, value_size, num_pairs)\n",
    "\n",
    "    selected_ids = torch.randperm(unique.shape[0])[:num_pairs]\n",
    "    keys = unique[selected_ids]\n",
    "\n",
    "    values = torch.randint(0, NUM_SYMBOLS, (num_pairs, value_size))\n",
    "    values[:, 0] = torch.randint(1, NUM_SYMBOLS, (num_pairs, ))\n",
    "    return keys, values\n",
    "\n",
    "\n",
    "class ARDataset:\n",
    "    def __init__(self, key_size, value_size, sample_len=1, num_samples=20_000):\n",
    "        self.sample_len = sample_len\n",
    "        keys, values = generate_pairs(key_size, value_size, sample_len * num_samples)\n",
    "\n",
    "        self.keys = keys.reshape(num_samples, -1)\n",
    "        self.values = values.reshape(num_samples, -1)\n",
    "        self.target_key_inds = torch.randint(sample_len, (num_samples, ))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        keys, values, tgt_ind = self.keys[idx], self.values[idx], self.target_key_inds[idx]\n",
    "        dim = 0 if keys.ndim == 1 else 1\n",
    "        keys = torch.chunk(keys, self.sample_len, dim=dim)\n",
    "        values = torch.chunk(values, self.sample_len, dim=dim)\n",
    "\n",
    "        sample = {'keys': keys, 'values': values, 'target_key_ind': tgt_ind}\n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.keys.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Holder:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            return super().__getattr__(name)\n",
    "        except AttributeError:\n",
    "            return None\n",
    "\n",
    "args = Holder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.from_pretrained = 'gpt2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### 1seg \n",
    "\n",
    "attrs = (\"--model_path\", \"/cephfs/home/bulatov/bulatov/RMT_light/runs/test/cnli_original\",\n",
    "\"--task_name\", \"exact_match\",\n",
    "\"--model_type\", \"decoder\",\n",
    "\"--memory_cell_cls\", \"modeling_rmt.experimental:MemoryCellGenerate\",\n",
    "\"--recurrent_wrapper_cls\", \"modeling_rmt.language_modeling:RecurrentWrapper\",\n",
    "\"--model_cls\", \"base_models.modeling_gpt_neox:GPTNeoXForCausalLM\", \n",
    "\"--model_cfg\",  \"/home/jovyan/rmt/wip/base_models/gptconfigs/neox_tiny.json\",\n",
    "\"--model_cpt\",  \"/home/jovyan/rmt/runs/associative_retrieval/lr1e-03_linear_adamw_wd1e-03_k4-v16-p1-1x2048_mem4_bs512_regular_bptt-1/run_4/\",\n",
    "\"--dataset_path\", \"/home/jovyan/rmt/datasets/associative_retrieval/\",\n",
    "\"--optimizer\", \"AdamW\",\n",
    "\"--weight_decay\", \"0.001\",\n",
    "\"--lr\", \"1e-03\", \n",
    "\"--lr_scheduler\", \"constant_with_warmup\",\n",
    "\"--clip_grad_value\", \"5.0\")\n",
    "\n",
    "int_attrs = (\n",
    "    # \"--input_seq_len\", \"512\",\n",
    "\"--input_size\", \"2048\",\n",
    "\"--key_size\", \"4\",\n",
    "\"--value_size\", \"16\",\n",
    "\"--num_pairs\", \"1\",\n",
    "\"--train_size\", \"10000\",\n",
    "\"--valid_size\", \"1000\",\n",
    "\"--test_size\", \"1000\",\n",
    "\"--segment_size\", \"44\",\n",
    "\"--num_mem_tokens\", \"4\",\n",
    "\"--max_n_segments\" ,\"1\", \n",
    "\"--batch_size\", \"2\", \n",
    "\"--gradient_accumulation_steps\", \"1\",\n",
    "# \"--use_generate_on_valid\",\n",
    "\"--iters\", \"100\",\n",
    "\"--num_warmup_steps\", \"100\",\n",
    "\"--data_n_workers\", \"2\",\n",
    "\"--log_interval\", \"10\",\n",
    "\"--show_valid_examples\", \"5\",\n",
    "\"--early_stopping_patience\", \"15\",\n",
    "\"--seed\", \"42\",\n",
    "\"--k2\", \"-1\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### 2seg \n",
    "\n",
    "# attrs = (\"--model_path\", \"/cephfs/home/bulatov/bulatov/RMT_light/runs/test/cnli_original\",\n",
    "# \"--task_name\", \"exact_match\",\n",
    "# \"--model_type\", \"decoder\",\n",
    "# \"--memory_cell_cls\", \"modeling_rmt.experimental:MemoryCellGenerate\",\n",
    "# \"--recurrent_wrapper_cls\", \"modeling_rmt.language_modeling:RecurrentWrapper\",\n",
    "# \"--model_cls\", \"base_models.modeling_gpt_neox:GPTNeoXForCausalLM\", \n",
    "# \"--model_cfg\",  \"/home/jovyan/rmt/wip/base_models/gptconfigs/neox_tiny.json\",\n",
    "# \"--model_cpt\",  \"/home/jovyan/rmt/runs/associative_retrieval/lr1e-03_linear_adamw_wd1e-03_k4-v8-p1-2x2048_mem4_bs512_regular_bptt-2_from_cpt/run_4/\",\n",
    "# \"--dataset_path\", \"/home/jovyan/rmt/datasets/associative_retrieval/\",\n",
    "# \"--optimizer\", \"AdamW\",\n",
    "# \"--weight_decay\", \"0.001\",\n",
    "# \"--lr\", \"1e-03\", \n",
    "# \"--lr_scheduler\", \"constant_with_warmup\",\n",
    "# \"--clip_grad_value\", \"5.0\")\n",
    "\n",
    "# int_attrs = (\n",
    "#     # \"--input_seq_len\", \"512\",\n",
    "# \"--input_size\", \"2048\",\n",
    "# \"--key_size\", \"4\",\n",
    "# \"--value_size\", \"8\",\n",
    "# \"--num_pairs\", \"1\",\n",
    "# \"--train_size\", \"10000\",\n",
    "# \"--valid_size\", \"1000\",\n",
    "# \"--test_size\", \"1000\",\n",
    "# \"--segment_size\", \"14\",\n",
    "# \"--num_mem_tokens\", \"4\",\n",
    "# \"--max_n_segments\" ,\"2\", \n",
    "# \"--batch_size\", \"2\", \n",
    "# \"--gradient_accumulation_steps\", \"1\",\n",
    "# # \"--use_generate_on_valid\",\n",
    "# \"--iters\", \"100\",\n",
    "# \"--num_warmup_steps\", \"100\",\n",
    "# \"--data_n_workers\", \"2\",\n",
    "# \"--log_interval\", \"10\",\n",
    "# \"--show_valid_examples\", \"5\",\n",
    "# \"--early_stopping_patience\", \"15\",\n",
    "# \"--seed\", \"42\",\n",
    "# \"--k2\", \"-1\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### 1-2seg \n",
    "\n",
    "# attrs = (\"--model_path\", \"/cephfs/home/bulatov/bulatov/RMT_light/runs/test/cnli_original\",\n",
    "# \"--task_name\", \"exact_match\",\n",
    "# \"--model_type\", \"decoder\",\n",
    "# \"--memory_cell_cls\", \"modeling_rmt.experimental:MemoryCellGenerate\",\n",
    "# \"--recurrent_wrapper_cls\", \"modeling_rmt.language_modeling:RecurrentWrapper\",\n",
    "# \"--model_cls\", \"base_models.modeling_gpt_neox:GPTNeoXForCausalLM\", \n",
    "# \"--model_cfg\",  \"/home/jovyan/rmt/wip/base_models/gptconfigs/neox_tiny.json\",\n",
    "# \"--model_cpt\",  \"/home/jovyan/rmt/runs/associative_retrieval/lr1e-03_linear_adamw_wd1e-03_k4-v16-p1-1x2048_mem4_bs512_regular_bptt-1/run_4/\",\n",
    "# \"--dataset_path\", \"/home/jovyan/rmt/datasets/associative_retrieval/\",\n",
    "# \"--optimizer\", \"AdamW\",\n",
    "# \"--weight_decay\", \"0.001\",\n",
    "# \"--lr\", \"1e-03\", \n",
    "# \"--lr_scheduler\", \"constant_with_warmup\",\n",
    "# \"--clip_grad_value\", \"5.0\")\n",
    "\n",
    "# int_attrs = (\n",
    "#     # \"--input_seq_len\", \"512\",\n",
    "# \"--input_size\", \"2048\",\n",
    "# \"--key_size\", \"4\",\n",
    "# \"--value_size\", \"16\",\n",
    "# \"--num_pairs\", \"1\",\n",
    "# \"--train_size\", \"10000\",\n",
    "# \"--valid_size\", \"1000\",\n",
    "# \"--test_size\", \"1000\",\n",
    "# \"--segment_size\", \"22\",\n",
    "# \"--num_mem_tokens\", \"4\",\n",
    "# \"--max_n_segments\" ,\"2\", \n",
    "# \"--batch_size\", \"2\", \n",
    "# \"--gradient_accumulation_steps\", \"1\",\n",
    "# # \"--use_generate_on_valid\",\n",
    "# \"--iters\", \"100\",\n",
    "# \"--num_warmup_steps\", \"100\",\n",
    "# \"--data_n_workers\", \"2\",\n",
    "# \"--log_interval\", \"10\",\n",
    "# \"--show_valid_examples\", \"5\",\n",
    "# \"--early_stopping_patience\", \"15\",\n",
    "# \"--seed\", \"42\",\n",
    "# \"--k2\", \"-1\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a, v in zip(int_attrs[::2], int_attrs[1::2]):\n",
    "    setattr(args, a.split('--')[1], int(v))\n",
    "\n",
    "for a, v in zip(attrs[::2], attrs[1::2]):\n",
    "    try:\n",
    "        setattr(args, a.split('--')[1], float(v))\n",
    "    except ValueError:\n",
    "        setattr(args, a.split('--')[1], v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = args.segment_size\n",
    "sep_token, gen_token, eos_token = 100, 101, 102\n",
    "\n",
    "def collate_fn(batch):\n",
    "    keys = [b['keys'] for b in batch]\n",
    "    values = [b['values'] for b in batch]\n",
    "    tgt_inds = [b['target_key_ind'].item() for b in batch]\n",
    "\n",
    "    bs = len(keys)\n",
    "    sep_tokens = torch.ones(bs, 1) * sep_token\n",
    "    eos_tokens = torch.ones(bs, 1) * eos_token\n",
    "    gen_tokens = torch.ones(bs, 1) * gen_token\n",
    "    sample = []\n",
    "    for i in range(args.num_pairs):\n",
    "        sample.append(torch.stack([k[i] for k in keys]))\n",
    "        sample.append(sep_tokens)\n",
    "        sample.append(torch.stack([v[i] for v in values]))\n",
    "        sample.append(eos_tokens)\n",
    "\n",
    "    target_keys = torch.stack([k[i] for i, k in zip(tgt_inds, keys)])\n",
    "    target_values = torch.stack([k[i] for i, k in zip(tgt_inds, values)])\n",
    "\n",
    "    sample.append(target_keys)\n",
    "    sample.append(gen_tokens)\n",
    "\n",
    "    input_ids_generate = torch.cat(sample, dim=1)\n",
    "\n",
    "    sample.append(target_values)\n",
    "    sample.append(eos_tokens)\n",
    "    input_ids = torch.cat(sample, dim=1)\n",
    "\n",
    "    labels_mask = torch.zeros_like(input_ids).bool()\n",
    "    labels_mask[:, -args.value_size - 2:] = True\n",
    "\n",
    "    collated = {'input_ids': input_ids.long(), \n",
    "                'input_ids_generate': input_ids_generate.long(), \n",
    "                'attention_mask': torch.ones_like(input_ids).bool(),\n",
    "                'attention_mask_generate': torch.ones_like(input_ids_generate).bool(),\n",
    "                'labels': input_ids.long(), \n",
    "                'labels_mask': labels_mask, \n",
    "                }\n",
    "    return collated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = f\"AR_k{args.key_size}_v{args.value_size}_p{args.num_pairs}\"\n",
    "path = os.path.join(args.dataset_path, dataset_name)\n",
    "\n",
    "# if os.path.exists(path):\n",
    "#     print(f\"Loading {dataset_name} from disk.\")\n",
    "#     train_dataset = torch.load(os.path.join(path, 'train'))\n",
    "#     valid_dataset = torch.load(os.path.join(path, 'valid'))\n",
    "#     test_dataset = torch.load(os.path.join(path, 'test'))\n",
    "# else:\n",
    "# os.system(f\"mkdir {path}\")\n",
    "train_dataset = ARDataset(args.key_size, args.value_size, sample_len=args.num_pairs, num_samples=args.train_size)\n",
    "valid_dataset = ARDataset(args.key_size, args.value_size, sample_len=args.num_pairs, num_samples=args.valid_size)\n",
    "test_dataset = ARDataset(args.key_size, args.value_size, sample_len=args.num_pairs, num_samples=args.test_size)\n",
    "\n",
    "# torch.save(train_dataset, os.path.join(path, 'train'))\n",
    "# torch.save(valid_dataset, os.path.join(path, 'valid'))\n",
    "# torch.save(test_dataset,  os.path.join(path, 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_worker_batch_size = 2 # args.batch_size * args.gradient_accumulation_steps\n",
    "kwargs = {'pin_memory': True, 'num_workers': args.data_n_workers}\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=per_worker_batch_size,\n",
    "                                collate_fn=collate_fn, **kwargs)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=per_worker_batch_size,\n",
    "                                collate_fn=collate_fn, **kwargs)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=per_worker_batch_size,\n",
    "                                collate_fn=collate_fn, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cls = get_cls_by_name(args.model_cls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-31 13:50:23,216 - root - INFO - Wrapping in: <class 'modeling_rmt.experimental.MemoryCellGenerate'> and <class 'modeling_rmt.language_modeling.RecurrentWrapper'>\n"
     ]
    }
   ],
   "source": [
    "if not args.from_pretrained:\n",
    "    model_cfg = AutoConfig.from_pretrained(args.model_cfg)\n",
    "    model = model_cls(config=model_cfg)\n",
    "else:\n",
    "    logger.info(f'Loading pretrained model: {args.from_pretrained}')\n",
    "    model = model_cls.from_pretrained(args.from_pretrained)\n",
    "\n",
    "# ## add [GEN] token\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "## load cpt of backbone model\n",
    "if args.backbone_cpt:\n",
    "    backbone_cpt = os.path.join(args.backbone_cpt, \"model_best.pth\")\n",
    "    cpt = torch.load(backbone_cpt, map_location='cpu')\n",
    "    model.load_state_dict(cpt['model_state_dict'], strict=False)\n",
    "    logger.info(f'Loaded baseline state dict from: {args.backbone_cpt}')\n",
    "\n",
    "# Pass memory settings to pretrained model\n",
    "if args.num_mem_tokens is not None:\n",
    "    memory_cell_cls = get_cls_by_name(args.memory_cell_cls)\n",
    "    recurrent_wrapper_cls = get_cls_by_name(args.recurrent_wrapper_cls)\n",
    "    logger.info(f'Wrapping in: {memory_cell_cls} and {recurrent_wrapper_cls}')\n",
    "    \n",
    "    \n",
    "    cell = memory_cell_cls(model, args.num_mem_tokens)\n",
    "    model = recurrent_wrapper_cls(cell, \n",
    "                                    segment_size=block_size,\n",
    "                                    max_n_segments=args.max_n_segments, \n",
    "                                #   vary_n_segments=args.vary_n_segments,\n",
    "                                    k2=args.k2,\n",
    "                                    segment_alignment=args.segment_alignment\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-31 13:50:23,763 - root - INFO - Loaded RMT state dict from: /home/jovyan/rmt/runs/associative_retrieval/lr1e-03_linear_adamw_wd1e-03_k4-v16-p1-1x2048_mem4_bs512_regular_bptt-1/run_4/\n"
     ]
    }
   ],
   "source": [
    "# if args.model_cpt:\n",
    "#     model_cpt = os.path.join(args.model_cpt, \"model_best.pth\")\n",
    "#     cpt = torch.load(model_cpt, map_location='cpu')\n",
    "#     model.load_state_dict(cpt['model_state_dict'], strict=False)\n",
    "#     logger.info(f'Loaded RMT state dict from: {args.model_cpt}')\n",
    "if args.model_cpt:\n",
    "    model_cpt = os.path.join(args.model_cpt, \"model_best/pytorch_model.bin\")\n",
    "    # model = torch.load(model_cpt, map_location='cpu')\n",
    "    cpt = torch.load(model_cpt, map_location='cpu')\n",
    "    # print(cpt.keys())\n",
    "    model.load_state_dict(cpt, strict=False)\n",
    "    logger.info(f'Loaded RMT state dict from: {args.model_cpt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen = iter(train_dataloader)\n",
    "# batch = next(gen)\n",
    "\n",
    "batch = collate_fn([train_dataset[i] for i in range(4)])\n",
    "# batch_valid = collate_valid([valid_dataset[i] for i in range(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_input = batch.pop('input_ids_generate')\n",
    "gen_attn_mask = batch.pop('attention_mask_generate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([101, 101, 101, 101])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'][:, -18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[101,  13,   4,   9,  11,  12,  12,  14,   8,  14,   0,  15,  13,   0,\n",
       "           1,  10,   3, 102],\n",
       "        [101,   4,  10,  14,   0,   5,  11,   0,   3,  10,   2,  14,  13,   9,\n",
       "          15,   9,   7, 102],\n",
       "        [101,  12,   8,   1,   8,   5,   7,   4,  13,   5,   0,   8,   8,   1,\n",
       "          10,  12,   6, 102],\n",
       "        [101,   6,   6,  12,  10,   5,   2,   8,  14,  10,   7,   8,  14,  10,\n",
       "           9,   8,  12, 102]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = torch.stack([ids[m] for ids, m in zip(batch['input_ids'], batch['labels_mask'])])\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['labels_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1793, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(**batch)\n",
    "out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -3,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0],\n",
       "        [  6,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0],\n",
       "        [-12,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0],\n",
       "        [  4,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.logits.argmax(dim=-1)[:, -18:-1] - labels[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[101,  13,   4,   9,  11,  12,  12,  14,   8,  14,   0,  15,  13,   0,\n",
       "           1,  10,   3, 102],\n",
       "        [101,   4,  10,  14,   0,   5,  11,   0,   3,  10,   2,  14,  13,   9,\n",
       "          15,   9,   7, 102],\n",
       "        [101,  12,   8,   1,   8,   5,   7,   4,  13,   5,   0,   8,   8,   1,\n",
       "          10,  12,   6, 102],\n",
       "        [101,   6,   6,  12,  10,   5,   2,   8,  14,  10,   7,   8,  14,  10,\n",
       "           9,   8,  12, 102]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented = model.segment(input_ids=batch['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 22])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segmented[0]['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:102 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "generated = model.memory_cell.model.generate(input_ids=gen_input, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 10,   8,   9,   4, 100,  13,   4,   9,  11,  12,  12,  14,   8,  14,\n",
       "           0,  15,  13,   0,   1,  10,   3, 102,  10,   8,   9,   4, 101,  10,\n",
       "           4,   9,  11,  12,  12,  14,   8,  14,   0,  15,  13,   0,   1,  10,\n",
       "           3, 102],\n",
       "        [ 15,  10,  11,  13, 100,   4,  10,  14,   0,   5,  11,   0,   3,  10,\n",
       "           2,  14,  13,   9,  15,   9,   7, 102,  15,  10,  11,  13, 101,  10,\n",
       "          10,  14,   0,   5,  11,   0,   3,  10,   2,  14,  13,   9,  15,   9,\n",
       "           7, 102],\n",
       "        [ 14,  12,   0,   7, 100,  12,   8,   1,   8,   5,   7,   4,  13,   5,\n",
       "           0,   8,   8,   1,  10,  12,   6, 102,  14,  12,   0,   7, 101,   0,\n",
       "           8,   1,   8,   5,   7,   4,  13,   5,   0,   8,   8,   1,  10,  12,\n",
       "           6, 102],\n",
       "        [  3,  11,  13,  13, 100,   6,   6,  12,  10,   5,   2,   8,  14,  10,\n",
       "           7,   8,  14,  10,   9,   8,  12, 102,   3,  11,  13,  13, 101,  10,\n",
       "           6,  12,  10,   5,   2,   8,  14,  10,   7,   8,  14,  10,   9,   8,\n",
       "          12, 102]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  3,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0, -6,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0, 12,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0, -4,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['labels'][:, -generated.shape[1]:] - generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gen with embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "self =model\n",
    "input_ids = gen_input\n",
    "attention_mask = None\n",
    "\n",
    "generate_kwargs = {'max_new_tokens': 100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:102 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "memory_state = None\n",
    "segmented = self.segment(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "for seg_num, segment in enumerate(segmented[:-1]):\n",
    "    cell_out, memory_state = self.memory_cell(**segment, memory_state=memory_state, output_hidden_states=True)\n",
    "\n",
    "final_segment = segmented[-1]\n",
    "out = self.memory_cell.generate(**final_segment, memory_state=memory_state, attention_mask=None, **generate_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[101,  14,  11,  14,  14,  11,  14,   6,  11,  14,   6,  11,  14,   6,\n",
       "          11,  14,  10,  10,  10,   6,  14,  11,  11,  14,  11,  14,  14,  11,\n",
       "          14,   6,  11,  14,   6,  11,  14,   6,  11,  14,  10,  10,  10,   6,\n",
       "          14,  11,  11,  14,  11,  14,  14,  11,  14,   6,  11,  14,   6,  11,\n",
       "          14,   6,  11,  14,  10,  10,  10,   6,  14,  11,  11,  14,  11,  14,\n",
       "          14,  11,  14,   6,  11,  14,   6,  11,  14,   6,  11,  14,  10,  10,\n",
       "          10,   6,  14,  11,  11,  14,  11,  14,  14,  11,  14,   6,  11,  14,\n",
       "           6,  11,  14],\n",
       "        [101,   1,   1,   1,   1,   1,   1,  11,   1,   1,  11,   1,   1,  11,\n",
       "           1,   1,   1,   1,   1,  11,   8,   1,   1,   1,   1,   1,   1,   1,\n",
       "           1,  11,   1,   1,  11,   1,   1,  11,   1,   1,   1,   1,   1,  11,\n",
       "           8,   1,   1,   1,   1,   1,   1,   1,   1,  11,   1,   1,  11,   1,\n",
       "           1,  11,   1,   1,   1,   1,   1,  11,   8,   1,   1,   1,   1,   1,\n",
       "           1,   1,   1,  11,   1,   1,  11,   1,   1,  11,   1,   1,   1,   1,\n",
       "           1,  11,   8,   1,   1,   1,   1,   1,   1,   1,   1,  11,   1,   1,\n",
       "          11,   1,   1],\n",
       "        [101,   3,   7,   3,   3,   7,   3,   3,   7,   3,   3,   7,   3,   3,\n",
       "           7,   3,   5,   5,   5,   3,   6,   7,   7,   3,   7,   3,   3,   7,\n",
       "           3,   3,   7,   3,   3,   7,   3,   3,   7,   3,   5,   5,   5,   3,\n",
       "           6,   7,   7,   3,   7,   3,   3,   7,   3,   3,   7,   3,   3,   7,\n",
       "           3,   3,   7,   3,   5,   5,   5,   3,   6,   7,   7,   3,   7,   3,\n",
       "           3,   7,   3,   3,   7,   3,   3,   7,   3,   3,   7,   3,   5,   5,\n",
       "           5,   3,   6,   7,   7,   3,   7,   3,   3,   7,   3,   3,   7,   3,\n",
       "           3,   7,   3],\n",
       "        [101,   3,  10,   3,   3,  10,   3,   9,   3,  10,   9,   3,  10,   9,\n",
       "           3,  10,   1,   1,   1,   9,   3,  10,  10,   3,  10,   3,   3,  10,\n",
       "           3,   9,   3,  10,   9,   3,  10,   9,   3,  10,   1,   1,   1,   9,\n",
       "           3,  10,  10,   3,  10,   3,   3,  10,   3,   9,   3,  10,   9,   3,\n",
       "          10,   9,   3,  10,   1,   1,   1,   9,   3,  10,  10,   3,  10,   3,\n",
       "           3,  10,   3,   9,   3,  10,   9,   3,  10,   9,   3,  10,   1,   1,\n",
       "           1,   9,   3,  10,  10,   3,  10,   3,   3,  10,   3,   9,   3,  10,\n",
       "           9,   3,  10]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.memory_cell.model.config.bos_token = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_embeds = model.memory_cell.model.get_input_embeddings()(gen_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 27])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 27, 128])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "generate() missing 2 required positional arguments: 'input_ids' and 'attention_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m generated \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds, max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: generate() missing 2 required positional arguments: 'input_ids' and 'attention_mask'"
     ]
    }
   ],
   "source": [
    "generated = model.memory_cell.model.generate(inputs_embeds=inputs_embeds, max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[101,  10,  14,   3,  12,  15,  13,   4,   0,   4,  10,  10,   6,   9,\n",
       "           4,  13,  14, 102],\n",
       "        [101,  12,   2,   1,   5,  11,   9,  12,  12,   0,  10,  12,   7,  12,\n",
       "           3,  12,   4, 102],\n",
       "        [101,  14,   5,  11,   3,   5,   8,   9,  13,  11,  10,   7,   5,  11,\n",
       "          13,   9,   6, 102],\n",
       "        [101,   4,   8,  12,   2,   0,   4,   9,  14,  12,  10,   6,   0,   3,\n",
       "          13,   9,   5, 102]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,  -9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0],\n",
       "        [  0, -13,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0],\n",
       "        [  0,  -2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['labels'][:, -generated.shape[1]:] - generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 10,   8,  11,   7, 100,   1,  14,   3,  12,  15,  13,   4,   0,   4,\n",
       "         10,  10,   6,   9,   4,  13,  14, 102,  10,   8,  11,   7, 101,   1,\n",
       "         14,   3,  12,  15,  13,   4,   0,   4,  10,  10,   6,   9,   4,  13,\n",
       "         14, 102])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['labels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = model.memory_cell.model.generate(inputs_embeds=inputs_embeds, max_new_tokens=17, pad_token_id=104, eos_token_id=102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 10,   2,   7,  11, 100,   2,   3,   4,  11,   3,   6,   5,   3,  13,\n",
       "         12,   0,   7,  13,   6,   3,  14, 102,  10,   2,   7,  11, 101])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 10,   2,   7,  11, 100,   2,   3,   4,  11,   3,   6,   5,   3,  13,\n",
       "         12,   0,   7,  13,   6,   3,  14, 102,  10,   2,   7,  11, 101,   2,\n",
       "          3,   4,  11,   3,   6,   5,   3,  13,  12,   0,   7,  13,   6,   3,\n",
       "         14, 102])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['labels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[101,   2,   3,   4,  11,   3,   6,   5,   3,  13,  12,   0,   7,  13,\n",
       "           6,   3,  14, 102],\n",
       "        [101,  12,   8,   2,   3,   9,   2,   5,  10,   0,  15,   4,   1,  15,\n",
       "           7,  15,  12, 102],\n",
       "        [101,  10,  15,  12,  13,  13,  13,   9,   6,   0,   9,  14,  11,   0,\n",
       "          12,  12,   6, 102],\n",
       "        [101,   7,   8,   7,   0,   0,   5,  11,   1,  14,  11,  13,  11,  14,\n",
       "           8,   8,   6, 102]])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:102 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[101,   3,   5,   3,   5,   5,   3,   5,   5,   3,   5,   3,   5,   3,\n",
       "           5,   5,   3,   5]])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.memory_cell.model.generate(max_new_tokens=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.stack([l[m] for l, m in zip(batch['labels'], batch['labels_mask'])])\n",
    "p = generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[101,  12,   7,   6,   1,  11,   8,  15,   0,   7,   4,  12,  13,   7,\n",
       "           8,   9,   8, 102],\n",
       "        [101,   1,   1,   0,  12,  11,   6,   9,   7,  14,  11,   3,  13,   4,\n",
       "           1,   9,  14, 102],\n",
       "        [101,   4,  13,   0,   1,   7,  14,   1,  11,   9,   2,   5,   3,   1,\n",
       "           7,  10,   1, 102],\n",
       "        [101,   8,   5,   9,   8,  12,   7,  12,  13,   8,  11,   1,  11,   5,\n",
       "           3,  12,   0, 102]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0, 11,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0, -3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[:, -y.shape[1]:]  - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 12,   8,   8,   9,   8,   5,  12,   7,   7,  14,  11,   1,  12,   0,\n",
       "          12,  14, 102],\n",
       "        [ 12,   6,  13,  13,   2,   3,   8,   6,   1,   3,   0,   7,   2,  14,\n",
       "           5,  15, 102],\n",
       "        [  9,   9,   2,   2,   9,  14,   3,   1,   0,   4,   0,   7,   8,  14,\n",
       "           6,   5, 102],\n",
       "        [ 12,   5,   6,  14,  12,   6,  15,   4,   2,  10,  13,  15,  14,   7,\n",
       "           1,   8, 102]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated[:, -y.shape[1]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  5,   8,   8,   9,   8,   5,  12,   7,   7,  14,  11,   1,  12,   0,\n",
       "          12,  14, 102],\n",
       "        [  8,   6,  13,  13,   2,   3,   8,   6,   1,   3,   0,   7,   2,  14,\n",
       "           5,  15, 102],\n",
       "        [  7,   9,   2,   2,   9,  14,   3,   1,   0,   4,   0,   7,   8,  14,\n",
       "           6,   5, 102],\n",
       "        [  7,   5,   6,  14,  12,   6,  15,   4,   2,  10,  13,  15,  14,   7,\n",
       "           1,   8, 102]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  5,   8,   8,   9,   8,   5,  12,   7,   7,  14,  11,   1,  12,   0,\n",
       "          12,  14, 102],\n",
       "        [  8,   6,  13,  13,   2,   3,   8,   6,   1,   3,   0,   7,   2,  14,\n",
       "           5,  15, 102],\n",
       "        [  7,   9,   2,   2,   9,  14,   3,   1,   0,   4,   0,   7,   8,  14,\n",
       "           6,   5, 102],\n",
       "        [  7,   5,   6,  14,  12,   6,  15,   4,   2,  10,  13,  15,  14,   7,\n",
       "           1,   8, 102]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  5,   8,   8,   9,   8,   5,  12,   7,   7,  14,  11,   1,  12,   0,\n",
       "          12,  14, 102],\n",
       "        [  8,   6,  13,  13,   2,   3,   8,   6,   1,   3,   0,   7,   2,  14,\n",
       "           5,  15, 102],\n",
       "        [  7,   9,   2,   2,   9,  14,   3,   1,   0,   4,   0,   7,   8,  14,\n",
       "           6,   5, 102],\n",
       "        [  7,   5,   6,  14,  12,   6,  15,   4,   2,  10,  13,  15,  14,   7,\n",
       "           1,   8, 102]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9411764705882353"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em  = np.mean([y_ == p_[-len(y_):] for p_, y_ in zip (p, y)])\n",
    "em\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_input.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  7,  13,   0,  10, 100,   5,   8,   8,   9,   8,   5,  12,   7,   7,\n",
       "          14,  11,   1,  12,   0,  12,  14, 102,   7,  13,   0,  10, 101,  12,\n",
       "           8,   8,   9,   8,   5,  12,   7,   7,  14,  11,   1,  12,   0,  12,\n",
       "          14, 102],\n",
       "        [ 14,   6,   5,   5, 100,   8,   6,  13,  13,   2,   3,   8,   6,   1,\n",
       "           3,   0,   7,   2,  14,   5,  15, 102,  14,   6,   5,   5, 101,  12,\n",
       "           6,  13,  13,   2,   3,   8,   6,   1,   3,   0,   7,   2,  14,   5,\n",
       "          15, 102],\n",
       "        [  9,  12,  10,   8, 100,   7,   9,   2,   2,   9,  14,   3,   1,   0,\n",
       "           4,   0,   7,   8,  14,   6,   5, 102,   9,  12,  10,   8, 101,   9,\n",
       "           9,   2,   2,   9,  14,   3,   1,   0,   4,   0,   7,   8,  14,   6,\n",
       "           5, 102],\n",
       "        [  1,   4,   3,  14, 100,   7,   5,   6,  14,  12,   6,  15,   4,   2,\n",
       "          10,  13,  15,  14,   7,   1,   8, 102,   1,   4,   3,  14, 101,  12,\n",
       "           5,   6,  14,  12,   6,  15,   4,   2,  10,  13,  15,  14,   7,   1,\n",
       "           8, 102]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  2,  15,  12,  10, 100,   4,   8,   6,   2,   3,   3,   2,   0,  15,\n",
       "          11,   3,  15,   6,  13,   0,  13, 102,   2,  15,  12,  10, 101,   9,\n",
       "          32, 100,  80,  80,  80,  80,  80,  80,  80,  80,  80,  73,  59,  70,\n",
       "          64,  70,  64,  70,  64,  70,  64,  68, 117,   9,  32, 120, 110,  41,\n",
       "         111, 126,  58,  71,  43,  71,  43,   8, 112, 103, 120, 110,  94, 126,\n",
       "          95,  97,   9,  32, 100,  43,   8],\n",
       "        [  4,   0,   5,   1, 100,  13,  10,  13,   2,   3,   3,  14,   7,  11,\n",
       "           8,   6,  15,  10,  12,   9,  11, 102,   4,   0,   5,   1, 101,   9,\n",
       "          71,  71,  71,  71,  71,  43, 100,  43, 100,  43, 100,  43, 100,  92,\n",
       "          80, 107,   1,  58,  71,  43, 100,  92,  80, 107,   1,  58,  71,  43,\n",
       "         100,  92,  80, 107,   1,  58,  71,  43, 100,  92,  80, 107,   1,  58,\n",
       "          71,  43, 100,  92,  80, 107,   1],\n",
       "        [  6,  11,  11,   1, 100,   5,  14,  14,   1,   5,   1,  13,  13,   5,\n",
       "           1,   2,  11,   8,   9,  10,   5, 102,   6,  11,  11,   1, 101,  41,\n",
       "          79,  44,  71,  40,  69, 118,  89,  40,  69, 118,  89,  40,  69, 118,\n",
       "          89,  40,  69, 118,  89,  40,  69, 118,  89,  40,  69, 118,  89,  40,\n",
       "          69, 118,  89,  40,  69, 118,  89,  40,  69, 118,  89,  40,  69, 118,\n",
       "          89,  40,  69, 118,  89,  40,  69],\n",
       "        [ 10,   5,  10,   1, 100,   5,  12,   0,  11,   4,   0,  15,   7,   3,\n",
       "           9,   8,  15,   6,  11,   2,   6, 102,  10,   5,  10,   1, 101,   9,\n",
       "          71,  71,  71,  71,  71,  71,  71,  71,  43, 100,  43, 100,  43, 100,\n",
       "          43, 100,  92,  80, 107,   1, 107,   1, 107,   1, 107,   1, 107,   1,\n",
       "         107,   1, 107,   1,  58,  71,  41, 100,  92,  41, 100,  92,  41, 100,\n",
       "          92,  41, 100,  92,  41, 100,  92]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Receiving Party may share some Confidential Information with some of Receiving Party's employees. Attachment 8\\nNon-Disclosure Agreement for Request for Proposal – California Courts Protective Order Registry Development\\nTHIS CONFIDENTIALITY AGREEMENT (“Agreement”), effective as of _________________[date of company’s signed proposal] (“Effective Date”), is entered into by and between the Administrative Office of the Courts (the “AOC”) and [company name], a [ ] corporation, with its principal address at [complete address], (hereinafter the “Company”).\\nWHEREAS, the State of California, acting through the Judicial Council of California, Administrative Office of the Courts is planning to disclose certain confidential information to potential vendors who wish to bid on the Request for Proposal for Judicial Council of California Administrative Office of the Courts (AOC) for California Courts Protective Order Registry Development, including all Statements of Work, dated on or about April, 2009 (collectively, the “RFP”);\\nWHEREAS, each potential vendor who wishes to submit a response to the RFP must have access to that confidential information in order to construct a response to the RFP, including a bid; and\\nWHEREAS, the AOC requires each potential vendor identified to receive the RFP to sign this Confidentiality Agreement and return it to the AOC as a condition of receiving the RFP;\\nNOW THEREFORE, in consideration of the promises and of the mutual promises and agreements herein contained, it is agreed by and between the parties hereto as follows:\\n1. Company agrees that all information, documents, data, materials and the AOC proprietary software systems disclosed to, or accessed by, Company in connection with, or related to, the RFP or the RFP process, in any form whether oral or written, or in any medium, including but not limited to any of the following is “Confidential Information” under the terms of this Agreement:\\na. The subject matter described in, and referred to, in the RFP, its associated Statements of Work (“SOWs”) or any other attachments, or during the RFP process (collectively “AOC Requirements”);\\nb. The RFP document (including all attachments), its content, and all supporting data, materials and all other information provided to Company in any form or medium in or[GEN]NotMentioned<|endoftext|><|endoftext|>\",\n",
       " 'Confidential Information may include verbally conveyed information. CONFIDENTIALITY (NON-DISCLOSURE) AGREEMENT\\nThis CONFIDENTIALITY (NON-DISCLOSURE) AGREEMENT (“Agreement”) is made and entered into this ______ day of _____________, 201__, by and between BROOKS’ BOTTLING COMPANY, LLC, a New York Limited Liability Company (“BROOKS”) and [ ] (“CLIENT”) (BROOKS and CLIENT are sometimes collectively referred to as the “Parties”)\\nRECITALS\\nA. BROOKS is engaged in the business of bottling products for various CLIENTS and has entered into discussion with relating to establishing a business relationship in connection with preparing and bottling certain of CLIENT’s product(s). As such, BROOKS will need access to certain personal, financial and product information relating to CLIENT and its products.\\nB. CLIENT desires to maintain the confidentiality of all personal, financial and product information disclosed to BROOKS. BROOKS is willing to receive all such personal, financial and product information in confidence, and the Parties deem it to be in their mutual best interest to protect such personal, financial and product information as provided in this Agreement.\\nNOW, THEREFORE, in consideration of the mutual promises, agreements, covenants, conditions and undertakings herein contained, and for other good and valuable consideration, the receipt and sufficiency of which are hereby acknowledged, the Parties agree as follows:\\n1. Confidential Information\\n1.1 With the understanding that CLIENT’s Formulations and Processing Procedures are proprietary to CLIENT, to the extent that CLIENT shall disclose to BROOKS personal, financial and product information, including their Formulations and Processing Procedures (“Confidential Information”), then all the Confidential Information disclosed to BROOKS shall be received by BROOKS in confidence for purposes of this Agreement except as otherwise provided under Section 3 below.\\n1.2 BROOKS, its directors, employees, agents and representatives shall not disclose, disseminate, publish, communicate or divulge any Confidential Information to anyone outside BROOKS, or to any employee of BROOKS not having reasonable need for access to such information, unless CLIENT expressly consents to such disclosure in writing.\\n2. Representations and Warranties\\n2.1 Each of[GEN]<|endoftext|>[GEN]NotMentioned<|endoftext|>']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Entailment<|endoftext|><|endoftext|>', 'NotMentioned<|endoftext|>']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = model\n",
    "input_ids = batch['input_ids']\n",
    "attention_mask = batch['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleAttributeError",
     "evalue": "'MemoryCell' object has no attribute 'generate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb#Y241sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m# self.manage_gradients(memory_state, seg_num)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb#Y241sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m final_segment \u001b[39m=\u001b[39m segmented[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb#Y241sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmemory_cell\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfinal_segment, memory_state\u001b[39m=\u001b[39mmemory_state)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/nn/modules/module.py:778\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m    777\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m--> 778\u001b[0m \u001b[39mraise\u001b[39;00m ModuleAttributeError(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    779\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mModuleAttributeError\u001b[0m: 'MemoryCell' object has no attribute 'generate'"
     ]
    }
   ],
   "source": [
    "memory_state = None\n",
    "segmented = self.segment(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "cell_outputs = []\n",
    "for seg_num, segment in enumerate(segmented[:-1]):\n",
    "    cell_out, memory_state = self.memory_cell(**segment, memory_state=memory_state, output_hidden_states=True)\n",
    "    cell_outputs.append(cell_out)\n",
    "    # self.manage_gradients(memory_state, seg_num)\n",
    "\n",
    "final_segment = segmented[-1]\n",
    "out = self.memory_cell.generate(**final_segment, memory_state=memory_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = dict(**final_segment)\n",
    "input_ids = kwargs.pop('input_ids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "If inputs_embeds is passed as model-specific keyword input then model has to be an encoder-decoder and not a GPT2LMHeadModel.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb#Y246sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m seg_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory_cell\u001b[39m.\u001b[39mprocess_input(input_ids, memory_state, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb#Y246sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmemory_cell\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mseg_kwargs)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:26\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     25\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m():\n\u001b[0;32m---> 26\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/transformers/generation_utils.py:1083\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1074\u001b[0m return_dict_in_generate \u001b[39m=\u001b[39m (\n\u001b[1;32m   1075\u001b[0m     return_dict_in_generate \u001b[39mif\u001b[39;00m return_dict_in_generate \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mreturn_dict_in_generate\n\u001b[1;32m   1076\u001b[0m )\n\u001b[1;32m   1078\u001b[0m \u001b[39m# 2. Define model inputs\u001b[39;00m\n\u001b[1;32m   1079\u001b[0m \u001b[39m# inputs_tensor has to be defined\u001b[39;00m\n\u001b[1;32m   1080\u001b[0m \u001b[39m# model_input_name is defined if model-specific keyword input is passed\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m \u001b[39m# otherwise model_input_name is None\u001b[39;00m\n\u001b[1;32m   1082\u001b[0m \u001b[39m# all model-specific keyword inputs are removed from `model_kwargs`\u001b[39;00m\n\u001b[0;32m-> 1083\u001b[0m inputs_tensor, model_input_name, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_model_inputs(inputs, bos_token_id, model_kwargs)\n\u001b[1;32m   1084\u001b[0m batch_size \u001b[39m=\u001b[39m inputs_tensor\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1086\u001b[0m \u001b[39m# 3. Define other model kwargs\u001b[39;00m\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/transformers/generation_utils.py:423\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_model_inputs\u001b[0;34m(self, inputs, bos_token_id, model_kwargs)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[39m# 4. Only encoder-decoder models can have non `input_ids` input format\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder \u001b[39mand\u001b[39;00m input_name \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 423\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    424\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIf \u001b[39m\u001b[39m{\u001b[39;00minput_name\u001b[39m}\u001b[39;00m\u001b[39m is passed as model-specific keyword \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    425\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minput then model has to be an encoder-decoder and not a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    426\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    427\u001b[0m     )\n\u001b[1;32m    429\u001b[0m \u001b[39m# 5. if `inputs` is still None, try to create `input_ids` from BOS token\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[39mif\u001b[39;00m inputs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: If inputs_embeds is passed as model-specific keyword input then model has to be an encoder-decoder and not a GPT2LMHeadModel."
     ]
    }
   ],
   "source": [
    "seg_kwargs = self.memory_cell.process_input(input_ids, memory_state, **kwargs)\n",
    "out = self.memory_cell.model.generate(**seg_kwargs)\n",
    "# out, new_memory_state = self.memory_cell.process_output(out, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 768])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_kwargs['inputs_embeds'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 768]' is invalid for input of size 32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb Cell 27\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb#Y245sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mmemory_cell\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(seg_kwargs[\u001b[39m'\u001b[39;49m\u001b[39minputs_embeds\u001b[39;49m\u001b[39m'\u001b[39;49m], max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:26\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     25\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m():\n\u001b[0;32m---> 26\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/transformers/generation_utils.py:1190\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1185\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1186\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnum_return_sequences has to be 1, but is \u001b[39m\u001b[39m{\u001b[39;00mnum_return_sequences\u001b[39m}\u001b[39;00m\u001b[39m when doing greedy search.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1187\u001b[0m         )\n\u001b[1;32m   1189\u001b[0m     \u001b[39m# 10. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgreedy_search(\n\u001b[1;32m   1191\u001b[0m         input_ids,\n\u001b[1;32m   1192\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1193\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1194\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mpad_token_id,\n\u001b[1;32m   1195\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49meos_token_id,\n\u001b[1;32m   1196\u001b[0m         output_scores\u001b[39m=\u001b[39;49moutput_scores,\n\u001b[1;32m   1197\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1198\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1199\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1200\u001b[0m     )\n\u001b[1;32m   1202\u001b[0m \u001b[39melif\u001b[39;00m is_sample_gen_mode:\n\u001b[1;32m   1203\u001b[0m     \u001b[39m# 10. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1204\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(\n\u001b[1;32m   1205\u001b[0m         top_k\u001b[39m=\u001b[39mtop_k, top_p\u001b[39m=\u001b[39mtop_p, typical_p\u001b[39m=\u001b[39mtypical_p, temperature\u001b[39m=\u001b[39mtemperature, num_beams\u001b[39m=\u001b[39mnum_beams\n\u001b[1;32m   1206\u001b[0m     )\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/transformers/generation_utils.py:1530\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1527\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 1530\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   1531\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   1532\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1533\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1534\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1535\u001b[0m )\n\u001b[1;32m   1537\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   1538\u001b[0m     cur_len \u001b[39m=\u001b[39m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:1047\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1044\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1045\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1047\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m   1048\u001b[0m     input_ids,\n\u001b[1;32m   1049\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1050\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1051\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1052\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1053\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1054\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1055\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1056\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   1057\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1058\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1059\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1060\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1061\u001b[0m )\n\u001b[1;32m   1062\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1064\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:784\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    782\u001b[0m     token_type_ids \u001b[39m=\u001b[39m token_type_ids\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, input_shape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m    783\u001b[0m \u001b[39mif\u001b[39;00m position_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 784\u001b[0m     position_ids \u001b[39m=\u001b[39m position_ids\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, input_shape[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m])\n\u001b[1;32m    786\u001b[0m \u001b[39mif\u001b[39;00m past_key_values \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    787\u001b[0m     past_length \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 768]' is invalid for input of size 32"
     ]
    }
   ],
   "source": [
    "model.memory_cell.model.generate(seg_kwargs['inputs_embeds'], max_new_tokens=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 5)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cell_outputs), len(segmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 508])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Receiving Party may share some Confidential Information with some of Receiving Party's employees. Attachment 8\\nNon-Disclosure Agreement for Request for Proposal – California Courts Protective Order Registry Development\\nTHIS CONFIDENTIALITY AGREEMENT (“Agreement”), effective as of _________________[date of company’s signed proposal] (“Effective Date”), is entered into by and between the Administrative Office of the Courts (the “AOC”) and [company name], a [ ] corporation, with its principal address at [complete address], (hereinafter the “Company”).\\nWHEREAS, the State of California, acting through the Judicial Council of California, Administrative Office of the Courts is planning to disclose certain confidential information to potential vendors who wish to bid on the Request for Proposal for Judicial Council of California Administrative Office of the Courts (AOC) for California Courts Protective Order Registry Development, including all Statements of Work, dated on or about April, 2009 (collectively, the “RFP”);\\nWHEREAS, each potential vendor who wishes to submit a response to the RFP must have access to that confidential information in order to construct a response to the RFP, including a bid; and\\nWHEREAS, the AOC requires each potential vendor identified to receive the RFP to sign this Confidentiality Agreement and return it to the AOC as a condition of receiving the RFP;\\nNOW THEREFORE, in consideration of the promises and of the mutual promises and agreements herein contained, it is agreed by and between the parties hereto as follows:\\n1. Company agrees that all information, documents, data, materials and the AOC proprietary software systems disclosed to, or accessed by, Company in connection with, or related to, the RFP or the RFP process, in any form whether oral or written, or in any medium, including but not limited to any of the following is “Confidential Information” under the terms of this Agreement:\\na. The subject matter described in, and referred to, in the RFP, its associated Statements of Work (“SOWs”) or any other attachments, or during the RFP process (collectively “AOC Requirements”);\\nb. The RFP document (including all attachments), its content, and all supporting data, materials and all other information provided to Company in any form or medium in or[GEN]\",\n",
       " 'Confidential Information may include verbally conveyed information. CONFIDENTIALITY (NON-DISCLOSURE) AGREEMENT\\nThis CONFIDENTIALITY (NON-DISCLOSURE) AGREEMENT (“Agreement”) is made and entered into this ______ day of _____________, 201__, by and between BROOKS’ BOTTLING COMPANY, LLC, a New York Limited Liability Company (“BROOKS”) and [ ] (“CLIENT”) (BROOKS and CLIENT are sometimes collectively referred to as the “Parties”)\\nRECITALS\\nA. BROOKS is engaged in the business of bottling products for various CLIENTS and has entered into discussion with relating to establishing a business relationship in connection with preparing and bottling certain of CLIENT’s product(s). As such, BROOKS will need access to certain personal, financial and product information relating to CLIENT and its products.\\nB. CLIENT desires to maintain the confidentiality of all personal, financial and product information disclosed to BROOKS. BROOKS is willing to receive all such personal, financial and product information in confidence, and the Parties deem it to be in their mutual best interest to protect such personal, financial and product information as provided in this Agreement.\\nNOW, THEREFORE, in consideration of the mutual promises, agreements, covenants, conditions and undertakings herein contained, and for other good and valuable consideration, the receipt and sufficiency of which are hereby acknowledged, the Parties agree as follows:\\n1. Confidential Information\\n1.1 With the understanding that CLIENT’s Formulations and Processing Procedures are proprietary to CLIENT, to the extent that CLIENT shall disclose to BROOKS personal, financial and product information, including their Formulations and Processing Procedures (“Confidential Information”), then all the Confidential Information disclosed to BROOKS shall be received by BROOKS in confidence for purposes of this Agreement except as otherwise provided under Section 3 below.\\n1.2 BROOKS, its directors, employees, agents and representatives shall not disclose, disseminate, publish, communicate or divulge any Confidential Information to anyone outside BROOKS, or to any employee of BROOKS not having reasonable need for access to such information, unless CLIENT expressly consents to such disclosure in writing.\\n2. Representations and Warranties\\n2.1 Each of[GEN]<|endoftext|>']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(batch['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  ...,  True,  True,  True],\n",
       "        [ True,  True,  True,  ...,  True,  True, False]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Entailment<|endoftext|><|endoftext|>', 'NotMentioned<|endoftext|>']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(batch['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpt = torch.load(\"../../runs/cnli_original/gpt2/lr5e-05_linear_adamw_wd1e-03_124-128-1x128_mem2_bs32_iters6000_regular_bptt-1_refactor/run_1/model_best.pth\", map_location='cpu')\n",
    "model.load_state_dict(cpt['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "#     out = model(**batch)\n",
    "#     out.loss.backward()\n",
    "#     optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([50257,  3673,    44,  1463,   276])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'][0][batch['labels_mask'][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3551, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = out.logits.argmax(dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Notailment<|endoftext|>', 'Notailment<|endoftext|>']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode([p[m] for p, m in zip(pred, batch['labels_mask'])], add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[GEN]Entailment', '[GEN]Entailment']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode([p[m] for p, m in zip(batch['labels'], batch['labels_mask'])], add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NotMentioned<|endoftext|>', 'Notailment<|endoftext|>']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer.batch_decode([p[m] for p, m in zip(pred, batch['labels_mask'])], add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NotMentioned<|endoftext|>', 'NotMentionedNot']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer.batch_decode([p[m] for p, m in zip(pred, batch['labels_mask'])], add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2207, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_inputs = [b[~m] for b, m in zip(batch['input_ids'], batch['labels_mask'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Receiving Party shall not disclose the fact that Agreement was agreed or negotiated. Johns Hopkins University\\nNON-DISCLOSURE AGREEMENT For Bilateral Disclosure\\nThis Agreement is effective this of in the year ______ is by and between JHU and COMPANY, each defined below.\\nJHU: The Johns Hopkins University\\nAddress: 100 N. Charles St., 5th Floor\\nBaltimore, Maryland 21201\\nJHU Contact:\\nCOMPANY:\\nAddress:\\nCOMPANY Contact:\\nWHEREAS, each party has certain technical information described below which shall hereinafter be referred to as \"CONFIDENTIAL INFORMATION\";\\nCONFIDENTIAL INFORMATION:\\nWHEREAS, JHU and COMPANY are each interested in examining the CONFIDENTIAL INFORMATION of the other solely for the PURPOSE, defined below;\\nPURPOSE: To explore licensing, collaborative or sponsored research agreement opportunities related to the CONFIDENTIAL INFORMATION.\\nNOW, THEREFORE, in consideration of the premises and mutual covenants contained herein, the parties hereto agree as follows:\\n”PROVIDER” shall mean the party hereto disclosing CONFIDENTIAL INFORMATION to the RECIPIENT party.\\n“RECIPIENT” shall mean the party receiving CONFIDENTIAL INFORMATION from the PROVIDER party.\\n 1. PROVIDER, through its employee, the PROVIDER Contact, shall disclose CONFIDENTIAL INFORMATION to RECIPIENT, through its employee, the RECIPIENT Contact, to enable RECIPIENT to fully evaluate such disclosure solely for the PURPOSE. CONFIDENTIAL INFORMATION shall be indicated as confidential at the time of disclosure.\\n2. RECIPIENT agrees to accept the CONFIDENTIAL INFORMATION and to employ all reasonable efforts to maintain the CONFIDENTIAL INFORMATION as secret and confidential, such efforts to be no less than the degree of care employed by RECIPIENT to preserve and safeguard RECIPIENT\\'s own confidential information. The CONFIDENTIAL INFORMATION shall not be disclosed or revealed to anyone except employees of RECIPIENT who have a need to know the CONFIDENTIAL INFORMATION for the PURPOSE and who agree to be bound by the terms of this Agreement.\\n3. It is hereby acknowledged by PROVIDER that RECIPIENT shall incur no liability merely for examining and considering the CONFIDENTIAL INFORMATION. However, RECIPIENT agrees that it will not use the CONFIDENTIAL INFORMATION for any purpose other<|endoftext|>'"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(gen_inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = tokenizer.batch_decode([l[m] for l, m in zip(batch['labels'], batch['labels_mask'])], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Entailment', 'Entailment']"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(pred, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NotMentioned', 'Contradiction']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer.batch_decode([l[m] for l, m in zip(batch['labels'], batch['labels_mask'])], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(62.5257, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(15):\n",
    "    out = model(**batch)\n",
    "    out.loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.8596, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = out.logits.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_text = tokenizer.batch_decode(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "generated = model.memory_cell.model.generate(batch['labels'][:, -5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([50257,  3673,    44,  1463,   276, 50256])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'][0][batch['labels_mask'][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([50257,  3673,    44,  1463,   276, 50256])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['labels'][0][batch['labels_mask'][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_inputs = [torch.cat((b[~m], torch.tensor([gen_token]))) for b, m in zip(batch['input_ids'], batch['labels_mask'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb Cell 34\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb#Y310sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m gen \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mmemory_cell\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(gen_inputs)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:26\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     25\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m():\n\u001b[0;32m---> 26\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/transformers/generation_utils.py:1084\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[39m# 2. Define model inputs\u001b[39;00m\n\u001b[1;32m   1079\u001b[0m \u001b[39m# inputs_tensor has to be defined\u001b[39;00m\n\u001b[1;32m   1080\u001b[0m \u001b[39m# model_input_name is defined if model-specific keyword input is passed\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m \u001b[39m# otherwise model_input_name is None\u001b[39;00m\n\u001b[1;32m   1082\u001b[0m \u001b[39m# all model-specific keyword inputs are removed from `model_kwargs`\u001b[39;00m\n\u001b[1;32m   1083\u001b[0m inputs_tensor, model_input_name, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_model_inputs(inputs, bos_token_id, model_kwargs)\n\u001b[0;32m-> 1084\u001b[0m batch_size \u001b[39m=\u001b[39m inputs_tensor\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1086\u001b[0m \u001b[39m# 3. Define other model kwargs\u001b[39;00m\n\u001b[1;32m   1087\u001b[0m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39moutput_attentions\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m output_attentions\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "gen = model.memory_cell.model.generate(gen_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def keep_for_metrics_fn(batch, output):\n",
    "    data = {}\n",
    "    if isinstance(output, torch.Tensor):\n",
    "        data['labels'] = batch['labels']\n",
    "        \n",
    "    for key in batch.keys():\n",
    "        if 'loss' in key: \n",
    "            data[key] = batch[key]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[GEN]NotMentioned[GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN]',\n",
       " ' in[GEN]Contradiction[GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN]']"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(generated, add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  def keep_for_metrics_fn(batch, output):\n",
    "data = {}\n",
    "# if 'generation_outputs' in output:\n",
    "if isinstance(output, dict):\n",
    "    data['predictions'] = torch.argmax(output['logits'].detach(), dim=-1)\n",
    "    if 'labels_mask' in batch:\n",
    "        data['predictions'] = [data['predictions'][i, mask] for i, mask in enumerate(batch['labels_mask'])]\n",
    "else:\n",
    "    data['generation_outputs'] = output\n",
    "    \n",
    "data['labels'] = batch['labels']\n",
    "for key in batch.keys():\n",
    "    if 'loss' in key: \n",
    "        data[key] = batch[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = CausalLMOutputWithCrossAttentions()\n",
    "# full_logits = torch.cat([o.logits for o in cell_outputs], dim=1)\n",
    "# full_hidden_states = tuple([torch.cat(layer_hs, dim=1) for layer_hs in zip(*[o.hidden_states for o in cell_outputs])])\n",
    "\n",
    "full_logits = out.logits\n",
    "labels = kwargs.get('labels')\n",
    "if labels is not None:\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "    shift_logits = full_logits[..., :-1, :].contiguous()\n",
    "    flat_labels = shift_labels.view(-1)\n",
    "    flat_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "    \n",
    "    loss_fct = CrossEntropyLoss()\n",
    "    labels_mask = kwargs.get('labels_mask')\n",
    "    if labels_mask is not None:\n",
    "        shift_mask = labels_mask[..., :-1].contiguous()\n",
    "\n",
    "        flat_labels = flat_labels[shift_mask.view(-1)]\n",
    "        flat_logits = flat_logits[shift_mask.view(-1)]\n",
    "        \n",
    "    out['loss'] = loss_fct(flat_logits, flat_labels)\n",
    "\n",
    "out['logits'] = full_logits\n",
    "segment_keys = ['loss', 'logits']\n",
    "if kwargs.get('output_attentions'):\n",
    "    segment_keys.append('attentions')\n",
    "if kwargs.get('output_hidden_states'):\n",
    "    segment_keys.append('hidden_states')\n",
    "    out['hidden_states'] = full_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_sampler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb#Y221sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m### booydar\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb#Y221sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m batch_metrics_fn \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m _, y: {key: y[key] \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m y\u001b[39m.\u001b[39mkeys() \u001b[39mif\u001b[39;00m ((\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m key) \u001b[39mor\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39m!log\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m key))}\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb#Y221sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(args, model, optimizer, train_dataloader, valid_dataloader, train_sampler,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb#Y221sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m                     keep_for_metrics_fn\u001b[39m=\u001b[39mkeep_for_metrics_fn, metrics_fn\u001b[39m=\u001b[39mmetrics_fn,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb#Y221sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m                     \u001b[39m###booydar\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb#Y221sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m                     batch_metrics_fn\u001b[39m=\u001b[39mbatch_metrics_fn,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb#Y221sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m                     generate_kwargs\u001b[39m=\u001b[39m{})\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb#Y221sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m args\u001b[39m.\u001b[39mvalidate_only:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb#Y221sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# train loop\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb#Y221sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     trainer\u001b[39m.\u001b[39mtrain()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_sampler' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "### booydar\n",
    "batch_metrics_fn = lambda _, y: {key: y[key] for key in y.keys() if (('loss' in key) or ('!log' in key))}\n",
    "trainer = Trainer(args, model, optimizer, train_dataloader, valid_dataloader, train_sampler,\n",
    "                    keep_for_metrics_fn=keep_for_metrics_fn, metrics_fn=metrics_fn,\n",
    "                    ###booydar\n",
    "                    batch_metrics_fn=batch_metrics_fn,\n",
    "                    generate_kwargs={})\n",
    "\n",
    "if not args.validate_only:\n",
    "    # train loop\n",
    "    trainer.train()\n",
    "    # make sure all workers are done\n",
    "    hvd.barrier()\n",
    "    # run validation after training\n",
    "    if args.save_best:\n",
    "        best_model_path = str(Path(args.model_path) / 'model_best.pth')\n",
    "        if hvd.rank() == 0:\n",
    "            logger.info(f'Loading best saved model from {best_model_path}')\n",
    "        trainer.load(best_model_path)\n",
    "    if valid_dataloader is not None:\n",
    "        if hvd.rank() == 0:\n",
    "            logger.info('Runnning validation on valid data:')\n",
    "        trainer.validate(valid_dataloader, write_tb=False)\n",
    "else:\n",
    "    # run validation, do not write to tensorboard\n",
    "    if hvd.rank() == 0:\n",
    "        logger.info('Running validation on train set:')\n",
    "    trainer.validate(train_dataloader, split='train', write_tb=True)\n",
    "    if valid_dataloader is not None:\n",
    "        if hvd.rank() == 0:\n",
    "            logger.info('Running validation on valid data:')\n",
    "        trainer.validate(valid_dataloader, write_tb=True)\n",
    "    # if test_dataloader is not None:\n",
    "    #     if hvd.rank() == 0:\n",
    "    #         logger.info('Runnning validation on test data:')\n",
    "    #     trainer.validate(test_dataloader, write_tb=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Holder:\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq_len = 512\n",
    "target_seq_len = 512\n",
    "\n",
    "num_mem_tokens = 2\n",
    "input_size = 128\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "args = Holder\n",
    "args.target_seq_len = target_seq_len\n",
    "args.input_seq_len = input_seq_len\n",
    "args.num_mem_tokens = num_mem_tokens\n",
    "args.input_size = input_size\n",
    "args.input_prefix = ''\n",
    "args.block_size = None\n",
    "args.task_name = 'wikitext-2-v1'\n",
    "\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/cryptography/hazmat/backends/openssl/x509.py:14: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = 'gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/home/bulatov/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1360fde036a34b9292dfbe062f075a5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/bulatov/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-389b922bfc5fe729.arrow\n",
      "Loading cached processed dataset at /home/bulatov/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-6067a66e735cfbb1.arrow\n",
      "Loading cached processed dataset at /home/bulatov/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-941845a5470f2db7.arrow\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "raw_datasets = datasets.load_dataset('wikitext', args.task_name)\n",
    "column_names = raw_datasets[\"train\"].column_names\n",
    "text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[text_column_name])\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=column_names,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "block_size = args.input_size \n",
    "if args.num_mem_tokens is not None:\n",
    "    block_size -= 2 * args.num_mem_tokens\n",
    "history_size = args.input_seq_len - block_size\n",
    "\n",
    "def group_texts(examples, block_size, history_size=None):\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "\n",
    "    if history_size is None:\n",
    "        result = {\n",
    "            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "    else:\n",
    "        result = {\n",
    "            k: [t[max({0, i - history_size}) : i + block_size] for i in range(0, total_length, block_size)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_pad_value = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124, 388)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size, history_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/bulatov/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-1504f9373e317eca.arrow\n",
      "Loading cached processed dataset at /home/bulatov/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-c6da793e710ea6d8.arrow\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     input_ids = [torch.tensor(b['input_ids'][::-1]) for b in batch]\n",
    "#     labels = [torch.tensor(b['labels'][::-1]) for b in batch]\n",
    "#     attention_mask = [torch.tensor(b['attention_mask'][::-1]) for b in batch]\n",
    "#     input_ids = pad_sequence(input_ids, padding_value=id_pad_value).T.flip(1)\n",
    "#     labels = pad_sequence(labels, padding_value=-100).T.flip(1)\n",
    "#     attention_mask = pad_sequence(attention_mask, padding_value=0).T.flip(1)\n",
    "\n",
    "#     collated = {'input_ids': input_ids,\n",
    "#                 'labels': labels, \n",
    "#                 'attention_mask': attention_mask}\n",
    "\n",
    "#     if input_ids.shape[1] != block_size:\n",
    "#         labels_mask = torch.ones_like(input_ids, dtype=bool)\n",
    "#         labels_mask[:, :-block_size] = False\n",
    "#         collated['labels_mask'] = labels_mask\n",
    "\n",
    "#     return collated\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [torch.tensor(b['input_ids']) for b in batch]\n",
    "    labels = [torch.tensor(b['labels']) for b in batch]\n",
    "    labels_mask = [torch.ones_like(l, dtype=bool) for l in labels]\n",
    "    attention_mask = [torch.tensor(b['attention_mask']) for b in batch]\n",
    "\n",
    "    input_ids = pad_sequence(input_ids, padding_value=id_pad_value).T\n",
    "    labels = pad_sequence(labels, padding_value=-100).T\n",
    "    labels_mask = pad_sequence(labels_mask, padding_value=False).T\n",
    "    attention_mask = pad_sequence(attention_mask, padding_value=0).T\n",
    "\n",
    "    collated = {'input_ids': input_ids,\n",
    "                'labels': labels, \n",
    "                'labels_mask': labels_mask,\n",
    "                'attention_mask': attention_mask}\n",
    "\n",
    "    # if args.vary_n_segments:\n",
    "    #     n_segments = np.random.randint(1, args.max_n_segments + 1)\n",
    "    #     n_tokens = n_segments * block_size\n",
    "    #     for k in collated:\n",
    "    #         collated[k] = collated[k][:, -n_tokens:]\n",
    "\n",
    "    return collated\n",
    "\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"].map(lambda x: group_texts(x, block_size, history_size), \n",
    "                                        batched=True, desc=f\"Grouping train in chunks of {block_size} and history {history_size}\")\n",
    "valid_dataset = tokenized_datasets[\"validation\"].map(lambda x: group_texts(x, block_size), \n",
    "                                        batched=True, desc=f\"Grouping valid in chunks of {block_size}\")\n",
    "\n",
    "\n",
    "# shuffle train data each epoch (one loop over train_dataset)\n",
    "# train_sampler = DistributedStrain_dataset[i] for i in range(4)ampler(train_dataset, rank=hvd.rank(), num_replicas=hvd.size(), shuffle=True,\n",
    "#                                     drop_last=False, seed=args.seed)\n",
    "# per_worker_batch_size = args.batch_size * args.gradient_accumulation_steps\n",
    "# global_batch_size = per_worker_batch_size * hvd.size()\n",
    "\n",
    "# train_sampler = RandomSampler(train_dataset)\n",
    "kwargs = {'pin_memory': True}#, 'num_workers': args.data_n_workers}\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [train_dataset[i] for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in b[0]:\n",
    "    b[0][k] = b[0][k][:124]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = collate_fn(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = iter(train_dataloader)\n",
    "batch = next(gen)\n",
    "batch = next(gen)\n",
    "batch = next(gen)\n",
    "batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_datasets['train'][1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions\n",
    "\n",
    "class MemoryCell(torch.nn.Module):\n",
    "    def __init__(self, base_model, num_mem_tokens):\n",
    "        super().__init__()\n",
    "        self.model = base_model\n",
    "        self.create_memory(num_mem_tokens)\n",
    "\n",
    "    def create_memory(self, num_mem_tokens):\n",
    "        self.num_mem_tokens = num_mem_tokens\n",
    "        embeddings = self.model.get_input_embeddings()\n",
    "        memory_weights = torch.randn((num_mem_tokens, self.model.config.n_embd)) * embeddings.weight.data.std()\n",
    "        self.register_parameter('memory', torch.nn.Parameter(memory_weights, requires_grad=True))\n",
    "\n",
    "        self.read_memory_position = range(num_mem_tokens)\n",
    "        self.write_memory_position = range(-num_mem_tokens, 0)\n",
    "\n",
    "    def set_memory(self, input_shape):\n",
    "        memory = self.memory.repeat(input_shape[0], 1, 1)\n",
    "        return memory\n",
    "\n",
    "    def forward(self, input_ids, memory_state=None, **kwargs):\n",
    "        if memory_state is None:\n",
    "            memory_state = self.set_memory(input_ids.shape)\n",
    "\n",
    "        seg_kwargs = self.process_input(input_ids, memory_state, **kwargs)\n",
    "        out = self.model(**seg_kwargs)\n",
    "        out, new_memory_state = self.process_output(out, **kwargs)\n",
    "\n",
    "        return out, new_memory_state\n",
    "    \n",
    "    def process_input(self, input_ids, memory_state, **kwargs):\n",
    "        seg_kwargs = dict(**kwargs)\n",
    "\n",
    "        inputs_embeds = kwargs.get('inputs_embeds')\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.model.get_input_embeddings()(input_ids)\n",
    "        inputs_embeds = torch.cat([memory_state, inputs_embeds, memory_state], dim=1)\n",
    "\n",
    "        seg_kwargs['input_ids'] = None\n",
    "        seg_kwargs['inputs_embeds'] = inputs_embeds\n",
    "        if kwargs.get('attention_mask') is not None:\n",
    "            seg_kwargs['attention_mask'] = self.pad_attention_mask(kwargs['attention_mask'], inputs_embeds.shape)\n",
    "        seg_kwargs['output_hidden_states'] = True\n",
    "        return seg_kwargs\n",
    "    \n",
    "    def pad_attention_mask(self, attention_mask, shape):\n",
    "        if self.num_mem_tokens in {0, None}:\n",
    "            return attention_mask\n",
    "        else:\n",
    "            mask = torch.ones(*shape[:2], dtype=torch.int64).to(attention_mask.device)\n",
    "            mask[:, self.num_mem_tokens:-self.num_mem_tokens] = attention_mask\n",
    "            return mask\n",
    "    \n",
    "    def process_output(self, model_outputs, **kwargs):\n",
    "        if self.num_mem_tokens not in {0, None}:\n",
    "            out = CausalLMOutputWithCrossAttentions()\n",
    "            memory_state = model_outputs.hidden_states[-1][:, -self.num_mem_tokens:]\n",
    "            out['logits'] = model_outputs.logits[:, self.num_mem_tokens:-self.num_mem_tokens]\n",
    "            \n",
    "            if kwargs.get('output_hidden_states'):\n",
    "                out['hidden_states'] = [lh[:, self.num_mem_tokens:-self.num_mem_tokens] for lh in model_outputs.hidden_states]\n",
    "            if kwargs.get('output_attentions'):\n",
    "                out['attentions'] = model_outputs['attentions']\n",
    "        else:\n",
    "            memory_state = None\n",
    "            out = model_outputs\n",
    "            \n",
    "        return out, memory_state \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentWrapper(torch.nn.Module):\n",
    "    def __init__(self, memory_cell, **rmt_kwargs):\n",
    "        super().__init__()\n",
    "        self.memory_cell = memory_cell\n",
    "        self.rmt_config = rmt_kwargs\n",
    "\n",
    "    def forward(self, input_ids, labels=None, labels_mask=None, inputs_embeds=None, attention_mask=None, output_attentions=None, output_hidden_states=None):\n",
    "        memory_state = None\n",
    "        segmented = self.segment(input_ids=input_ids, inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
    "\n",
    "        cell_outputs = []\n",
    "        for seg_num, segment in enumerate(segmented):\n",
    "            cell_out, memory_state = self.memory_cell(**segment, memory_state=memory_state, output_hidden_states=True)\n",
    "            cell_outputs.append(cell_out)\n",
    "            self.manage_gradients(memory_state, seg_num)\n",
    "\n",
    "        out = self.process_outputs(cell_outputs, labels=labels, \n",
    "                                   labels_mask=labels_mask,\n",
    "                                   output_attentions=output_attentions, \n",
    "                                   output_hidden_states=output_hidden_states)\n",
    "        return out\n",
    "\n",
    "    def segment(self, **kwargs):\n",
    "        segments = []\n",
    "        for k, tensor in kwargs.items():\n",
    "            if tensor is not None:\n",
    "                k_segments = self.split_tensor(tensor)\n",
    "                for s, k_seg in enumerate(k_segments):\n",
    "                    if s < len(segments):\n",
    "                        segments[s][k] = k_seg\n",
    "                    else:\n",
    "                        segments.append({k: k_seg})\n",
    "\n",
    "        return segments\n",
    "    \n",
    "    def split_tensor(self, tensor):\n",
    "        align = self.rmt_config.get('segment_alignment')\n",
    "        segment_size = self.rmt_config.get('segment_size')\n",
    "        if align in {'left', None}:\n",
    "            split_inds = list(range(0, tensor.shape[1], segment_size)) + [tensor.shape[1]]\n",
    "            segments = [tensor[:, start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "        elif align in {'right', None}:\n",
    "            split_inds = (list(range(tensor.shape[1], 0, -segment_size)) + [0])[::-1]\n",
    "            segments = [tensor[:, start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "        elif align == 'center':\n",
    "            n_seg = math.ceil(tensor.shape[1] / segment_size)\n",
    "            segments = torch.chunk(tensor, n_seg, dim=1)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return segments\n",
    "\n",
    "    def process_outputs(self, cell_outputs, **kwargs):\n",
    "        out = CausalLMOutputWithCrossAttentions()\n",
    "        full_logits = torch.cat([o.logits for o in cell_outputs], dim=1)\n",
    "        full_hidden_states = tuple([torch.cat(layer_hs, dim=1) for layer_hs in zip(*[o.hidden_states for o in cell_outputs])])\n",
    "\n",
    "        labels = kwargs.get('labels')\n",
    "        if labels is not None:\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            shift_logits = full_logits[..., :-1, :].contiguous()\n",
    "            flat_labels = shift_labels.view(-1)\n",
    "            flat_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "            \n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            labels_mask = kwargs.get('labels_mask')\n",
    "            if labels_mask is not None:\n",
    "                shift_mask = labels_mask[..., :-1].contiguous()\n",
    "\n",
    "                flat_labels = flat_labels[shift_mask.view(-1)]\n",
    "                flat_logits = flat_logits[shift_mask.view(-1)]\n",
    "                \n",
    "            out['loss'] = loss_fct(flat_logits, flat_labels)\n",
    "\n",
    "        out['logits'] = full_logits\n",
    "        segment_keys = ['loss', 'logits']\n",
    "        if kwargs.get('output_attentions'):\n",
    "            segment_keys.append('attentions')\n",
    "        if kwargs.get('output_hidden_states'):\n",
    "            segment_keys.append('hidden_states')\n",
    "            out['hidden_states'] = full_hidden_states\n",
    "\n",
    "        # for seg_num, o in enumerate(cell_outputs):\n",
    "        #     for key, value in o.items():\n",
    "        #         if any([sk in key for sk in segment_keys]):\n",
    "        #             out[f'{key}_{seg_num}'] = value\n",
    "\n",
    "        return out \n",
    "        \n",
    "    def manage_gradients(self, memory_state, seg_num):\n",
    "        k2, max_n_segments = self.rmt_config.get('k2'), self.rmt_config.get('max_n_segments')\n",
    "        if seg_num == 0 \\\n",
    "            or k2 in {-1, None} \\\n",
    "            or seg_num + k2 > max_n_segments:\n",
    "                return True\n",
    "        \n",
    "        memory_state = memory_state.detach()\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/cryptography/hazmat/backends/openssl/x509.py:14: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_mem_tokens = 10\n",
    "# device = torch.device(3)\n",
    "device = 'cpu'\n",
    "\n",
    "rmt_config = {'num_mem_tokens': 10, \n",
    "               #  'max_n_segments': 1,\n",
    "               #  'segment_alignment': 'right',\n",
    "               #  'tokenizer': tokenizer,\n",
    "               #  'memory_layers': 'all', \n",
    "               #  'share_memory_layers': True,\n",
    "               #  'reconstruction_loss_coef': 0.1,\n",
    "               #  'k1': -1, 'k2': 3,\n",
    "               #  'segment_ordering': 'regular',\n",
    "               #  'input_size': 1024, \n",
    "               #  'bptt_depth': -1, \n",
    "               #  'sum_loss': False,\n",
    "             }\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "cell = MemoryCell(base_model, num_mem_tokens=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmt = RecurrentWrapper(cell, max_n_segments=5, segment_size=124, segment_alignment='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = iter(train_dataloader)\n",
    "batch = next(gen)\n",
    "batch = next(gen)\n",
    "batch = next(gen)\n",
    "batch.pop('labels_mask')\n",
    "# batch.pop('labels')\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmt_out = rmt(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['loss', 'logits'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmt_out.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contract-NLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "class CNLIDataset(Dataset):\n",
    "    def __init__(self, json_path):\n",
    "        with open(json_path, 'r') as f:\n",
    "            self.samples = json.load(f)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.samples[item]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done   1 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Batch computation too fast (0.0032s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=12)]: Done   8 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Batch computation too fast (0.0160s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=12)]: Done  28 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done  50 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 123 out of 123 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "dataset = CNLIDataset('/cephfs/home/bulatov/bulatov/datasets/contract_nli/contract-nli/test_processed.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.input_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_tokens('[GEN]', special_tokens=True)\n",
    "gen_token = tokenizer.encode('[GEN]')[0]\n",
    "id_pad_value = tokenizer.eos_token_id\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs = [b['context'][:args.input_seq_len * 10] for b in batch]\n",
    "    labels = [b['answer'][:args.input_seq_len * 10] for b in batch]\n",
    "\n",
    "    collated = {}\n",
    "    inputs = tokenizer.batch_encode_plus(list(inputs), padding=False)\n",
    "    labels = tokenizer.batch_encode_plus(list(labels), padding=False)\n",
    "\n",
    "    full_inputs, labels_mask = [], []\n",
    "    for inp, lab in zip(inputs['input_ids'], labels['input_ids']):\n",
    "        inp = inp[:args.input_seq_len - len(lab) - 1]\n",
    "        full_inputs.append(torch.tensor(inp + [gen_token] + lab))\n",
    "        labels_mask.append(torch.tensor([False] * len(inp) + [True] * (len(lab) + 1)))\n",
    "\n",
    "    full_inputs = pad_sequence(full_inputs, padding_value=id_pad_value).T\n",
    "    labels_mask = pad_sequence(labels_mask, padding_value=False).T\n",
    "\n",
    "    collated['input_ids'] = collated['labels'] = full_inputs\n",
    "    collated['labels_mask'] = labels_mask\n",
    "    collated['attention_mask'] = collated['input_ids'] != id_pad_value\n",
    "    return collated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = collate_fn([dataset[i] for i in range(4)])\n",
    "# batch = [dataset[i] for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented = rmt.segment(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([4, 103]),\n",
       " torch.Size([4, 103]),\n",
       " torch.Size([4, 103]),\n",
       " torch.Size([4, 103]),\n",
       " torch.Size([4, 100])]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s['input_ids'].shape for s in segmented]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = batch.pop('labels')\n",
    "rmt_out = rmt(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512, 50258])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmt_out.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 50258])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmt_out.logits[batch['labels_mask']].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[batch['labels_mask']].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NotMentioned<|endoftext|>'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([ 3673,    44,  1463,   276, 50256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Entailment'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([14539,   603,   434])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmt_out = rmt(**batch)\n",
    "\n",
    "self = rmt\n",
    "input_ids = batch['input_ids']\n",
    "\n",
    "memory_state = None\n",
    "segmented = self.segment(input_ids=input_ids, inputs_embeds=None, attention_mask=None)\n",
    "\n",
    "cell_outputs = []\n",
    "for seg_num, segment in enumerate(segmented):\n",
    "    cell_out, memory_state = self.memory_cell(**segment, memory_state=memory_state, output_hidden_states=True)#**batch)\n",
    "    cell_outputs.append(cell_out)\n",
    "    self.manage_gradients(memory_state, seg_num)\n",
    "\n",
    "# out = self.process_outputs(cell_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 248])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1.]), tensor([1., 1., 1., 1.]), tensor([1., 1.]))"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.chunk(torch.ones(10), 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50258, 768)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "# scrolls_metric_path = hf_hub_download(repo_id=\"datasets/tau/scrolls\", filename=\"metrics/scrolls.py\")tokenizer.add_tokens('[GEN]', special_tokens=True)\n",
    "gen_token = tokenizer.encode('[GEN]')[0]\n",
    "\n",
    "rmt.memory_cell.model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# gen_token = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<|endoftext|>',\n",
       " 'eos_token': '<|endoftext|>',\n",
       " 'unk_token': '<|endoftext|>'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_attention_first_token = False  # should be True for LED\n",
    "encode_plus_kwargs = {'truncation': True, 'padding': 'longest', 'pad_to_multiple_of': 1}\n",
    "# generate_kwargs = {'max_length': args.targettokenizer.add_tokens('[GEN]', special_tokens=True)\n",
    "# gen_token = tokenizer.encode('[GEN]')[0]\n",
    "\n",
    "# rmt.memory_cell.model.resize_token_embeddings(len(tokenizer))_seq_len, 'min_length': args.target_seq_len}\n",
    "generate_kwargs = {}\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs = [b['input'][:args.input_seq_len * 10] for b in batch]\n",
    "    labels = [b['output'][:args.input_seq_len * 10] for b in batch]\n",
    "\n",
    "    collated = {}\n",
    "    inputs = tokenizer.batch_encode_plus(list(inputs), padding=False)\n",
    "    labels = tokenizer.batch_encode_plus(list(labels), padding=False)\n",
    "\n",
    "    full_inputs = [torch.tensor(i[:input_size - len(l) - 1] + [gen_token] + l) for i, l in zip(inputs['input_ids'], labels['input_ids'])]\n",
    "    full_inputs = pad_sequence(full_inputs, padding_value=tokenizer.pad_token_id).T\n",
    "    \n",
    "    labels_mask = torch.zeros_like(full_inputs).bool()\n",
    "    for i, l in enumerate(labels['input_ids']):\n",
    "        labels_mask[i, -len(l) -1:] = True\n",
    "    collated['input_ids'] = collated['labels'] = full_inputs\n",
    "    collated['labels_mask'] = labels_mask\n",
    "    collated['attention_mask'] = collated['input_ids'] != tokenizer.pad_token_id\n",
    "\n",
    "    return collated\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset scrolls (/home/bulatov/.cache/huggingface/datasets/tau___scrolls/quality/1.0.0/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bb2ffcdd0604c9bb263db2f85f18d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq2seq_task_name = 'quality'\n",
    "dataset = datasets.load_dataset('tau/scrolls', seq2seq_task_name)\n",
    "train_dataset = dataset['train']\n",
    "# shuffle train data each epoch (one loop over train_dataset)\n",
    "# train_sampler = DistributedSampler(train_dataset, shuffle=True, drop_last=False, seed=args.seed)\n",
    "kwargs = {'pin_memory': True}\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2,\n",
    "                                collate_fn=collate_fn, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = iter(train_dataloader)\n",
    "batch = next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'labels', 'labels_mask', 'attention_mask'])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['labels'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Why is Si retirement so significant to the Space Exploration Team? \\n\\n (A) There aren’t enough working people in the world. They won’t be able to find a replacement.\\n (B) As one of two remaining spacemen, it would likely mean the defunding and shut down of the Space Exploration Team.\\n (C) Training new spacemen is costly and time consuming. They won’t have anyone else ready after him.\\n (D) His retirement may inspire others to[GEN]Training new spacemen is costly and time consuming. They won’t have anyone else ready after him.',\n",
       " 'What makes Gubelin an outlier in the present day?\\n\\n (A) He is much older than the rest of the population.\\n (B) He refuses new operations that could improve his health.\\n (C) His mind is still active, and he values hard work.\\n (D) He still wears glasses and value objects like the gold watch given to Si.\\n\\n\\nSPACEMAN ON A SPREE\\n\\n\\n\\n\\n   BY MACK REYNOLDS\\n\\n\\n\\n\\n   Illustrated by Nodel\\n\\n\\n\\n\\n[GEN]He still wears glasses and value objects like the gold watch given to Si.']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(batch['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[GEN]Training new spacemen is costly and time consuming. They won’t have anyone else ready after him.',\n",
       " '[GEN]He still wears glasses and value objects like the gold watch given to Si.']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode([c[i] for c, i in zip(batch['input_ids'], batch['labels_mask'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Why is Si retirement so significant to the Space Exploration Team? \\n\\n (A) There aren’t enough working people in the world. They won’t be able to find a replacement.\\n (B) As one of two remaining spacemen, it would likely mean the defunding and shut down of the Space Exploration Team.\\n (C) Training new spacemen is costly and time consuming. They won’t have anyone else ready after him.\\n (D) His retirement may inspire others toTraining new spacemen is costly and time consuming. They won’t have anyone else ready after him.',\n",
       " 'What makes Gubelin an outlier in the present day?\\n\\n (A) He is much older than the rest of the population.\\n (B) He refuses new operations that could improve his health.\\n (C) His mind is still active, and he values hard work.\\n (D) He still wears glasses and value objects like the gold watch given to Si.\\n\\n\\nSPACEMAN ON A SPREE\\n\\n\\n\\n\\n   BY MACK REYNOLDS\\n\\n\\n\\n\\n   Illustrated by Nodel\\n\\n\\n\\n\\nHe still wears glasses and value objects like the gold watch given to Si.']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode([c[i] for c, i in zip(batch['input_ids'], batch['attention_mask'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = rmt(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(60.2067, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(61.4377, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hvdenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
