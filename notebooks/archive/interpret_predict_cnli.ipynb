{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers import PreTrainedModel, AutoModelForSequenceClassification, T5ForConditionalGeneration\n",
    "from transformers import AutoTokenizer\n",
    "import datasets\n",
    "\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers import BertForSequenceClassification\n",
    "import transformers\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from modeling_rmt import RMTEncoderForSequenceClassification\n",
    "# from modeling_rmt_enc_dec import RMTEncoderDecoderForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_segments = 6\n",
    "num_mem_tokens = 10\n",
    "# device = torch.device(3)\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/cryptography/hazmat/backends/openssl/x509.py:14: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-cased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "rmt_config = {'num_mem_tokens': num_mem_tokens, \n",
    "                'max_n_segments': num_segments,\n",
    "                'tokenizer': tokenizer,\n",
    "               #  'memory_layers': 'all', \n",
    "               #  'share_memory_layers': True,\n",
    "               #  'reconstruction_loss_coef': 0.1,\n",
    "               #  'separate_memory_classifier': True,\n",
    "               #  'memory_aggregation': 'avg',\n",
    "               #  'memory_task_loss_coef': 1e-2,\n",
    "                'segment_ordering': 'regular',\n",
    "                'input_size': 512, \n",
    "                'bptt_depth': -1, \n",
    "                'sum_loss': False,\n",
    "             }\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3).to(device)\n",
    "rmt = RMTEncoderForSequenceClassification(base_model, **rmt_config).to(device)\n",
    "# rmt = RMTEncoderCPUOffload(base_model, **rmt_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpt = torch.load('../../runs/curriculum/contract_nli/bert-base-cased/lr1e-05_linear_adamw_wd1e-03_2994-512-{6}seg_mem10_bs32_iters8000_regular_sum_loss_from_cpt_5-6/run_1/model_best.pth', map_location='cpu')\n",
    "rmt.load_state_dict(cpt['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['loss', 'logits', 'hidden_states', 'loss_0', 'loss_1', 'loss_2'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = rmt(sample_input_ids, **kwargs, output_hidden_states=False, output_attentions=False)\n",
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('loss', tensor(1.0148, grad_fn=<NllLossBackward>)),\n",
       " ('loss_0', tensor(1.3241, grad_fn=<NllLossBackward>)),\n",
       " ('loss_1', tensor(1.5897, grad_fn=<NllLossBackward>)),\n",
       " ('loss_2', tensor(1.0148, grad_fn=<NllLossBackward>))]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(k, out[k]) for k in out if 'loss' in k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = iter(valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('loss', tensor(0.2106, grad_fn=<NllLossBackward>)),\n",
       " ('loss_0', tensor(0.6295, grad_fn=<NllLossBackward>)),\n",
       " ('loss_1', tensor(0.4058, grad_fn=<NllLossBackward>)),\n",
       " ('loss_2', tensor(0.2319, grad_fn=<NllLossBackward>)),\n",
       " ('loss_3', tensor(0.1637, grad_fn=<NllLossBackward>)),\n",
       " ('loss_4', tensor(0.1651, grad_fn=<NllLossBackward>)),\n",
       " ('loss_5', tensor(0.2106, grad_fn=<NllLossBackward>))]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(gen)\n",
    "\n",
    "if 'id' in sample:\n",
    "    id = sample.pop('id')\n",
    "if 'target_text' in sample:\n",
    "    tgt_text = sample.pop('target_text')\n",
    "\n",
    "# rmt.to(device)\n",
    "for k in sample:\n",
    "    sample[k] = sample[k].to(device)\n",
    "    \n",
    "sample_input_ids = sample.pop('input_ids').to(device)\n",
    "kwargs = sample\n",
    "\n",
    "out = rmt(sample_input_ids, **kwargs, output_hidden_states=False, output_attentions=False)\n",
    "[(k, out[k]) for k in out if 'loss' in k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('loss', tensor(1.1045, grad_fn=<NllLossBackward>)),\n",
       " ('loss_0', tensor(1.9142, grad_fn=<NllLossBackward>)),\n",
       " ('loss_1', tensor(1.2701, grad_fn=<NllLossBackward>)),\n",
       " ('loss_2', tensor(2.6680, grad_fn=<NllLossBackward>)),\n",
       " ('loss_3', tensor(2.3840, grad_fn=<NllLossBackward>)),\n",
       " ('loss_4', tensor(1.1045, grad_fn=<NllLossBackward>))]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample = next(gen)\n",
    "\n",
    "if 'id' in sample:\n",
    "    id = sample.pop('id')\n",
    "if 'target_text' in sample:\n",
    "    tgt_text = sample.pop('target_text')\n",
    "\n",
    "# rmt.to(device)\n",
    "for k in sample:\n",
    "    sample[k] = sample[k].to(device)\n",
    "    \n",
    "sample_input_ids = sample.pop('input_ids').to(device)\n",
    "kwargs = sample\n",
    "\n",
    "out = rmt(sample_input_ids, **kwargs, output_hidden_states=False, output_attentions=False)\n",
    "[(k, out[k]) for k in out if 'loss' in k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('loss', tensor(0.1997, grad_fn=<NllLossBackward>)),\n",
       " ('loss_0', tensor(0.2862, grad_fn=<NllLossBackward>)),\n",
       " ('loss_1', tensor(0.4708, grad_fn=<NllLossBackward>)),\n",
       " ('loss_2', tensor(0.4086, grad_fn=<NllLossBackward>)),\n",
       " ('loss_3', tensor(0.2547, grad_fn=<NllLossBackward>)),\n",
       " ('loss_4', tensor(0.2181, grad_fn=<NllLossBackward>)),\n",
       " ('loss_5', tensor(0.1997, grad_fn=<NllLossBackward>))]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(gen)\n",
    "\n",
    "if 'id' in sample:\n",
    "    id = sample.pop('id')\n",
    "if 'target_text' in sample:\n",
    "    tgt_text = sample.pop('target_text')\n",
    "\n",
    "# rmt.to(device)\n",
    "for k in sample:\n",
    "    sample[k] = sample[k].to(device)\n",
    "    \n",
    "sample_input_ids = sample.pop('input_ids').to(device)\n",
    "kwargs = sample\n",
    "\n",
    "out = rmt(sample_input_ids, **kwargs, output_hidden_states=False, output_attentions=False)\n",
    "[(k, out[k]) for k in out if 'loss' in k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('loss', tensor(0.2881, grad_fn=<NllLossBackward>)),\n",
       " ('loss_0', tensor(0.2883, grad_fn=<NllLossBackward>)),\n",
       " ('loss_1', tensor(0.2028, grad_fn=<NllLossBackward>)),\n",
       " ('loss_2', tensor(0.1835, grad_fn=<NllLossBackward>)),\n",
       " ('loss_3', tensor(0.1985, grad_fn=<NllLossBackward>)),\n",
       " ('loss_4', tensor(0.2881, grad_fn=<NllLossBackward>))]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(gen)\n",
    "\n",
    "if 'id' in sample:\n",
    "    id = sample.pop('id')\n",
    "if 'target_text' in sample:\n",
    "    tgt_text = sample.pop('target_text')\n",
    "\n",
    "# rmt.to(device)\n",
    "for k in sample:\n",
    "    sample[k] = sample[k].to(device)\n",
    "    \n",
    "sample_input_ids = sample.pop('input_ids').to(device)\n",
    "kwargs = sample\n",
    "\n",
    "out = rmt(sample_input_ids, **kwargs, output_hidden_states=False, output_attentions=False)\n",
    "[(k, out[k]) for k in out if 'loss' in k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('loss', tensor(0.1461, grad_fn=<NllLossBackward>)),\n",
       " ('loss_0', tensor(0.2555, grad_fn=<NllLossBackward>)),\n",
       " ('loss_1', tensor(0.1634, grad_fn=<NllLossBackward>)),\n",
       " ('loss_2', tensor(0.1461, grad_fn=<NllLossBackward>))]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(gen)\n",
    "\n",
    "if 'id' in sample:\n",
    "    id = sample.pop('id')\n",
    "if 'target_text' in sample:\n",
    "    tgt_text = sample.pop('target_text')\n",
    "\n",
    "# rmt.to(device)\n",
    "for k in sample:\n",
    "    sample[k] = sample[k].to(device)\n",
    "    \n",
    "sample_input_ids = sample.pop('input_ids').to(device)\n",
    "kwargs = sample\n",
    "\n",
    "out = rmt(sample_input_ids, **kwargs, output_hidden_states=False, output_attentions=False)\n",
    "[(k, out[k]) for k in out if 'loss' in k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('loss', tensor(0.3367, grad_fn=<NllLossBackward>)),\n",
       " ('loss_0', tensor(0.6846, grad_fn=<NllLossBackward>)),\n",
       " ('loss_1', tensor(0.2950, grad_fn=<NllLossBackward>)),\n",
       " ('loss_2', tensor(0.8451, grad_fn=<NllLossBackward>)),\n",
       " ('loss_3', tensor(0.3367, grad_fn=<NllLossBackward>))]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(gen)\n",
    "\n",
    "if 'id' in sample:\n",
    "    id = sample.pop('id')\n",
    "if 'target_text' in sample:\n",
    "    tgt_text = sample.pop('target_text')\n",
    "\n",
    "# rmt.to(device)\n",
    "for k in sample:\n",
    "    sample[k] = sample[k].to(device)\n",
    "    \n",
    "sample_input_ids = sample.pop('input_ids').to(device)\n",
    "kwargs = sample\n",
    "\n",
    "out = rmt(sample_input_ids, **kwargs, output_hidden_states=True, output_attentions=False)\n",
    "[(k, out[k]) for k in out if 'loss' in k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2322])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(gen)\n",
    "sample['input_ids'].shape\n",
    "# sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 768])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['hidden_states_0'][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kwargs['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.logits.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.6167, grad_fn=<NllLossBackward>),\n",
       " tensor(0.9928, grad_fn=<NllLossBackward>),\n",
       " tensor(1.4698, grad_fn=<NllLossBackward>),\n",
       " tensor(1.1471, grad_fn=<NllLossBackward>),\n",
       " tensor(1.3934, grad_fn=<NllLossBackward>),\n",
       " tensor(1.3934, grad_fn=<NllLossBackward>))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out.loss_0, out.loss_1, out.loss_2, out.loss_3, out.loss_4, out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.4808, grad_fn=<NllLossBackward>),\n",
       " tensor(1.4873, grad_fn=<NllLossBackward>),\n",
       " tensor(1.4421, grad_fn=<NllLossBackward>),\n",
       " tensor(1.3990, grad_fn=<NllLossBackward>),\n",
       " tensor(1.4740, grad_fn=<NllLossBackward>),\n",
       " tensor(1.4740, grad_fn=<NllLossBackward>))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out.loss_0, out.loss_1, out.loss_2, out.loss_3, out.loss_4, out.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Holder:\n",
    "#     def __init__(self):\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_seq_len = 2495\n",
    "# target_seq_len = 512\n",
    "# batch_size = 2\n",
    "\n",
    "# args = Holder\n",
    "# args.target_seq_len = target_seq_len\n",
    "# args.input_seq_len = input_seq_len\n",
    "# args.input_prefix = ''\n",
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global_attention_first_token = False  # should be True for LED\n",
    "# encode_plus_kwargs = {'truncation': True, 'padding': 'longest', 'pad_to_multiple_of': 1}\n",
    "# # generate_kwargs = {'max_length': args.target_seq_len, 'min_length': args.target_seq_len}\n",
    "# generate_kwargs = {}\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     # cut too long strings because they may slow down tokenization\n",
    "#     inputs = [b['input'][:args.input_seq_len * 10] for b in batch]\n",
    "#     if 'outputs' in batch[0]:\n",
    "#         # if we have more than 1 label per example (only in valid) take only one of them\n",
    "#         # to compute loss on valid\n",
    "#         labels = [b['outputs'][0][:args.target_seq_len * 10] for b in batch]\n",
    "#     else:\n",
    "#         labels = [b['output'][:args.target_seq_len * 10] for b in batch]\n",
    "#     if args.input_prefix:\n",
    "#         inputs = [args.input_prefix + inp for inp in inputs]\n",
    "#     features = tokenizer.batch_encode_plus(list(inputs), max_length=args.input_seq_len, return_tensors='pt',\n",
    "#                                            **encode_plus_kwargs)\n",
    "#     with tokenizer.as_target_tokenizer():\n",
    "#         labels = tokenizer.batch_encode_plus(list(labels), max_length=args.target_seq_len, return_tensors='pt',\n",
    "#                                              **encode_plus_kwargs).input_ids\n",
    "#     labels[labels == tokenizer.pad_token_id] = -100\n",
    "#     features['labels'] = labels\n",
    "#     features['id'] = [b['id'] for b in batch]\n",
    "#     if 'outputs' in batch[0]:\n",
    "#         features['target_text'] = [b['outputs'] for b in batch]\n",
    "#     else:\n",
    "#         features['target_text'] = [b['output'] for b in batch]\n",
    "#     if 'global_attention_mask' in features:\n",
    "#         raise RuntimeError('What global attention mask for Longformer and LongformerEncoder-Decoder should be?')\n",
    "#     return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_name = 'qasper'\n",
    "# dataset = datasets.load_dataset('tau/scrolls', task_name)\n",
    "# train_dataset = dataset['train']\n",
    "\n",
    "# train_sampler = RandomSampler(train_dataset,)\n",
    "# kwargs = {'pin_memory': True, 'num_workers': 0}\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler,\n",
    "#                                 collate_fn=collate_fn, **kwargs)\n",
    "\n",
    "# valid_dataset = dataset['validation']\n",
    "# valid_sampler = RandomSampler(valid_dataset)\n",
    "# valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, sampler=valid_sampler,\n",
    "#                                 collate_fn=collate_fn, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # gen = iter(train_dataloader)\n",
    "# gen = iter(valid_dataloader)\n",
    "# sample = next(gen)\n",
    "\n",
    "# if 'id' in sample:\n",
    "#     id = sample.pop('id')\n",
    "# if 'target_text' in sample:\n",
    "#     tgt_text = sample.pop('target_text')\n",
    "\n",
    "# # rmt.to(device)\n",
    "# for k in sample:\n",
    "#     sample[k] = sample[k].to(device)\n",
    "    \n",
    "# sample_input_ids = sample.pop('input_ids').to(device)\n",
    "# kwargs = sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset scrolls (/home/bulatov/.cache/huggingface/datasets/tau___scrolls/contract_nli/1.0.0/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbc9e182f0e04326bc0e584e12a4fe75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Holder:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "input_seq_len = 100_000\n",
    "target_seq_len = 512\n",
    "batch_size = 1\n",
    "\n",
    "args = Holder\n",
    "args.target_seq_len = target_seq_len\n",
    "args.input_seq_len = input_seq_len\n",
    "args.input_prefix = ''\n",
    "device = 'cpu'\n",
    "\n",
    "encode_plus_kwargs = {'max_length': args.input_seq_len,\n",
    "                        'truncation': True,\n",
    "                        'padding': 'longest',\n",
    "                        'pad_to_multiple_of': 1}\n",
    "generate_kwargs = {}\n",
    "labels_map = {'Contradiction': 0, 'Entailment': 1, 'Not mentioned': 2}\n",
    "num_labels = len(labels_map)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # cut too long strings because they may slow down tokenization\n",
    "    inputs = [b['input'][:args.input_seq_len * 10] for b in batch]\n",
    "    labels = [b['output'][:args.target_seq_len * 10] for b in batch]\n",
    "    if args.input_prefix:\n",
    "        inputs = [args.input_prefix + inp for inp in inputs]\n",
    "    features = tokenizer.batch_encode_plus(list(inputs), return_tensors='pt', **encode_plus_kwargs)\n",
    "    labels = np.array([labels_map[t] for t in labels])\n",
    "    features['labels'] = torch.from_numpy(labels)\n",
    "    return features\n",
    "\n",
    "task_name = 'contract_nli'\n",
    "dataset = datasets.load_dataset('tau/scrolls', task_name)\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "train_sampler = RandomSampler(train_dataset,)\n",
    "kwargs = {'pin_memory': True, 'num_workers': 0}\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler,\n",
    "                                collate_fn=collate_fn, **kwargs)\n",
    "\n",
    "valid_dataset = dataset['validation']\n",
    "valid_sampler = RandomSampler(valid_dataset)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, sampler=valid_sampler,\n",
    "                                collate_fn=collate_fn, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/interpret_predict_cnli.ipynb Cell 32\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu6/home/bulatov/bulatov/RMT_light/framework/notebooks/interpret_predict_cnli.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m labels \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu6/home/bulatov/bulatov/RMT_light/framework/notebooks/interpret_predict_cnli.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10000\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu6/home/bulatov/bulatov/RMT_light/framework/notebooks/interpret_predict_cnli.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     sample \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(gen)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu6/home/bulatov/bulatov/RMT_light/framework/notebooks/interpret_predict_cnli.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     l \u001b[39m=\u001b[39m sample[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu6/home/bulatov/bulatov/RMT_light/framework/notebooks/interpret_predict_cnli.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     lab \u001b[39m=\u001b[39m sample[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:435\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    434\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> 435\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    436\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    437\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    438\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    439\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:474\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 474\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_fetcher\u001b[39m.\u001b[39mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    476\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:427\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter._next_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_index\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 427\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sampler_iter)\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gen = iter(valid_dataloader)\n",
    "\n",
    "ls = []\n",
    "labels = []\n",
    "for _ in range(10000):\n",
    "    sample = next(gen)\n",
    "    l = sample['input_ids'].shape[1]\n",
    "    lab = sample['labels'][0].item()\n",
    "    ls.append(l)\n",
    "    labels.append(lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = np.array(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 34., 136., 221., 119., 182.,  89.,  86.,  51.,  51.,  17.,   0.,\n",
       "          0.,  17.,  17.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  17.]),\n",
       " array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
       "        14., 15., 16., 17., 18., 19., 20., 21., 22., 23.]),\n",
       " <BarContainer object of 22 artists>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist((ls / 499).astype(int), bins=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hvdenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
