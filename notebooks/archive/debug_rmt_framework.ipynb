{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers import PreTrainedModel, AutoModelForSequenceClassification, T5ForConditionalGeneration\n",
    "from transformers import AutoTokenizer\n",
    "import datasets\n",
    "\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers import BertForSequenceClassification\n",
    "import transformers\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "# from modeling_rmt import RMTEncoderForSequenceClassification\n",
    "# from modeling_rmt_enc_dec import RMTEncoderDecoderForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RMTBaseModel(torch.nn.Module):\n",
    "    def __init__(self, base_model, **rmt_kwargs):\n",
    "        super().__init__()\n",
    "        self.model = base_model\n",
    "        self.set_params(**rmt_kwargs)\n",
    "\n",
    "    def set_params(self, num_mem_tokens, tokenizer, **rmt_config):\n",
    "        self.rmt_config = rmt_config\n",
    "        self.extract_special_tokens(tokenizer)\n",
    "        self.extend_word_embeddings(num_mem_tokens, tokenizer)\n",
    "\n",
    "        self.segment_size = rmt_config['input_size'] - num_mem_tokens - tokenizer.num_special_tokens_to_add()\n",
    "        if 'sep_token' in tokenizer.special_tokens_map:\n",
    "            self.segment_size -= 1\n",
    "\n",
    "    def set_memory(self, input_shape):\n",
    "        memory = self.model.embeddings(self.mem_token_ids)\n",
    "        memory = memory.repeat(input_shape[0], 1, 1)\n",
    "        return memory\n",
    "\n",
    "    def extract_special_tokens(self, tokenizer):\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "        self.special_token_ids = [tokenizer.pad_token_id]\n",
    "        for token in ['cls_token', 'sep_token', 'eos_token', 'bos_token']:\n",
    "            token_id = getattr(tokenizer, f'{token}_id')\n",
    "            if token_id is not None:\n",
    "                self.register_buffer(token, torch.tensor([token_id]))\n",
    "                self.special_token_ids.append(token_id)\n",
    "            else:\n",
    "                setattr(self, token, None)\n",
    "\n",
    "    def extend_word_embeddings(self, num_mem_tokens, tokenizer):\n",
    "            \n",
    "        vocab_size = self.model.config.vocab_size\n",
    "        extended_vocab_size = vocab_size + num_mem_tokens\n",
    "        self.num_mem_tokens = num_mem_tokens\n",
    "        self.register_buffer('mem_token_ids', torch.arange(vocab_size, vocab_size + num_mem_tokens))\n",
    "        self.model.resize_token_embeddings(extended_vocab_size)\n",
    "\n",
    "        special_tokens = tokenizer.special_tokens_map\n",
    "        mem_start_ind = int('cls_token' in special_tokens or 'bos_token' in special_tokens)\n",
    "        self.memory_position = range(mem_start_ind, mem_start_ind + num_mem_tokens)\n",
    "        \n",
    "        if hasattr(self.model.base_model, 'embeddings'): # enc-only\n",
    "            self.model.embeddings = self.model.base_model.embeddings.word_embeddings\n",
    "        elif hasattr(self.model.encoder, 'embed_tokens'): # enc-dec\n",
    "            self.model.embeddings = self.model.encoder.embed_tokens\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "       raise NotImplementedError\n",
    "\n",
    "    def pad_and_segment(self, input_ids):\n",
    "        segmented_batch = []\n",
    "        for seq in input_ids:\n",
    "            drop_mask = sum([seq == t for t in self.special_token_ids])\n",
    "            seq = seq[(1 - drop_mask).bool()]\n",
    "            seq = seq[:self.segment_size * self.rmt_config['max_n_segments']]\n",
    "\n",
    "            align = self.rmt_config.get('segment_alignment')\n",
    "            if align in {'right', None}:\n",
    "                split_inds = (list(range(len(seq), 0, -self.segment_size)) + [0])[::-1]\n",
    "                input_segments = [seq[start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "            elif align == 'left':\n",
    "                split_inds = list(range(0, len(seq), self.segment_size)) + [len(seq)]\n",
    "                input_segments = [seq[start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "            else:\n",
    "                n_seg = math.ceil(len(seq) / self.segment_size)\n",
    "                input_segments = torch.chunk(seq, n_seg)\n",
    "\n",
    "            input_segments = [self.pad_add_special_tokens(t, self.rmt_config['input_size']) for t in input_segments]\n",
    "\n",
    "            # add empty segment markers if needed\n",
    "            n_empty_segments = self.rmt_config['max_n_segments'] - len(input_segments)\n",
    "            input_segments = [None] * n_empty_segments + input_segments\n",
    "\n",
    "            segmented_batch.append(input_segments)\n",
    "\n",
    "        segmented_batch = [[sample[seg_num] for sample in segmented_batch] \\\n",
    "                            for seg_num in range(self.rmt_config['max_n_segments'])]\n",
    "        return segmented_batch\n",
    "\n",
    "    def pad_add_special_tokens(self, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def prepare_kwargs(self, segment_input_ids, kwargs):\n",
    "        seg_kwargs = dict(**kwargs)\n",
    "        non_empty_mask = [s is not None for s in segment_input_ids]\n",
    "        if sum(non_empty_mask) == 0:\n",
    "            return None, non_empty_mask\n",
    "            \n",
    "        input_ids = torch.stack([s for s in segment_input_ids if s is not None])\n",
    "        inputs_embeds = self.model.embeddings(input_ids)\n",
    "\n",
    "        seg_kwargs['input_ids'] = None\n",
    "        seg_kwargs['inputs_embeds'] = inputs_embeds\n",
    "        if seg_kwargs.get('labels') is not None:\n",
    "            seg_kwargs['labels'] = seg_kwargs['labels'][non_empty_mask]\n",
    "        seg_kwargs['attention_mask'] = self.get_attention_mask(input_ids)\n",
    "        if seg_kwargs.get('token_type_ids') is not None:\n",
    "            seg_kwargs['token_type_ids'] = self.get_token_type_ids(input_ids)\n",
    "        seg_kwargs['output_hidden_states'] = True\n",
    "\n",
    "        return seg_kwargs, non_empty_mask\n",
    "\n",
    "    def process_outputs(self, model_outputs, output_attentions, output_hidden_states):\n",
    "        rmt_out = model_outputs[-1]\n",
    "\n",
    "        segment_keys = ['loss']\n",
    "        if output_attentions:\n",
    "            segment_keys.append('attentions')\n",
    "        if output_hidden_states:\n",
    "            segment_keys.append('hidden_states')\n",
    "\n",
    "        extracted = {}\n",
    "        for seg_num, out in enumerate(model_outputs):\n",
    "            for key, value in out.items():\n",
    "                if any([sk in key for sk in segment_keys]):\n",
    "                    extracted[f'{key}_{seg_num}'] = value\n",
    "\n",
    "        if self.rmt_config['sum_loss']:\n",
    "            losses = [out['loss'] for out in model_outputs]\n",
    "            extracted['loss'] = torch.stack(losses).mean(dim=0)\n",
    "\n",
    "        for key, value in extracted.items():\n",
    "            rmt_out[key] = value\n",
    "        \n",
    "        # drop unnecessary hiddens to save memory\n",
    "        if not output_hidden_states:\n",
    "            for key in rmt_out.keys():\n",
    "                if 'hidden_state' in key:\n",
    "                    rmt_out[key] = None\n",
    "\n",
    "        return rmt_out \n",
    "        \n",
    "    def get_token_type_ids(self, tensor):\n",
    "        return torch.zeros_like(tensor)\n",
    "\n",
    "    def get_attention_mask(self, tensor):\n",
    "        mask = torch.ones_like(tensor)\n",
    "        mask[tensor == self.pad_token_id] = 0\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# from .base import RMTBaseModel\n",
    "\n",
    "class RMTEncoderForSequenceClassification(RMTBaseModel):\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\n",
    "                inputs_embeds=None, labels=None, output_attentions=None, output_hidden_states=None, return_dict=None):\n",
    "        kwargs = {'attention_mask': attention_mask, 'token_type_ids': token_type_ids,\n",
    "                  'position_ids': position_ids, 'inputs_embeds': inputs_embeds,\n",
    "                  'labels': labels, 'output_attentions': output_attentions,\n",
    "                  'output_hidden_states': output_hidden_states, 'return_dict': return_dict,\n",
    "                  }\n",
    "\n",
    "        memory = self.set_memory(input_ids.shape)\n",
    "        segmented = self.pad_and_segment(input_ids)\n",
    "\n",
    "        base_model_outputs = []\n",
    "        for seg_num, segment_input_ids in enumerate(segmented):                \n",
    "            if self.rmt_config['bptt_depth'] != -1:\n",
    "                raise NotImplementedError\n",
    "\n",
    "            seg_kwargs, non_empty_mask = self.prepare_kwargs(segment_input_ids, kwargs)\n",
    "            if sum(non_empty_mask) == 0:\n",
    "                continue\n",
    "            \n",
    "            seg_kwargs['inputs_embeds'][:, self.memory_position] = memory[non_empty_mask]\n",
    "            out = self.model(**seg_kwargs)\n",
    "            base_model_outputs.append(out)\n",
    "            \n",
    "            memory[non_empty_mask] = out.hidden_states[-1][:, self.memory_position]\n",
    "\n",
    "        out = self.process_outputs(base_model_outputs, output_attentions, output_hidden_states)\n",
    "        return out\n",
    "\n",
    "    def pad_add_special_tokens(self, tensor, segment_size):\n",
    "        input_elements = []\n",
    "        input_elements += [self.cls_token, self.mem_token_ids, self.sep_token, tensor, self.sep_token]\n",
    "        tensor = torch.cat(input_elements)\n",
    "\n",
    "        pad_size = segment_size - tensor.shape[0]\n",
    "        if pad_size > 0:\n",
    "            tensor = F.pad(tensor, (0, pad_size))\n",
    "        return tensor\n",
    "\n",
    "import copy\n",
    "import types\n",
    "class RMTEncoderMemoryLayers(RMTEncoderForSequenceClassification):\n",
    "    def set_params(self, num_mem_tokens, tokenizer, **rmt_config):\n",
    "        super().set_params(num_mem_tokens, tokenizer, **rmt_config)\n",
    "        self.add_memory_layers()\n",
    "        if self.rmt_config.get('memory_layers') is not None:\n",
    "            self.override_encoder_forward(rmt_config.get('memory_forward_func'))\n",
    "\n",
    "    def override_encoder_forward(self, memory_forward_func):\n",
    "        if memory_forward_func is None:\n",
    "            from rmt_utils.encoder.memory_layers import memory_layers_forward\n",
    "            memory_forward_func = memory_layers_forward\n",
    "        encoder_forward = lambda *args, **kwargs: memory_forward_func(*args, **kwargs, rmt_parent=self)\n",
    "        self.model.base_model.encoder.forward = types.MethodType(encoder_forward, self.model.base_model.encoder)\n",
    "\n",
    "    def add_memory_layers(self):\n",
    "        memory_layers, share_memory_layers = self.rmt_config.get('memory_layers'), self.rmt_config.get('share_memory_layers')\n",
    "        if memory_layers is None:\n",
    "            self.memory_layers = None\n",
    "        else:\n",
    "            if memory_layers == 'all':\n",
    "                memory_layers = range(len(self.model.base_model.encoder.layer))\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "                \n",
    "            if share_memory_layers:\n",
    "                memory_layer = copy.deepcopy(self.model.base_model.encoder.layer[0])\n",
    "                self.memory_layers = [memory_layer for _ in range(len(memory_layers))]\n",
    "                for n, p in memory_layer.named_parameters():\n",
    "                    param_name = re.sub('\\.', '_', f'memory_{n}')\n",
    "                    self.register_parameter(param_name, p)\n",
    "            else:\n",
    "                self.memory_layers = [copy.deepcopy(self.model.base_model.encoder.layer[int(l)]) for l in memory_layers]\n",
    "                for ln, layer in enumerate(self.memory_layers):\n",
    "                    for n, p in layer.named_parameters():\n",
    "                        param_name = re.sub('\\.', '_', f'{ln}_memory_{n}')\n",
    "                        self.register_parameter(param_name, p)\n",
    "\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "from torch.nn import CrossEntropyLoss\n",
    "class RMTEncoderMLMMemLoss(RMTEncoderMemoryLayers):\n",
    "    def set_params(self, num_mem_tokens, tokenizer, **rmt_config):\n",
    "        super().set_params(num_mem_tokens, tokenizer, **rmt_config)\n",
    "        self.add_reconstruction_layers()\n",
    "\n",
    "    def add_reconstruction_layers(self):\n",
    "        self.rec_attn = copy.deepcopy(self.model.base_model.encoder.layer[-1])\n",
    "        self.rec_cls = torch.nn.Linear(self.model.config.hidden_size, self.model.config.vocab_size)\n",
    "\n",
    "        for n, p in self.rec_attn.named_parameters():\n",
    "            param_name = re.sub('\\.', '_', f'rec_attn_{n}')\n",
    "            self.register_buffer(param_name, p)\n",
    "        \n",
    "        for n, p in self.rec_cls.named_parameters():\n",
    "            param_name = re.sub('\\.', '_', f'rec_cls_{n}')\n",
    "            self.register_parameter(param_name, p)\n",
    "            \n",
    "    def segment_reconstruction_forward(self, memory_outputs, previous_input_ids):\n",
    "        mlm_prob = self.rmt_config['mlm_prob'] if 'mlm_prob' in self.rmt_config else 0.15\n",
    "        \n",
    "        input_embeddings = self.model.embeddings(previous_input_ids)\n",
    "        input_embeddings[:, self.memory_position] = memory_outputs\n",
    "\n",
    "        token_inds = list(range(self.num_mem_tokens + 2, input_embeddings.shape[1] - 1))\n",
    "        mask_inds = np.random.choice(token_inds, round(len(token_inds) * mlm_prob))\n",
    "        attention_mask = torch.ones(input_embeddings.shape[1]).to(device=input_embeddings.device)\n",
    "        attention_mask[mask_inds] = 0\n",
    "\n",
    "        rec_attn_out = self.rec_attn(input_embeddings, attention_mask=attention_mask)\n",
    "        rec_logits = self.rec_cls(rec_attn_out[0])\n",
    "\n",
    "        loss_fct = CrossEntropyLoss(ignore_index=-100)\n",
    "        reconstruction_loss = loss_fct(rec_logits.view(-1, rec_logits.size(-1)), previous_input_ids.view(-1))\n",
    "        \n",
    "        return reconstruction_loss\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\n",
    "                inputs_embeds=None, labels=None, output_attentions=None, output_hidden_states=None, return_dict=None):\n",
    "        kwargs = {'attention_mask': attention_mask, 'token_type_ids': token_type_ids,\n",
    "                  'position_ids': position_ids, 'inputs_embeds': inputs_embeds,\n",
    "                  'labels': labels, 'output_attentions': output_attentions,\n",
    "                  'output_hidden_states': output_hidden_states, 'return_dict': return_dict,\n",
    "                  }\n",
    "\n",
    "        memory = self.set_memory(input_ids.shape)\n",
    "        segmented = self.pad_and_segment(input_ids)\n",
    "\n",
    "        base_model_outputs = []\n",
    "        for seg_num, segment_input_ids in enumerate(segmented):                \n",
    "            if self.rmt_config['bptt_depth'] != -1:\n",
    "                raise NotImplementedError\n",
    "\n",
    "            seg_kwargs, non_empty_mask = self.prepare_kwargs(segment_input_ids, kwargs)\n",
    "            if sum(non_empty_mask) == 0:\n",
    "                continue\n",
    "            \n",
    "            seg_kwargs['inputs_embeds'][:, self.memory_position] = memory[non_empty_mask]\n",
    "            out = self.model(**seg_kwargs)\n",
    "            \n",
    "            memory[non_empty_mask] = out.hidden_states[-1][:, self.memory_position]\n",
    "\n",
    "            input_ids = torch.stack([s for s in segment_input_ids if s is not None])\n",
    "            segment_reconstruction_loss = self.segment_reconstruction_forward(memory[non_empty_mask], input_ids)\n",
    "            out['reconstruction_loss'] = segment_reconstruction_loss\n",
    "            base_model_outputs.append(out)\n",
    "\n",
    "        out = self.process_outputs(base_model_outputs, output_attentions, output_hidden_states)\n",
    "        return out\n",
    "\n",
    "    def process_outputs(self, model_outputs, output_attentions, output_hidden_states):\n",
    "        rmt_out = model_outputs[-1]\n",
    "\n",
    "        segment_keys = ['loss']\n",
    "        if output_attentions:\n",
    "            segment_keys.append('attentions')\n",
    "        if output_hidden_states:\n",
    "            segment_keys.append('hidden_states')\n",
    "\n",
    "        extracted = {}\n",
    "        for seg_num, out in enumerate(model_outputs):\n",
    "            for key, value in out.items():\n",
    "                if any([sk in key for sk in segment_keys]):\n",
    "                    extracted[f'{key}_{seg_num}'] = value\n",
    "\n",
    "        if self.rmt_config['sum_loss']:\n",
    "            losses = [out['loss'] for out in model_outputs]\n",
    "            extracted['loss'] = torch.stack(losses).mean()\n",
    "        else:\n",
    "            extracted['loss'] = rmt_out['loss']\n",
    "\n",
    "        reconstruction_loss = torch.stack([out['reconstruction_loss'] for out in model_outputs]).mean()\n",
    "        rec_coef = self.rmt_config['reconstruction_loss_coef']\n",
    "        extracted['reconstruction_loss'] = reconstruction_loss\n",
    "        extracted['loss'] =  reconstruction_loss * rec_coef + extracted['loss'] * (1 - rec_coef)\n",
    "        \n",
    "\n",
    "        for key, value in extracted.items():\n",
    "            rmt_out[key] = value\n",
    "\n",
    "        # drop unnecessary hiddens to save memory\n",
    "        if not output_hidden_states:\n",
    "            for key in rmt_out.keys():\n",
    "                if 'hidden_state' in key:\n",
    "                    rmt_out[key] = None\n",
    "\n",
    "        return rmt_out \n",
    "\n",
    "from rmt_utils.encoder.horizontal_memory import horizontal_memory_forward\n",
    "class RMTEncoderHorizontalMemory(RMTEncoderMemoryLayers):\n",
    "    def set_params(self, num_mem_tokens, tokenizer, **rmt_config):\n",
    "        rmt_config['input_size'] -= num_mem_tokens\n",
    "        super().set_params(num_mem_tokens, tokenizer, **rmt_config)\n",
    "        self.add_memory_layers()\n",
    "        \n",
    "        memory_forward_func = rmt_config.get('memory_forward_func')\n",
    "        if memory_forward_func is None:\n",
    "            memory_forward_func = horizontal_memory_forward\n",
    "        self.override_encoder_forward(memory_forward_func)\n",
    "\n",
    "    def set_memory(self, input_shape):\n",
    "        self.memory_storage = {}\n",
    "        memory = self.model.embeddings(self.mem_token_ids)\n",
    "        memory = memory.repeat(input_shape[0], 1, 1)\n",
    "        \n",
    "        # fill layer memories \n",
    "        memory_input = self.pad_add_special_tokens(self.mem_token_ids, self.num_mem_tokens)\n",
    "        mem_out = self.model(memory_input.reshape((1, -1)))\n",
    "        \n",
    "        return memory\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\n",
    "                inputs_embeds=None, labels=None, output_attentions=None, output_hidden_states=None, return_dict=None):\n",
    "        kwargs = {'attention_mask': attention_mask, 'token_type_ids': token_type_ids,\n",
    "                  'position_ids': position_ids, 'inputs_embeds': inputs_embeds,\n",
    "                  'labels': labels, 'output_attentions': output_attentions,\n",
    "                  'output_hidden_states': output_hidden_states, 'return_dict': return_dict,\n",
    "                  }\n",
    "\n",
    "        memory = self.set_memory(input_ids.shape)\n",
    "        segmented = self.pad_and_segment(input_ids)\n",
    "\n",
    "        base_model_outputs = []\n",
    "        for seg_num, segment_input_ids in enumerate(segmented):                \n",
    "            if self.rmt_config['bptt_depth'] != -1:\n",
    "                raise NotImplementedError\n",
    "\n",
    "            seg_kwargs, non_empty_mask = self.prepare_kwargs(segment_input_ids, kwargs)\n",
    "            if sum(non_empty_mask) == 0:\n",
    "                continue\n",
    "            self.memory_storage['non_empty_mask'] = non_empty_mask\n",
    "            \n",
    "            seg_kwargs['inputs_embeds'][:, self.memory_position] = memory[non_empty_mask]\n",
    "            out = self.model(**seg_kwargs)\n",
    "            base_model_outputs.append(out)\n",
    "            \n",
    "            memory[non_empty_mask] = out.hidden_states[-1][:, self.memory_position]\n",
    "\n",
    "        out = self.process_outputs(base_model_outputs, output_attentions, output_hidden_states)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# from .base import RMTBaseModel\n",
    "\n",
    "class RMTEncoderDecoderForConditionalGeneration(RMTBaseModel):\n",
    "    def forward(self, input_ids, attention_mask=None, position_ids=None, head_mask=None,\n",
    "                inputs_embeds=None, labels=None, output_attentions=None, output_hidden_states=None, return_dict=None):\n",
    "        kwargs = {'attention_mask': attention_mask,\n",
    "                #   'position_ids': position_ids, \n",
    "                  'inputs_embeds': inputs_embeds,\n",
    "                  'labels': labels, 'output_attentions': output_attentions,\n",
    "                  'output_hidden_states': output_hidden_states, 'return_dict': return_dict,\n",
    "                  }\n",
    "\n",
    "        memory = self.set_memory(input_ids.shape)\n",
    "        segmented = self.pad_and_segment(input_ids)\n",
    "\n",
    "        base_model_outputs = []\n",
    "        for seg_num, segment_input_ids in enumerate(segmented):                \n",
    "            if self.rmt_config['bptt_depth'] != -1:\n",
    "                raise NotImplementedError\n",
    "\n",
    "            seg_kwargs, non_empty_mask = self.prepare_kwargs(segment_input_ids, kwargs)\n",
    "            if sum(non_empty_mask) == 0:\n",
    "                continue\n",
    "            \n",
    "            seg_kwargs['inputs_embeds'][:, self.memory_position] = memory[non_empty_mask]\n",
    "            out = self.model(**seg_kwargs)\n",
    "            base_model_outputs.append(out)\n",
    "            \n",
    "            memory[non_empty_mask] = out.encoder_hidden_states[-1][:, self.memory_position]\n",
    "\n",
    "        out = self.process_outputs(base_model_outputs, output_attentions, output_hidden_states)\n",
    "        return out\n",
    "\n",
    "    def generate(self, input_ids, attention_mask=None, position_ids=None, head_mask=None,\n",
    "                inputs_embeds=None, output_attentions=None, output_hidden_states=None, return_dict=None,\n",
    "                min_length=None, max_length=None):\n",
    "        kwargs = {'attention_mask': attention_mask,\n",
    "                  'inputs_embeds': inputs_embeds,\n",
    "                  'output_attentions': output_attentions,\n",
    "                  'output_hidden_states': output_hidden_states, 'return_dict': return_dict,\n",
    "                  'min_length': min_length, 'max_length': max_length\n",
    "                  }\n",
    "\n",
    "        memory = self.set_memory(input_ids.shape)\n",
    "        segmented = self.pad_and_segment(input_ids)\n",
    "\n",
    "        base_model_outputs = []\n",
    "        for seg_num, segment_input_ids in enumerate(segmented):                \n",
    "            if self.rmt_config['bptt_depth'] != -1:\n",
    "                raise NotImplementedError\n",
    "\n",
    "            seg_kwargs, non_empty_mask = self.prepare_kwargs(segment_input_ids, kwargs)\n",
    "            seg_kwargs['inputs_embeds'][:, self.memory_position] = memory[non_empty_mask]\n",
    "            if sum(non_empty_mask) == 0:\n",
    "                continue\n",
    "\n",
    "            if seg_num == len(segmented) - 1:\n",
    "                out = self.model.generate(**seg_kwargs)\n",
    "            else:\n",
    "                for param in ['min_length', 'max_length']:\n",
    "                    if param in seg_kwargs:\n",
    "                        seg_kwargs.pop(param)\n",
    "                        \n",
    "                out = self.model.encoder(**seg_kwargs)\n",
    "                memory[non_empty_mask] = out.last_hidden_state[:, self.memory_position]\n",
    "                # base_model_outputs.append(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def pad_add_special_tokens(self, tensor, segment_size):\n",
    "        input_elements = []\n",
    "        if self.bos_token is not None:\n",
    "            input_elements.append(self.bos_token)\n",
    "        input_elements += [self.mem_token_ids, tensor, self.eos_token]\n",
    "        tensor = torch.cat(input_elements)\n",
    "\n",
    "        pad_size = segment_size - tensor.shape[0]\n",
    "        if pad_size > 0:\n",
    "            tensor = F.pad(tensor, (0, pad_size))\n",
    "        return tensor\n",
    "\n",
    "\n",
    "import types\n",
    "class RMTEncoderDecoderMemoryLayers(RMTEncoderDecoderForConditionalGeneration):\n",
    "    def set_params(self, num_mem_tokens, tokenizer, **rmt_config):\n",
    "        super().set_params(num_mem_tokens, tokenizer, **rmt_config)\n",
    "        self.add_memory_layers()\n",
    "        self.override_encoder_forward(rmt_config.get('memory_forward_func'))\n",
    "\n",
    "    def override_encoder_forward(self, memory_forward_func):\n",
    "        if self.rmt_config.get('memory_layers') is None:\n",
    "            return\n",
    "        if memory_forward_func is None:\n",
    "            from rmt_utils.encoder_decoder.memory_layers import memory_layers_forward\n",
    "            memory_forward_func = memory_layers_forward\n",
    "        encoder_forward = lambda *args, **kwargs: memory_forward_func(*args, **kwargs, rmt_parent=self)\n",
    "        self.model.base_model.encoder.forward = types.MethodType(encoder_forward, self.model.base_model.encoder)\n",
    "\n",
    "    def add_memory_layers(self):\n",
    "        memory_layers, share_memory_layers = self.rmt_config.get('memory_layers'), self.rmt_config.get('share_memory_layers')\n",
    "        if memory_layers is None:\n",
    "            self.memory_layers = None\n",
    "        else:\n",
    "            if memory_layers == 'all':\n",
    "                memory_layers = range(len(self.model.encoder.block))\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "                \n",
    "            if share_memory_layers:\n",
    "                memory_layer = copy.deepcopy(self.model.encoder.block[0])\n",
    "                self.memory_layers = [memory_layer for _ in range(len(memory_layers))]\n",
    "                for n, p in memory_layer.named_parameters():\n",
    "                    param_name = re.sub('\\.', '_', f'memory_{n}')\n",
    "                    self.register_parameter(param_name, p)\n",
    "            else:\n",
    "                self.memory_layers = [copy.deepcopy(self.model.encoder.block[int(l)]) for l in memory_layers]\n",
    "                for ln, layer in enumerate(self.memory_layers):\n",
    "                    for n, p in layer.named_parameters():\n",
    "                        param_name = re.sub('\\.', '_', f'{ln}_memory_{n}')\n",
    "                        self.register_parameter(param_name, p)\n",
    "\n",
    "\n",
    "import copy\n",
    "import re\n",
    "from rmt_utils.encoder_decoder.horizontal_memory import horizontal_memory_forward\n",
    "class RMTEncoderDecoderHorizontalMemory(RMTEncoderDecoderMemoryLayers):\n",
    "    def set_params(self, num_mem_tokens, tokenizer, **rmt_config):\n",
    "        super().set_params(num_mem_tokens, tokenizer, **rmt_config)\n",
    "        self.add_memory_layers()\n",
    "        \n",
    "        memory_forward_func = rmt_config.get('memory_forward_func')\n",
    "        if not memory_forward_func:\n",
    "            memory_forward_func = horizontal_memory_forward\n",
    "        self.override_encoder_forward(memory_forward_func)\n",
    "\n",
    "    def set_memory(self, input_shape):\n",
    "        self.memory_storage = {}\n",
    "        memory = self.model.embeddings(self.mem_token_ids)\n",
    "        memory = memory.repeat(input_shape[0], 1, 1)\n",
    "        \n",
    "        # fill layer memories \n",
    "        memory_input = self.pad_add_special_tokens(self.mem_token_ids, self.num_mem_tokens)\n",
    "        mem_out = self.model.encoder(memory_input.reshape((1, -1)))\n",
    "        \n",
    "        return memory\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, position_ids=None, head_mask=None,\n",
    "                inputs_embeds=None, labels=None, output_attentions=None, output_hidden_states=None, return_dict=None):\n",
    "        kwargs = {'attention_mask': attention_mask,\n",
    "                #   'position_ids': position_ids, \n",
    "                  'inputs_embeds': inputs_embeds,\n",
    "                  'labels': labels, 'output_attentions': output_attentions,\n",
    "                  'output_hidden_states': output_hidden_states, 'return_dict': return_dict,\n",
    "                  }\n",
    "\n",
    "        memory = self.set_memory(input_ids.shape)\n",
    "        segmented = self.pad_and_segment(input_ids)\n",
    "\n",
    "        base_model_outputs = []\n",
    "        for seg_num, segment_input_ids in enumerate(segmented):                \n",
    "            if self.rmt_config['bptt_depth'] != -1:\n",
    "                raise NotImplementedError\n",
    "\n",
    "            seg_kwargs, non_empty_mask = self.prepare_kwargs(segment_input_ids, kwargs)\n",
    "            if sum(non_empty_mask) == 0:\n",
    "                continue\n",
    "            self.memory_storage['non_empty_mask'] = non_empty_mask\n",
    "            \n",
    "            seg_kwargs['inputs_embeds'][:, self.memory_position] = memory[non_empty_mask]\n",
    "            out = self.model(**seg_kwargs)\n",
    "            base_model_outputs.append(out)\n",
    "            \n",
    "            memory[non_empty_mask] = out.encoder_hidden_states[-1][:, self.memory_position]\n",
    "\n",
    "        out = self.process_outputs(base_model_outputs, output_attentions, output_hidden_states)\n",
    "        return out\n",
    "\n",
    "    def generate(self, input_ids, attention_mask=None, position_ids=None, head_mask=None,\n",
    "                inputs_embeds=None, output_attentions=None, output_hidden_states=None, return_dict=None,\n",
    "                min_length=None, max_length=None):\n",
    "        kwargs = {'attention_mask': attention_mask,\n",
    "                  'inputs_embeds': inputs_embeds,\n",
    "                  'output_attentions': output_attentions,\n",
    "                  'output_hidden_states': output_hidden_states, 'return_dict': return_dict,\n",
    "                  'min_length': min_length, 'max_length': max_length\n",
    "                  }\n",
    "\n",
    "        memory = self.set_memory(input_ids.shape)\n",
    "        segmented = self.pad_and_segment(input_ids)\n",
    "\n",
    "        for seg_num, segment_input_ids in enumerate(segmented):                \n",
    "            if self.rmt_config['bptt_depth'] != -1:\n",
    "                raise NotImplementedError\n",
    "\n",
    "            seg_kwargs, non_empty_mask = self.prepare_kwargs(segment_input_ids, kwargs)\n",
    "            seg_kwargs['inputs_embeds'][:, self.memory_position] = memory[non_empty_mask]\n",
    "            if sum(non_empty_mask) == 0:\n",
    "                continue\n",
    "            self.memory_storage['non_empty_mask'] = non_empty_mask\n",
    "\n",
    "            if seg_num == len(segmented) - 1:\n",
    "                out = self.model.generate(**seg_kwargs)\n",
    "            else:\n",
    "                for param in ['min_length', 'max_length']:\n",
    "                    if param in seg_kwargs:\n",
    "                        seg_kwargs.pop(param)\n",
    "                        \n",
    "                out = self.model.encoder(**seg_kwargs)\n",
    "                memory[non_empty_mask] = out.last_hidden_state[:, self.memory_position]\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Sequence\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "\n",
    "def deberta_horizontal_memory_forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask,\n",
    "        output_hidden_states=True,\n",
    "        output_attentions=False,\n",
    "        query_states=None,\n",
    "        relative_pos=None,\n",
    "        return_dict=True,\n",
    "        rmt_parent=None,\n",
    "    ):\n",
    "    attention_mask = self.get_attention_mask(attention_mask)\n",
    "    relative_pos = self.get_rel_pos(hidden_states, query_states, relative_pos)\n",
    "\n",
    "    all_hidden_states = () if output_hidden_states else None\n",
    "    all_attentions = () if output_attentions else None\n",
    "\n",
    "    if isinstance(hidden_states, Sequence):\n",
    "        next_kv = hidden_states[0]\n",
    "    else:\n",
    "        next_kv = hidden_states\n",
    "    rel_embeddings = self.get_rel_embedding()\n",
    "    for i, layer_module in enumerate(self.layer):\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if i in rmt_parent.memory_storage:\n",
    "            layer_memory = rmt_parent.memory_storage[i]\n",
    "            non_empty_mask = rmt_parent.memory_storage['non_empty_mask']\n",
    "            if layer_memory.shape[0] == 1:\n",
    "                layer_memory = layer_memory.repeat(len(non_empty_mask), 1, 1)\n",
    "                \n",
    "            next_kv = torch.cat([layer_memory[non_empty_mask], next_kv], dim=1)\n",
    "            print('attention_mask0', attention_mask.shape)#, attention_mask)\n",
    "            # if attention_mask is not None:\n",
    "                # layer_attention_mask = torch.cat((attention_mask[:, :, :, :rmt_parent.num_mem_tokens], attention_mask), dim=-1)\n",
    "                # layer_attention_mask = torch.cat((layer_attention_mask[:, :, :rmt_parent.num_mem_tokens], layer_attention_mask), dim=-2)\n",
    "            # else:\n",
    "            #     layer_attention_mask = attention_mask\n",
    "        else:\n",
    "            layer_memory = None\n",
    "            layer_attention_mask = attention_mask\n",
    "\n",
    "        # if i == 0:\n",
    "        #     attention_mask = self.get_attention_mask(next_kv)\n",
    "        #     relative_pos = self.get_rel_pos(next_kv, query_states, relative_pos)\n",
    "        \n",
    "        print('next_kv', next_kv.shape)\n",
    "        print('attention_mask1', layer_attention_mask.shape)#, layer_attention_mask)\n",
    "        print('query_states', query_states)\n",
    "        print('relative_pos', relative_pos.shape)\n",
    "        print('rel_embeddings', rel_embeddings.shape)\n",
    "        hidden_states = layer_module(\n",
    "            next_kv,\n",
    "            attention_mask=layer_attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            query_states=query_states,\n",
    "            relative_pos=relative_pos,\n",
    "            rel_embeddings=rel_embeddings,\n",
    "        )\n",
    "        \n",
    "        ### Update memory\n",
    "        if rmt_parent.memory_layers is not None:\n",
    "            memory_layer = rmt_parent.memory_layers[i]\n",
    "            memory_layer_out = memory_layer(\n",
    "                next_kv,\n",
    "                attention_mask=layer_attention_mask,\n",
    "                output_attentions=output_attentions,\n",
    "                query_states=query_states,\n",
    "                relative_pos=relative_pos,\n",
    "                rel_embeddings=rel_embeddings,\n",
    "            )\n",
    "\n",
    "        if output_attentions:\n",
    "            hidden_states, att_m = hidden_states\n",
    "            memory_layer_out, memory_attentions = memory_layer_out\n",
    "        \n",
    "        ### shorten hidden states\n",
    "        if i in rmt_parent.memory_storage:\n",
    "            hidden_states = hidden_states[:, rmt_parent.num_mem_tokens:]\n",
    "\n",
    "        ### update memory \n",
    "        if rmt_parent.memory_layers is not None:\n",
    "            memory_layer_hidden_states = memory_layer_out[0]\n",
    "            if i in rmt_parent.memory_storage:\n",
    "                memory_layer_hidden_states = memory_layer_hidden_states[:, rmt_parent.num_mem_tokens:]\n",
    "\n",
    "            updated_memory = memory_layer_hidden_states[:, rmt_parent.memory_position]\n",
    "            hidden_states[:, rmt_parent.memory_position] = updated_memory\n",
    "        \n",
    "        ### set layer memory\n",
    "        if layer_memory is not None:\n",
    "            layer_memory[non_empty_mask] = hidden_states[:, rmt_parent.memory_position].detach()\n",
    "        else:\n",
    "            layer_memory = hidden_states[:, rmt_parent.memory_position].detach()\n",
    "        rmt_parent.memory_storage[i] = layer_memory\n",
    "\n",
    "        if query_states is not None:\n",
    "            query_states = hidden_states\n",
    "            if isinstance(hidden_states, Sequence):\n",
    "                next_kv = hidden_states[i + 1] if i + 1 < len(self.layer) else None\n",
    "        else:\n",
    "            next_kv = hidden_states\n",
    "\n",
    "        if output_attentions:\n",
    "            all_attentions = all_attentions + (att_m,)\n",
    "\n",
    "    if output_hidden_states:\n",
    "        all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "    if not return_dict:\n",
    "        return tuple(v for v in [hidden_states, all_hidden_states, all_attentions] if v is not None)\n",
    "    return BaseModelOutput(\n",
    "        last_hidden_state=hidden_states, hidden_states=all_hidden_states, attentions=all_attentions\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_segments = 2\n",
    "num_mem_tokens = 10\n",
    "# device = torch.device(3)\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# from rmt_utils.encoder.memory_layers import memory_layers_forward as memory_layers_func\n",
    "# from rmt_utils.encoder.memory_layers import deberta_memory_layers_forward as memory_layers_func\n",
    "memory_forward_func = deberta_horizontal_memory_forward\n",
    "# model_name = \"google/bert_uncased_L-4_H-256_A-4\"\n",
    "# model_name = \"microsoft/deberta-v3-base\"\n",
    "# model_name = 'google/electra-base-discriminator'\n",
    "# model_name = \"google/bert_uncased_L-4_H-256_A-4\"\n",
    "model_name = 'bert-base-cased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "rmt_config = {'num_mem_tokens': 5, \n",
    "                'max_n_segments': 3,\n",
    "               #  'segment_alignment': 'right',\n",
    "                'tokenizer': tokenizer,\n",
    "               #  'memory_layers': 'all', \n",
    "                'memory_forward_func': memory_forward_func,\n",
    "               #  'share_memory_layers': True,\n",
    "                'reconstruction_loss_coef': 0.1,\n",
    "                'segment_ordering': 'regular',\n",
    "                'input_size': 512, \n",
    "                'bptt_depth': -1, \n",
    "                'sum_loss': False,\n",
    "             }\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "# rmt = RMTEncoderForSequenceClassification(base_model, **rmt_config)\n",
    "# rmt = RMTEncoderMemoryLayers(base_model, **rmt_config)\n",
    "# rmt = RMTEncoderMLMMemLoss(base_model, **rmt_config)\n",
    "# rmt = RMTEncoderHorizontalMemory(base_model, **rmt_config)\n",
    "# rmt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpt = torch.load('../../runs/framework/contract_nli/bert-base-cased/lr1e-05_linear_adamw_wd1e-03_1024-512-{1}seg_mem0_bs32_iters5000_regular/run_4/model_best.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cpt['model_state_dict'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_token\n",
      "sep_token\n",
      "mem_token_ids\n"
     ]
    }
   ],
   "source": [
    "for key, value in cpt['model_state_dict'].items():\n",
    "    if hasattr(rmt, key):\n",
    "        print(key)\n",
    "        setattr(rmt, key, value)\n",
    "    # print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_params = [n for n,p in base_model.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28996, 768])\n"
     ]
    }
   ],
   "source": [
    "drop_keys = { \"cls_token\", \"sep_token\", \"mem_token_ids\", \"embeddings.weight\"}\n",
    "fixed_state_dict = {}\n",
    "for key, value in cpt['model_state_dict'].items():\n",
    "    if 'model' in key:\n",
    "        key = key.split('model.')[1]\n",
    "    if 'embeddings.word_embeddings' in key:\n",
    "        print(value.shape) \n",
    "    if key not in drop_keys:\n",
    "        fixed_state_dict[key] = value\n",
    "\n",
    "\n",
    "#         if hasattr(rmt.model, model_key):\n",
    "#             print(model_key)\n",
    "#             setattr(rmt.model, model_key, value)\n",
    "#             print(key)\n",
    "# # rmt.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.load_state_dict(fixed_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rmt_utils.encoder_decoder.horizontal_memory import horizontal_memory_forward as memory_forward_func\n",
    "# # model_name = \"facebook/bart-base\"\n",
    "# model_name = 't5-small'\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# rmt_config = {'num_mem_tokens': 5, \n",
    "#                 'max_n_segments': 3,\n",
    "#                #  'segment_alignment': 'right',\n",
    "#                 'tokenizer': tokenizer,\n",
    "#                 'memory_layers': 'all', \n",
    "#                #  'memory_forward_func': memory_layers_func,\n",
    "#                 'share_memory_layers': True,\n",
    "#                 'reconstruction_loss_coef': 0.1,\n",
    "#                 'segment_ordering': 'regular',\n",
    "#                 'input_size': 512, \n",
    "#                 'bptt_depth': -1, \n",
    "#                 'sum_loss': False,\n",
    "#              }\n",
    "\n",
    "# base_model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# # rmt = RMTEncoderDecoderForConditionalGeneration(base_model, **rmt_config)\n",
    "# # rmt = RMTEncoderDecoderMemoryLoss(base_model, **rmt_config) # does not work\n",
    "# # rmt = RMTEncoderDecoderMemoryLayers(base_model, **rmt_config)\n",
    "# rmt = RMTEncoderDecoderHorizontalMemory(base_model, **rmt_config)\n",
    "# # rmt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_kv torch.Size([1, 13, 768])\n",
      "attention_mask1 torch.Size([1, 1, 13, 13])\n",
      "query_states None\n",
      "relative_pos torch.Size([1, 13, 13])\n",
      "rel_embeddings torch.Size([512, 768])\n",
      "next_kv torch.Size([1, 13, 768])\n",
      "attention_mask1 torch.Size([1, 1, 13, 13])\n",
      "query_states None\n",
      "relative_pos torch.Size([1, 13, 13])\n",
      "rel_embeddings torch.Size([512, 768])\n",
      "next_kv torch.Size([1, 13, 768])\n",
      "attention_mask1 torch.Size([1, 1, 13, 13])\n",
      "query_states None\n",
      "relative_pos torch.Size([1, 13, 13])\n",
      "rel_embeddings torch.Size([512, 768])\n",
      "next_kv torch.Size([1, 13, 768])\n",
      "attention_mask1 torch.Size([1, 1, 13, 13])\n",
      "query_states None\n",
      "relative_pos torch.Size([1, 13, 13])\n",
      "rel_embeddings torch.Size([512, 768])\n",
      "next_kv torch.Size([1, 13, 768])\n",
      "attention_mask1 torch.Size([1, 1, 13, 13])\n",
      "query_states None\n",
      "relative_pos torch.Size([1, 13, 13])\n",
      "rel_embeddings torch.Size([512, 768])\n",
      "next_kv torch.Size([1, 13, 768])\n",
      "attention_mask1 torch.Size([1, 1, 13, 13])\n",
      "query_states None\n",
      "relative_pos torch.Size([1, 13, 13])\n",
      "rel_embeddings torch.Size([512, 768])\n",
      "next_kv torch.Size([1, 13, 768])\n",
      "attention_mask1 torch.Size([1, 1, 13, 13])\n",
      "query_states None\n",
      "relative_pos torch.Size([1, 13, 13])\n",
      "rel_embeddings torch.Size([512, 768])\n",
      "next_kv torch.Size([1, 13, 768])\n",
      "attention_mask1 torch.Size([1, 1, 13, 13])\n",
      "query_states None\n",
      "relative_pos torch.Size([1, 13, 13])\n",
      "rel_embeddings torch.Size([512, 768])\n",
      "next_kv torch.Size([1, 13, 768])\n",
      "attention_mask1 torch.Size([1, 1, 13, 13])\n",
      "query_states None\n",
      "relative_pos torch.Size([1, 13, 13])\n",
      "rel_embeddings torch.Size([512, 768])\n",
      "next_kv torch.Size([1, 13, 768])\n",
      "attention_mask1 torch.Size([1, 1, 13, 13])\n",
      "query_states None\n",
      "relative_pos torch.Size([1, 13, 13])\n",
      "rel_embeddings torch.Size([512, 768])\n",
      "next_kv torch.Size([1, 13, 768])\n",
      "attention_mask1 torch.Size([1, 1, 13, 13])\n",
      "query_states None\n",
      "relative_pos torch.Size([1, 13, 13])\n",
      "rel_embeddings torch.Size([512, 768])\n",
      "next_kv torch.Size([1, 13, 768])\n",
      "attention_mask1 torch.Size([1, 1, 13, 13])\n",
      "query_states None\n",
      "relative_pos torch.Size([1, 13, 13])\n",
      "rel_embeddings torch.Size([512, 768])\n",
      "attention_mask0 torch.Size([2, 1, 507, 507])\n",
      "next_kv torch.Size([2, 512, 768])\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'layer_attention_mask' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m out \u001b[39m=\u001b[39m rmt(sample_input_ids, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs, output_hidden_states\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, output_attentions\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m out\u001b[39m.\u001b[39mkeys()\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb Cell 12\u001b[0m in \u001b[0;36mRMTEncoderHorizontalMemory.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=236'>237</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory_storage[\u001b[39m'\u001b[39m\u001b[39mnon_empty_mask\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m non_empty_mask\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=238'>239</a>\u001b[0m seg_kwargs[\u001b[39m'\u001b[39m\u001b[39minputs_embeds\u001b[39m\u001b[39m'\u001b[39m][:, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory_position] \u001b[39m=\u001b[39m memory[non_empty_mask]\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=239'>240</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mseg_kwargs)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=240'>241</a>\u001b[0m base_model_outputs\u001b[39m.\u001b[39mappend(out)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=242'>243</a>\u001b[0m memory[non_empty_mask] \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mhidden_states[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][:, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory_position]\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:1295\u001b[0m, in \u001b[0;36mDebertaV2ForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1287\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1288\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1289\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1290\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1295\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdeberta(\n\u001b[1;32m   1296\u001b[0m     input_ids,\n\u001b[1;32m   1297\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1298\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1299\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1300\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1301\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1302\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1303\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1304\u001b[0m )\n\u001b[1;32m   1306\u001b[0m encoder_layer \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1307\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(encoder_layer)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:1066\u001b[0m, in \u001b[0;36mDebertaV2Model.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1056\u001b[0m     token_type_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(input_shape, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m   1058\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m   1059\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1060\u001b[0m     token_type_ids\u001b[39m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1063\u001b[0m     inputs_embeds\u001b[39m=\u001b[39minputs_embeds,\n\u001b[1;32m   1064\u001b[0m )\n\u001b[0;32m-> 1066\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1067\u001b[0m     embedding_output,\n\u001b[1;32m   1068\u001b[0m     attention_mask,\n\u001b[1;32m   1069\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1070\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1071\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1072\u001b[0m )\n\u001b[1;32m   1073\u001b[0m encoded_layers \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m1\u001b[39m]\n\u001b[1;32m   1075\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mz_steps \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb Cell 12\u001b[0m in \u001b[0;36mRMTEncoderMemoryLayers.override_encoder_forward.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mrmt_utils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mencoder\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmemory_layers\u001b[39;00m \u001b[39mimport\u001b[39;00m memory_layers_forward\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m     memory_forward_func \u001b[39m=\u001b[39m memory_layers_forward\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m encoder_forward \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: memory_forward_func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs, rmt_parent\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mbase_model\u001b[39m.\u001b[39mencoder\u001b[39m.\u001b[39mforward \u001b[39m=\u001b[39m types\u001b[39m.\u001b[39mMethodType(encoder_forward, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mbase_model\u001b[39m.\u001b[39mencoder)\n",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb Cell 12\u001b[0m in \u001b[0;36mdeberta_horizontal_memory_forward\u001b[0;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict, rmt_parent)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39m# if i == 0:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39m#     attention_mask = self.get_attention_mask(next_kv)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39m#     relative_pos = self.get_rel_pos(next_kv, query_states, relative_pos)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mnext_kv\u001b[39m\u001b[39m'\u001b[39m, next_kv\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mattention_mask1\u001b[39m\u001b[39m'\u001b[39m, layer_attention_mask\u001b[39m.\u001b[39mshape)\u001b[39m#, layer_attention_mask)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mquery_states\u001b[39m\u001b[39m'\u001b[39m, query_states)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mrelative_pos\u001b[39m\u001b[39m'\u001b[39m, relative_pos\u001b[39m.\u001b[39mshape)\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'layer_attention_mask' referenced before assignment"
     ]
    }
   ],
   "source": [
    "out = rmt(sample_input_ids, **kwargs, output_hidden_states=False, output_attentions=False)\n",
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmt.memory_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 32099, 32131, 32131, 32131, 32131, 32131, 32131, 32131, 32131,\n",
       "         32131, 32131, 32131, 32131, 32131, 32131, 32131, 32131, 32131, 32131],\n",
       "        [    0, 32099, 32128, 32131, 32128, 32131, 32128, 32131, 32128, 32131,\n",
       "         32128, 32131, 32128, 32131, 32128, 32131, 32128, 32131, 32128, 32131]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmt.generate(sample_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmt.memory_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:1320: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  label_index = (labels >= 0).nonzero()\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m out \u001b[39m=\u001b[39m rmt(sample_input_ids, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs, output_hidden_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, output_attentions\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m out\u001b[39m.\u001b[39mkeys()\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb Cell 17\u001b[0m in \u001b[0;36mRMTEncoderMLMMemLoss.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=146'>147</a>\u001b[0m memory[non_empty_mask] \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mhidden_states[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][:, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory_position]\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=148'>149</a>\u001b[0m input_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([s \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m segment_input_ids \u001b[39mif\u001b[39;00m s \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m])\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=149'>150</a>\u001b[0m segment_reconstruction_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msegment_reconstruction_forward(memory[non_empty_mask], input_ids)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=150'>151</a>\u001b[0m out[\u001b[39m'\u001b[39m\u001b[39mreconstruction_loss\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m segment_reconstruction_loss\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=151'>152</a>\u001b[0m base_model_outputs\u001b[39m.\u001b[39mappend(out)\n",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb Cell 17\u001b[0m in \u001b[0;36mRMTEncoderMLMMemLoss.segment_reconstruction_forward\u001b[0;34m(self, memory_outputs, previous_input_ids)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=112'>113</a>\u001b[0m attention_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones(input_embeddings\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39minput_embeddings\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=113'>114</a>\u001b[0m attention_mask[mask_inds] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=115'>116</a>\u001b[0m rec_attn_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrec_attn(input_embeddings, attention_mask\u001b[39m=\u001b[39;49mattention_mask)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=116'>117</a>\u001b[0m rec_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrec_cls(rec_attn_out[\u001b[39m0\u001b[39m])\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=118'>119</a>\u001b[0m loss_fct \u001b[39m=\u001b[39m CrossEntropyLoss(ignore_index\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m100\u001b[39m)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:341\u001b[0m, in \u001b[0;36mDebertaV2Layer.forward\u001b[0;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    333\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    334\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    339\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    340\u001b[0m ):\n\u001b[0;32m--> 341\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    342\u001b[0m         hidden_states,\n\u001b[1;32m    343\u001b[0m         attention_mask,\n\u001b[1;32m    344\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    345\u001b[0m         query_states\u001b[39m=\u001b[39;49mquery_states,\n\u001b[1;32m    346\u001b[0m         relative_pos\u001b[39m=\u001b[39;49mrelative_pos,\n\u001b[1;32m    347\u001b[0m         rel_embeddings\u001b[39m=\u001b[39;49mrel_embeddings,\n\u001b[1;32m    348\u001b[0m     )\n\u001b[1;32m    349\u001b[0m     \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[1;32m    350\u001b[0m         attention_output, att_matrix \u001b[39m=\u001b[39m attention_output\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:272\u001b[0m, in \u001b[0;36mDebertaV2Attention.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    264\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    265\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    270\u001b[0m     rel_embeddings\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    271\u001b[0m ):\n\u001b[0;32m--> 272\u001b[0m     self_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    273\u001b[0m         hidden_states,\n\u001b[1;32m    274\u001b[0m         attention_mask,\n\u001b[1;32m    275\u001b[0m         output_attentions,\n\u001b[1;32m    276\u001b[0m         query_states\u001b[39m=\u001b[39;49mquery_states,\n\u001b[1;32m    277\u001b[0m         relative_pos\u001b[39m=\u001b[39;49mrelative_pos,\n\u001b[1;32m    278\u001b[0m         rel_embeddings\u001b[39m=\u001b[39;49mrel_embeddings,\n\u001b[1;32m    279\u001b[0m     )\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[1;32m    281\u001b[0m         self_output, att_matrix \u001b[39m=\u001b[39m self_output\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:699\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelative_attention:\n\u001b[1;32m    698\u001b[0m     rel_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_dropout(rel_embeddings)\n\u001b[0;32m--> 699\u001b[0m     rel_att \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdisentangled_attention_bias(\n\u001b[1;32m    700\u001b[0m         query_layer, key_layer, relative_pos, rel_embeddings, scale_factor\n\u001b[1;32m    701\u001b[0m     )\n\u001b[1;32m    703\u001b[0m \u001b[39mif\u001b[39;00m rel_att \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     attention_scores \u001b[39m=\u001b[39m attention_scores \u001b[39m+\u001b[39m rel_att\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:745\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.disentangled_attention_bias\u001b[0;34m(self, query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)\u001b[0m\n\u001b[1;32m    742\u001b[0m att_span \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_ebd_size\n\u001b[1;32m    743\u001b[0m relative_pos \u001b[39m=\u001b[39m relative_pos\u001b[39m.\u001b[39mlong()\u001b[39m.\u001b[39mto(query_layer\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 745\u001b[0m rel_embeddings \u001b[39m=\u001b[39m rel_embeddings[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpos_ebd_size \u001b[39m-\u001b[39;49m att_span : \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpos_ebd_size \u001b[39m+\u001b[39;49m att_span, :]\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m    746\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshare_att_key:\n\u001b[1;32m    747\u001b[0m     pos_query_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\n\u001b[1;32m    748\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquery_proj(rel_embeddings), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_attention_heads\n\u001b[1;32m    749\u001b[0m     )\u001b[39m.\u001b[39mrepeat(query_layer\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_attention_heads, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "out = rmt(sample_input_ids, **kwargs, output_hidden_states=True, output_attentions=True)\n",
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1291, -0.0159, -0.0419,  ..., -0.0337, -0.2189, -0.0095],\n",
       "        [-0.0350, -0.0167,  0.0482,  ..., -0.1350, -0.2535, -0.0246],\n",
       "        [-0.0531,  0.0108, -0.0384,  ..., -0.1367, -0.1615, -0.0312],\n",
       "        ...,\n",
       "        [-0.1999,  0.0540, -0.0645,  ..., -0.1501, -0.1413, -0.0326],\n",
       "        [-0.2171,  0.0586, -0.0722,  ..., -0.1467, -0.1235, -0.0330],\n",
       "        [-0.2195,  0.0811, -0.0253,  ..., -0.0988, -0.1056, -0.0275]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmt.model.base_model.encoder.get_rel_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory_input = self.pad_add_special_tokens(self.mem_token_ids, self.input_size)[:self.num_mem_tokens + ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mem_out = self.model.base_model(memory_input.reshape((1, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_input_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m sample_input_ids\u001b[39m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_input_ids' is not defined"
     ]
    }
   ],
   "source": [
    "sample_input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['loss', 'logits', 'hidden_states', 'loss_0', 'loss_1', 'loss_2'])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = rmt(sample_input_ids, **kwargs, output_hidden_states=False, output_attentions=False)\n",
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segmented = rmt.pad_and_segment(sample_input_ids[:, :600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n, p in rmt.named_parameters():\n",
    "#     if 'memory' in n:\n",
    "#         print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(rmt.memory_decoders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n, p in rmt.named_parameters():\n",
    "#     print(n, p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Holder:\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq_len = 1536\n",
    "target_seq_len = 1024\n",
    "batch_size = 2\n",
    "\n",
    "args = Holder\n",
    "args.target_seq_len = target_seq_len\n",
    "args.input_seq_len = input_seq_len\n",
    "args.input_prefix = ''\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_attention_first_token = False  # should be True for LED\n",
    "encode_plus_kwargs = {'truncation': True, 'padding': 'longest', 'pad_to_multiple_of': 1}\n",
    "# generate_kwargs = {'max_length': args.target_seq_len, 'min_length': args.target_seq_len}\n",
    "generate_kwargs = {}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # cut too long strings because they may slow down tokenization\n",
    "    inputs = [b['input'][:args.input_seq_len * 10] for b in batch]\n",
    "    if 'outputs' in batch[0]:\n",
    "        # if we have more than 1 label per example (only in valid) take only one of them\n",
    "        # to compute loss on valid\n",
    "        labels = [b['outputs'][0][:args.target_seq_len * 10] for b in batch]\n",
    "    else:\n",
    "        labels = [b['output'][:args.target_seq_len * 10] for b in batch]\n",
    "    if args.input_prefix:\n",
    "        inputs = [args.input_prefix + inp for inp in inputs]\n",
    "    features = tokenizer.batch_encode_plus(list(inputs), max_length=args.input_seq_len, return_tensors='pt',\n",
    "                                           **encode_plus_kwargs)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer.batch_encode_plus(list(labels), max_length=args.target_seq_len, return_tensors='pt',\n",
    "                                             **encode_plus_kwargs).input_ids\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    features['labels'] = labels\n",
    "    features['id'] = [b['id'] for b in batch]\n",
    "    if 'outputs' in batch[0]:\n",
    "        features['target_text'] = [b['outputs'] for b in batch]\n",
    "    else:\n",
    "        features['target_text'] = [b['output'] for b in batch]\n",
    "    if 'global_attention_mask' in features:\n",
    "        raise RuntimeError('What global attention mask for Longformer and LongformerEncoder-Decoder should be?')\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset scrolls (/home/bulatov/.cache/huggingface/datasets/tau___scrolls/qasper/1.0.0/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "838c84c4ec324405a331c2e966294514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_name = 'qasper'\n",
    "dataset = datasets.load_dataset('tau/scrolls', task_name)\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "train_sampler = RandomSampler(train_dataset,)\n",
    "kwargs = {'pin_memory': True, 'num_workers': 0}\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler,\n",
    "                                collate_fn=collate_fn, **kwargs)\n",
    "\n",
    "valid_dataset = dataset['validation']\n",
    "valid_sampler = RandomSampler(valid_dataset)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, sampler=train_sampler,\n",
    "                                collate_fn=collate_fn, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = iter(train_dataloader)\n",
    "sample = next(gen)\n",
    "\n",
    "if 'id' in sample:\n",
    "    id = sample.pop('id')\n",
    "if 'target_text' in sample:\n",
    "    tgt_text = sample.pop('target_text')\n",
    "\n",
    "rmt.to(device)\n",
    "for k in sample:\n",
    "    sample[k] = sample[k].to(device)\n",
    "    \n",
    "sample_input_ids = sample.pop('input_ids').to(device)\n",
    "kwargs = sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq_len = 1536\n",
    "target_seq_len = 3\n",
    "batch_size = 2\n",
    "\n",
    "args = Holder\n",
    "args.target_seq_len = target_seq_len\n",
    "args.input_seq_len = input_seq_len\n",
    "args.input_prefix = ''\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_plus_kwargs = {'max_length': args.input_seq_len,\n",
    "                        'truncation': True,\n",
    "                        'padding': 'longest',\n",
    "                        'pad_to_multiple_of': 1}\n",
    "generate_kwargs = {}\n",
    "labels_map = {'Contradiction': 0, 'Entailment': 1, 'Not mentioned': 2}\n",
    "num_labels = len(labels_map)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # cut too long strings because they may slow down tokenization\n",
    "    inputs = [b['input'][:args.input_seq_len * 10] for b in batch]\n",
    "    labels = [b['output'][:args.target_seq_len * 10] for b in batch]\n",
    "    if args.input_prefix:\n",
    "        inputs = [args.input_prefix + inp for inp in inputs]\n",
    "    features = tokenizer.batch_encode_plus(list(inputs), return_tensors='pt', **encode_plus_kwargs)\n",
    "    labels = np.array([labels_map[t] for t in labels])\n",
    "    features['labels'] = torch.from_numpy(labels)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset scrolls (/home/bulatov/.cache/huggingface/datasets/tau___scrolls/contract_nli/1.0.0/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6836211cf5cc41f891af03a7fd1f421c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_name = 'contract_nli'\n",
    "dataset = datasets.load_dataset('tau/scrolls', task_name)\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "train_sampler = RandomSampler(train_dataset,)\n",
    "kwargs = {'pin_memory': True, 'num_workers': 0}\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler,\n",
    "                                collate_fn=collate_fn, **kwargs)\n",
    "\n",
    "valid_dataset = dataset['validation']\n",
    "valid_sampler = RandomSampler(valid_dataset)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, sampler=train_sampler,\n",
    "                                collate_fn=collate_fn, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = iter(train_dataloader)\n",
    "sample = next(gen)\n",
    "\n",
    "if 'id' in sample:\n",
    "    id = sample.pop('id')\n",
    "if 'target_text' in sample:\n",
    "    tgt_text = sample.pop('target_text')\n",
    "\n",
    "rmt.to(device)\n",
    "for k in sample:\n",
    "    sample[k] = sample[k].to(device)\n",
    "    \n",
    "sample_input_ids = sample.pop('input_ids').to(device)\n",
    "kwargs = sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleAttributeError",
     "evalue": "'BertForSequenceClassification' object has no attribute 'encoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb Cell 37\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m out \u001b[39m=\u001b[39m rmt(sample_input_ids, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs, output_hidden_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, output_attentions \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m out\u001b[39m.\u001b[39mkeys()\n",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb Cell 37\u001b[0m in \u001b[0;36mRMTEncoderHorizontalMemory.__call__\u001b[0;34m(self, input_ids, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     memory \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mset_memory()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     memory \u001b[39m=\u001b[39m memory\u001b[39m.\u001b[39mrepeat(input_ids\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m     segmented \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad_and_segment(input_ids)\n",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb Cell 37\u001b[0m in \u001b[0;36mRMTEncoderHorizontalMemory.set_memory\u001b[0;34m(self, memory)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# fill layer memories \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m memory_input \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad_add_special_tokens(mem_token_ids, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_mem_tokens)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m mem_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mencoder(memory_input\u001b[39m.\u001b[39mreshape((\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mreturn\u001b[39;00m memory\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/nn/modules/module.py:778\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m    777\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m--> 778\u001b[0m \u001b[39mraise\u001b[39;00m ModuleAttributeError(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    779\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mModuleAttributeError\u001b[0m: 'BertForSequenceClassification' object has no attribute 'encoder'"
     ]
    }
   ],
   "source": [
    "out = rmt(sample_input_ids, **kwargs, output_hidden_states=True, output_attentions = True)\n",
    "out.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### replace forward signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect \n",
    "from functools import wraps\n",
    "\n",
    "model_name = \"google/bert_uncased_L-4_H-256_A-4\"\n",
    "\n",
    "def decorate(func, source):\n",
    "    @wraps(source)\n",
    "    def decorated(*args, **kwargs):\n",
    "        return func(*args, **kwargs)\n",
    "    return decorated\n",
    "\n",
    "class RMT(torch.nn.Module):\n",
    "    def __init__(self, base_model, **rmt_kwargs):\n",
    "        super().__init__()\n",
    "        self.model = base_model\n",
    "        self.forward = decorate(self.forward, self.model.forward)\n",
    "\n",
    "    def forward(self, new_rmt_arg, input_ids, **kwargs):\n",
    "        pass\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "rmt = RMT(base_model, **rmt_config)\n",
    "inspect.signature(rmt.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bert_uncased_L-4_H-256_A-4 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-4_H-256_A-4 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Signature (input_ids, **kwargs)>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import wraps\n",
    "\n",
    "def wrap_func(method):\n",
    "    @wraps(method)\n",
    "    def _impl(self, *method_args, **method_kwargs):\n",
    "        method_output = method(self, *method_args, **method_kwargs)\n",
    "        return method_output\n",
    "    return _impl\n",
    "    # return \n",
    "\n",
    "class RMTEncoderForSequenceClassification(RMTBaseModel):\n",
    "    def __init__(self, base_model, **rmt_kwargs):\n",
    "        super().__init__(base_model, **rmt_kwargs)\n",
    "\n",
    "    @wrap_func\n",
    "    def forward(self, input_ids, **kwargs):\n",
    "        memory = self.set_memory()\n",
    "        memory = memory.repeat(input_ids.shape[0], 1, 1)\n",
    "        segmented = self.pad_and_segment(input_ids)\n",
    "\n",
    "        losses = []\n",
    "        for seg_num, segment_input_ids in enumerate(segmented):                \n",
    "            if (self.rmt_config['bptt_depth'] > -1) and (len(segmented) - seg_num > self.rmt_config['bptt_depth']): \n",
    "                memory = memory.detach()\n",
    "\n",
    "            seg_kwargs = dict(**kwargs)\n",
    "            seg_kwargs['output_hidden_states'] = True\n",
    "\n",
    "            non_empty_mask = [s is not None for s in segment_input_ids]\n",
    "            if sum(non_empty_mask) == 0:\n",
    "                continue\n",
    "            input_ids = torch.stack([s for s in segment_input_ids if s is not None])\n",
    "            attention_mask = self.get_attention_mask(input_ids)\n",
    "            token_type_ids = self.get_token_type_ids(input_ids)\n",
    "            seg_kwargs['labels'] = seg_kwargs['labels'][non_empty_mask]\n",
    "\n",
    "            inputs_embeds = self.model.embeddings(input_ids)\n",
    "            inputs_embeds[:, self.memory_position] = memory[non_empty_mask]\n",
    "\n",
    "            seg_kwargs['input_ids'] = None\n",
    "            seg_kwargs['inputs_embeds'] = inputs_embeds\n",
    "            seg_kwargs['attention_mask'] = attention_mask\n",
    "            seg_kwargs['token_type_ids'] = token_type_ids\n",
    "\n",
    "            out = self.model(**seg_kwargs)\n",
    "            memory[non_empty_mask] = out.hidden_states[-1][:, self.memory_position]\n",
    "\n",
    "            losses.append(out['loss'])\n",
    "\n",
    "        # drop unnecessary hiddens to save memory\n",
    "        if not kwargs.get('output_hidden_states'):\n",
    "            for key in out.keys():\n",
    "                if 'hidden_state' in key:\n",
    "                    out[key] = None\n",
    "\n",
    "        for i, l in enumerate(losses):\n",
    "            out[f'loss_{i}'] = l.mean()\n",
    "\n",
    "        if self.rmt_config['sum_loss']:\n",
    "            out['loss'] = torch.stack(losses).sum(dim=0)\n",
    "\n",
    "        return out\n",
    "        \n",
    "    def pad_add_special_tokens(self, tensor, segment_size):\n",
    "        input_elements = []\n",
    "        input_elements += [self.cls_token, self.mem_token_ids, self.sep_token, tensor, self.sep_token]\n",
    "        tensor = torch.cat(input_elements)\n",
    "\n",
    "        pad_size = segment_size - tensor.shape[0]\n",
    "        if pad_size > 0:\n",
    "            tensor = F.pad(tensor, (0, pad_size))\n",
    "        return tensor\n",
    "    \n",
    "    def get_token_type_ids(self, tensor):\n",
    "        return torch.zeros_like(tensor)\n",
    "\n",
    "\n",
    "num_segments = 2\n",
    "num_mem_tokens = 10\n",
    "# device = torch.device(3)\n",
    "device = 'cpu'\n",
    "\n",
    "\n",
    "from rmt_utils.encoder.memory_layers import memory_layers_forward as memory_layers_func\n",
    "# from rmt_utils.encoder.memory_layers import deberta_memory_layers_forward as memory_layers_func\n",
    "\n",
    "model_name = \"google/bert_uncased_L-4_H-256_A-4\"\n",
    "# model_name = \"microsoft/deberta-v3-base\"\n",
    "# model_name = 'google/electra-base-discriminator'\n",
    "# model_name = \"google/bert_uncased_L-4_H-256_A-4\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "rmt_config = {'num_mem_tokens': 5, \n",
    "                'max_n_segments': 3,\n",
    "               #  'segment_alignment': 'right',\n",
    "                'tokenizer': tokenizer,\n",
    "                'memory_layers': 'all', \n",
    "                'memory_forward_func': memory_layers_func,\n",
    "                'share_memory_layers': True,\n",
    "                'reconstruction_loss_coef': 1,\n",
    "                'segment_ordering': 'regular',\n",
    "                'input_size': 512, \n",
    "                'bptt_depth': -1, \n",
    "                'sum_loss': False,\n",
    "             }\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "rmt = RMTEncoderForSequenceClassification(base_model, **rmt_config)\n",
    "# rmt = RMTEncoderMemoryLayers(base_model, **rmt_config)\n",
    "# rmt = RMTEncoderMLMMemLoss(base_model, **rmt_config)\n",
    "# rmt.to(device)\n",
    "\n",
    "import inspect\n",
    "inspect.signature(rmt.forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLM head for input decoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertLayer(\n",
       "  (attention): BertAttention(\n",
       "    (self): BertSelfAttention(\n",
       "      (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (output): BertSelfOutput(\n",
       "      (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (intermediate): BertIntermediate(\n",
       "    (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "    (intermediate_act_fn): GELUActivation()\n",
       "  )\n",
       "  (output): BertOutput(\n",
       "    (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "    (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.model.bert.encoder.layer[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.rec_attn = copy.deepcopy(self.model.base_model.encoder.layer[-1])\n",
    "self.rec_cls = torch.nn.Linear(self.model.config.hidden_size, self.model.config.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_kwargs = dict(**kwargs)\n",
    "rec_kwargs.pop('labels')\n",
    "# rec_kwargs.pop('token_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_prob = 0.15\n",
    "\n",
    "segmented = self.pad_and_segment(sample_input_ids)\n",
    "previous_input_ids = segmented[0]\n",
    "\n",
    "inputs = torch.stack(previous_input_ids)\n",
    "input_embeddings = self.model.embeddings(inputs)\n",
    "\n",
    "out = self.model(inputs_embeds=input_embeddings, output_hidden_states=True)\n",
    "memory_outputs = out['hidden_states'][-1][:, self.memory_position]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_forward(self, memory_outputs, previous_input_ids):\n",
    "    \n",
    "    inputs = torch.stack(previous_input_ids)\n",
    "    input_embeddings = self.model.embeddings(inputs)\n",
    "    input_embeddings[:, self.memory_position] = memory_outputs\n",
    "\n",
    "    token_inds = list(range(self.num_mem_tokens + 2, input_embeddings.shape[1] - 1))\n",
    "    mask_inds = np.random.choice(token_inds, round(len(token_inds) * mlm_prob))\n",
    "    attention_mask = torch.ones(input_embeddings.shape[1])\n",
    "    attention_mask[mask_inds] = 0\n",
    "\n",
    "    rec_attn_out = self.rec_attn(input_embeddings)\n",
    "    rec_logits = self.rec_cls(rec_attn_out[0])\n",
    "\n",
    "    loss_fct = CrossEntropyLoss(ignore_index=-100)\n",
    "    reconstruction_loss = loss_fct(rec_logits.view(-1, rec_logits.size(-1)), inputs.view(-1))\n",
    "    \n",
    "    return reconstruction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.4209, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruction_forward(self, memory_outputs=memory_outputs, previous_input_ids=segmented[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_inds = list(range(self.num_mem_tokens + 2, input_embeddings.shape[1] - 1))\n",
    "mask_inds = np.random.choice(token_inds, round(len(token_inds) * mlm_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = torch.ones(input_embeddings.shape[1])\n",
    "attention_mask[mask_inds] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512, 30527])"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_attn_out = self.rec_attn(input_embeddings)\n",
    "rec_logits = self.rec_cls(rec_attn_out[0])\n",
    "rec_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.4209, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fct = CrossEntropyLoss(ignore_index=-100)\n",
    "reconstruction_loss = loss_fct(rec_logits.view(-1, rec_logits.size(-1)), inputs.view(-1))\n",
    "reconstruction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512, 256])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_inds = list(range(self.num_mem_tokens + 2, input_embeddings.shape[1] - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(token_inds)\n",
    "mask_inds = token_inds[: round(len(token_inds) * mlm_prob) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_mask_inds = torch.randa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0159,  0.0027,  0.0078,  ...,  0.0175, -0.0240,  0.0109],\n",
       "         [ 0.0315, -0.0055,  0.0017,  ...,  0.0132, -0.0214, -0.0158],\n",
       "         [ 0.0016, -0.0042, -0.0412,  ...,  0.0192,  0.0082, -0.0007],\n",
       "         ...,\n",
       "         [-0.0986,  0.0014, -0.0430,  ..., -0.0016, -0.0158, -0.0046],\n",
       "         [-0.0986,  0.0014, -0.0430,  ..., -0.0016, -0.0158, -0.0046],\n",
       "         [-0.0986,  0.0014, -0.0430,  ..., -0.0016, -0.0158, -0.0046]],\n",
       "\n",
       "        [[-0.0159,  0.0027,  0.0078,  ...,  0.0175, -0.0240,  0.0109],\n",
       "         [ 0.0315, -0.0055,  0.0017,  ...,  0.0132, -0.0214, -0.0158],\n",
       "         [ 0.0016, -0.0042, -0.0412,  ...,  0.0192,  0.0082, -0.0007],\n",
       "         ...,\n",
       "         [-0.0226,  0.0497,  0.0308,  ..., -0.0470, -0.0116,  0.0216],\n",
       "         [-0.0381, -0.0252,  0.0037,  ...,  0.0464,  0.0336,  0.0329],\n",
       "         [-0.0637, -0.0239,  0.0430,  ..., -0.0894,  0.0181,  0.0181]]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1536])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_kwargs['token_type_ids'].shapea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 4909, 2283,  ...,    0,    0,    0],\n",
       "        [ 101, 4909, 2283,  ..., 2023, 3820,  102]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.nn import CrossEntropyLoss\n",
    "# # def segment_reconstruction_forward(self, segmented, hidden_states):\n",
    "\n",
    "# hidden_states = rec_kwargs['inputs_embeds']\n",
    "# previous_input_ids = segmented[-2]\n",
    "# non_empty_mask = [s is not None for s in previous_input_ids]\n",
    "# if sum(non_empty_mask) == 0:\n",
    "#     raise ValueError\n",
    "\n",
    "# previous_input_ids = torch.stack(previous_input_ids)[non_empty_mask]\n",
    "# reconstructor_input = hidden_states[non_empty_mask]\n",
    "\n",
    "# rec_attn_out = self.rec_attn(reconstructor_input)\n",
    "# rec_logits = self.rec_cls(rec_attn_out[0])\n",
    "\n",
    "# loss_fct = CrossEntropyLoss(ignore_index=-100)\n",
    "# reconstruction_loss = loss_fct(rec_logits.view(-1, rec_logits.size(-1)), previous_input_ids.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### segment to memory attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = rmt\n",
    "input_ids = sample_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory = self.set_memory()\n",
    "# memory = memory.repeat(input_ids.shape[0], 1, 1)\n",
    "# segmented = self.pad_and_segment(input_ids)\n",
    "\n",
    "# losses = {}\n",
    "# memories = []\n",
    "# inputs = []\n",
    "# non_memory_position = [i for i in range(self.rmt_config['input_size']) if i not in self.memory_position]\n",
    "\n",
    "# for seg_num, segment_input_ids in enumerate(segmented):\n",
    "#     if (self.rmt_config['bptt_depth'] > -1) and (len(segmented) - seg_num > self.rmt_config['bptt_depth']): \n",
    "#         memory = memory.detach()\n",
    "\n",
    "#     seg_kwargs = dict(**kwargs)\n",
    "#     seg_kwargs['output_hidden_states'] = True\n",
    "    \n",
    "#     non_empty_mask = [s is not None for s in segment_input_ids]\n",
    "#     if sum(non_empty_mask) == 0:\n",
    "#         continue\n",
    "#     input_ids = torch.stack(segment_input_ids)[non_empty_mask]\n",
    "#     attention_mask = self.get_attention_mask(input_ids)\n",
    "#     token_type_ids = self.get_token_type_ids(input_ids)\n",
    "#     seg_kwargs['labels'] = seg_kwargs['labels'][non_empty_mask]\n",
    "\n",
    "#     inputs_embeds = self.embeddings(input_ids)\n",
    "#     inputs_embeds[:, self.memory_position] = memory[non_empty_mask]\n",
    "\n",
    "#     seg_kwargs['inputs_embeds'] = inputs_embeds\n",
    "#     seg_kwargs['attention_mask'] = attention_mask\n",
    "        \n",
    "#     out = self.model.forward(**seg_kwargs)\n",
    "\n",
    "#     memory[non_empty_mask] = out.encoder_hidden_states[-1][:, self.memory_position]\n",
    "    \n",
    "#     memories.append(torch.clone(memory[non_empty_mask].detach()))\n",
    "#     inputs.append(out.encoder_hidden_states[-1][:, non_memory_position])\n",
    "\n",
    "#     losses[f'loss_{seg_num}'] = out['loss']\n",
    "\n",
    "# memory_out = out.encoder_last_hidden_state[:, self.memory_position]\n",
    "# reconstruction_loss = self.segment_reconstruction_forward(segmented, memory_out)\n",
    "# out['reconstruction_loss'] = reconstruction_loss\n",
    "\n",
    "# # drop unnecessary hiddens to save memory\n",
    "# # if not kwargs.get('output_hidden_states'):\n",
    "# #     for key in out.keys():\n",
    "# #         if 'hidden_state' in key:\n",
    "# #             out[key] = None\n",
    "            \n",
    "# for k, loss in losses.items():\n",
    "#     out[k] = loss\n",
    "\n",
    "# if self.rmt_config['sum_loss']:\n",
    "#     out['loss'] = torch.stack(losses).sum(dim=0)\n",
    "\n",
    "# rec_coef = self.rmt_config['reconstruction_loss_coef']\n",
    "# out['loss'] = reconstruction_loss * rec_coef + out['loss'] * (1 - rec_coef)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hvdenv",
   "language": "python",
   "name": "hvdenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "d97c21bf6e0a282ef40de70ecfbea211641441d72159ac4287fbae50906df39f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
