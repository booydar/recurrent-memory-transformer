{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import datasets\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "# from modeling_rmt import RMTEncoderForSequenceClassification\n",
    "# from modeling_rmt_enc_dec import RMTEncoderDecoderForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/cryptography/hazmat/backends/openssl/x509.py:14: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"facebook/bart-base\"\n",
    "model_name = 't5-small'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Holder:\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq_len = 1536\n",
    "target_seq_len = 1024\n",
    "batch_size = 2\n",
    "\n",
    "args = Holder\n",
    "args.target_seq_len = target_seq_len\n",
    "args.input_seq_len = input_seq_len\n",
    "args.input_prefix = ''\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_attention_first_token = False  # should be True for LED\n",
    "encode_plus_kwargs = {'truncation': True, 'padding': 'longest', 'pad_to_multiple_of': 1}\n",
    "# generate_kwargs = {'max_length': args.target_seq_len, 'min_length': args.target_seq_len}\n",
    "generate_kwargs = {}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # cut too long strings because they may slow down tokenization\n",
    "    inputs = [b['input'][:args.input_seq_len * 10] for b in batch]\n",
    "    if 'outputs' in batch[0]:\n",
    "        # if we have more than 1 label per example (only in valid) take only one of them\n",
    "        # to compute loss on valid\n",
    "        labels = [b['outputs'][0][:args.target_seq_len * 10] for b in batch]\n",
    "    else:\n",
    "        labels = [b['output'][:args.target_seq_len * 10] for b in batch]\n",
    "    if args.input_prefix:\n",
    "        inputs = [args.input_prefix + inp for inp in inputs]\n",
    "    features = tokenizer.batch_encode_plus(list(inputs), max_length=args.input_seq_len, return_tensors='pt',\n",
    "                                           **encode_plus_kwargs)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer.batch_encode_plus(list(labels), max_length=args.target_seq_len, return_tensors='pt',\n",
    "                                             **encode_plus_kwargs).input_ids\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    features['labels'] = labels\n",
    "    features['id'] = [b['id'] for b in batch]\n",
    "    if 'outputs' in batch[0]:\n",
    "        features['target_text'] = [b['outputs'] for b in batch]\n",
    "    else:\n",
    "        features['target_text'] = [b['output'] for b in batch]\n",
    "    if 'global_attention_mask' in features:\n",
    "        raise RuntimeError('What global attention mask for Longformer and LongformerEncoder-Decoder should be?')\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset scrolls (/home/bulatov/.cache/huggingface/datasets/tau___scrolls/qasper/1.0.0/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a57b5a912871483d84ede5990df841e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_name = 'qasper'\n",
    "dataset = datasets.load_dataset('tau/scrolls', task_name)\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "train_sampler = RandomSampler(train_dataset,)\n",
    "kwargs = {'pin_memory': True, 'num_workers': 0}\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler,\n",
    "                                collate_fn=collate_fn, **kwargs)\n",
    "\n",
    "valid_dataset = dataset['validation']\n",
    "valid_sampler = RandomSampler(valid_dataset)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, sampler=train_sampler,\n",
    "                                collate_fn=collate_fn, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/scrolls_eda.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu11/home/bulatov/bulatov/RMT_light/framework/notebooks/scrolls_eda.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m gen \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(train_dataloader)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu11/home/bulatov/bulatov/RMT_light/framework/notebooks/scrolls_eda.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m sample \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(gen)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu11/home/bulatov/bulatov/RMT_light/framework/notebooks/scrolls_eda.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m sample:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu11/home/bulatov/bulatov/RMT_light/framework/notebooks/scrolls_eda.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mid\u001b[39m \u001b[39m=\u001b[39m sample\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:435\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    434\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> 435\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    436\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    437\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    438\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    439\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:475\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    474\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    476\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    477\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:47\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 47\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/scrolls_eda.ipynb Cell 10\u001b[0m in \u001b[0;36mcollate_fn\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu11/home/bulatov/bulatov/RMT_light/framework/notebooks/scrolls_eda.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m features \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mbatch_encode_plus(\u001b[39mlist\u001b[39m(inputs), return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mencode_plus_kwargs)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu11/home/bulatov/bulatov/RMT_light/framework/notebooks/scrolls_eda.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([labels_map[t] \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m labels])\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu11/home/bulatov/bulatov/RMT_light/framework/notebooks/scrolls_eda.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m features[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(labels)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu11/home/bulatov/bulatov/RMT_light/framework/notebooks/scrolls_eda.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mreturn\u001b[39;00m features\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "gen = iter(train_dataloader)\n",
    "sample = next(gen)\n",
    "\n",
    "if 'id' in sample:\n",
    "    id = sample.pop('id')\n",
    "if 'target_text' in sample:\n",
    "    tgt_text = sample.pop('target_text')\n",
    "\n",
    "# rmt.to(device)\n",
    "for k in sample:\n",
    "    sample[k] = sample[k].to(device)\n",
    "    \n",
    "sample_input_ids = sample.pop('input_ids').to(device)\n",
    "kwargs = sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq_len = 1536\n",
    "target_seq_len = 3\n",
    "batch_size = 2\n",
    "\n",
    "args = Holder\n",
    "args.target_seq_len = target_seq_len\n",
    "args.input_seq_len = input_seq_len\n",
    "args.input_prefix = ''\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_plus_kwargs = {'max_length': args.input_seq_len,\n",
    "                        'truncation': True,\n",
    "                        'padding': 'longest',\n",
    "                        'pad_to_multiple_of': 1}\n",
    "generate_kwargs = {}\n",
    "labels_map = {'Contradiction': 0, 'Entailment': 1, 'Not mentioned': 2}\n",
    "num_labels = len(labels_map)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # cut too long strings because they may slow down tokenization\n",
    "    inputs = [b['input'][:args.input_seq_len * 10] for b in batch]\n",
    "    labels = [b['output'][:args.target_seq_len * 10] for b in batch]\n",
    "    if args.input_prefix:\n",
    "        inputs = [args.input_prefix + inp for inp in inputs]\n",
    "    features = tokenizer.batch_encode_plus(list(inputs), return_tensors='pt', **encode_plus_kwargs)\n",
    "    labels = np.array([labels_map[t] for t in labels])\n",
    "    features['labels'] = torch.from_numpy(labels)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset scrolls (/home/bulatov/.cache/huggingface/datasets/tau___scrolls/contract_nli/1.0.0/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d55cbba002d442d0809e527caf9243de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_name = 'contract_nli'\n",
    "dataset = datasets.load_dataset('tau/scrolls', task_name)\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "train_sampler = RandomSampler(train_dataset,)\n",
    "kwargs = {'pin_memory': True, 'num_workers': 0}\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler,\n",
    "                                collate_fn=collate_fn, **kwargs)\n",
    "\n",
    "valid_dataset = dataset['validation']\n",
    "valid_sampler = RandomSampler(valid_dataset)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, sampler=train_sampler,\n",
    "                                collate_fn=collate_fn, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = iter(train_dataloader)\n",
    "sample = next(gen)\n",
    "\n",
    "if 'id' in sample:\n",
    "    id = sample.pop('id')\n",
    "if 'target_text' in sample:\n",
    "    tgt_text = sample.pop('target_text')\n",
    "\n",
    "# rmt.to(device)\n",
    "for k in sample:\n",
    "    sample[k] = sample[k].to(device)\n",
    "    \n",
    "sample_input_ids = sample.pop('input_ids').to(device)\n",
    "kwargs = sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_outputs = [train_dataset[i]['output'] for i in range(len(train_dataset))]\n",
    "valid_outputs = [valid_dataset[i]['output'] for i in range(len(valid_dataset))]\n",
    "# test_outputs = [test_dataset[i]['output'] for i in range(len(test_dataset))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not mentioned 0.39215686274509803\n",
      "Entailment 0.4908913920178\n",
      "Contradiction 0.11695174523710193\n"
     ]
    }
   ],
   "source": [
    "for label, occ in Counter(train_outputs).items():\n",
    "    print(label, occ / len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entailment 0.5004821600771456\n",
      "Contradiction 0.09161041465766634\n",
      "Not mentioned 0.40790742526518803\n"
     ]
    }
   ],
   "source": [
    "for label, occ in Counter(valid_outputs).items():\n",
    "    print(label, occ / len(valid_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### replace forward signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect \n",
    "from functools import wraps\n",
    "\n",
    "model_name = \"google/bert_uncased_L-4_H-256_A-4\"\n",
    "\n",
    "def decorate(func, source):\n",
    "    @wraps(source)\n",
    "    def decorated(*args, **kwargs):\n",
    "        return func(*args, **kwargs)\n",
    "    return decorated\n",
    "\n",
    "class RMT(torch.nn.Module):\n",
    "    def __init__(self, base_model, **rmt_kwargs):\n",
    "        super().__init__()\n",
    "        self.model = base_model\n",
    "        self.forward = decorate(self.forward, self.model.forward)\n",
    "\n",
    "    def forward(self, new_rmt_arg, input_ids, **kwargs):\n",
    "        pass\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "rmt = RMT(base_model, **rmt_config)\n",
    "inspect.signature(rmt.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bert_uncased_L-4_H-256_A-4 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-4_H-256_A-4 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Signature (input_ids, **kwargs)>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import wraps\n",
    "\n",
    "def wrap_func(method):\n",
    "    @wraps(method)\n",
    "    def _impl(self, *method_args, **method_kwargs):\n",
    "        method_output = method(self, *method_args, **method_kwargs)\n",
    "        return method_output\n",
    "    return _impl\n",
    "    # return \n",
    "\n",
    "class RMTEncoderForSequenceClassification(RMTBaseModel):\n",
    "    def __init__(self, base_model, **rmt_kwargs):\n",
    "        super().__init__(base_model, **rmt_kwargs)\n",
    "\n",
    "    @wrap_func\n",
    "    def forward(self, input_ids, **kwargs):\n",
    "        memory = self.set_memory()\n",
    "        memory = memory.repeat(input_ids.shape[0], 1, 1)\n",
    "        segmented = self.pad_and_segment(input_ids)\n",
    "\n",
    "        losses = []\n",
    "        for seg_num, segment_input_ids in enumerate(segmented):                \n",
    "            if (self.rmt_config['bptt_depth'] > -1) and (len(segmented) - seg_num > self.rmt_config['bptt_depth']): \n",
    "                memory = memory.detach()\n",
    "\n",
    "            seg_kwargs = dict(**kwargs)\n",
    "            seg_kwargs['output_hidden_states'] = True\n",
    "\n",
    "            non_empty_mask = [s is not None for s in segment_input_ids]\n",
    "            if sum(non_empty_mask) == 0:\n",
    "                continue\n",
    "            input_ids = torch.stack([s for s in segment_input_ids if s is not None])\n",
    "            attention_mask = self.get_attention_mask(input_ids)\n",
    "            token_type_ids = self.get_token_type_ids(input_ids)\n",
    "            seg_kwargs['labels'] = seg_kwargs['labels'][non_empty_mask]\n",
    "\n",
    "            inputs_embeds = self.model.embeddings(input_ids)\n",
    "            inputs_embeds[:, self.memory_position] = memory[non_empty_mask]\n",
    "\n",
    "            seg_kwargs['input_ids'] = None\n",
    "            seg_kwargs['inputs_embeds'] = inputs_embeds\n",
    "            seg_kwargs['attention_mask'] = attention_mask\n",
    "            seg_kwargs['token_type_ids'] = token_type_ids\n",
    "\n",
    "            out = self.model(**seg_kwargs)\n",
    "            memory[non_empty_mask] = out.hidden_states[-1][:, self.memory_position]\n",
    "\n",
    "            losses.append(out['loss'])\n",
    "\n",
    "        # drop unnecessary hiddens to save memory\n",
    "        if not kwargs.get('output_hidden_states'):\n",
    "            for key in out.keys():\n",
    "                if 'hidden_state' in key:\n",
    "                    out[key] = None\n",
    "\n",
    "        for i, l in enumerate(losses):\n",
    "            out[f'loss_{i}'] = l.mean()\n",
    "\n",
    "        if self.rmt_config['sum_loss']:\n",
    "            out['loss'] = torch.stack(losses).sum(dim=0)\n",
    "\n",
    "        return out\n",
    "        \n",
    "    def pad_add_special_tokens(self, tensor, segment_size):\n",
    "        input_elements = []\n",
    "        input_elements += [self.cls_token, self.mem_token_ids, self.sep_token, tensor, self.sep_token]\n",
    "        tensor = torch.cat(input_elements)\n",
    "\n",
    "        pad_size = segment_size - tensor.shape[0]\n",
    "        if pad_size > 0:\n",
    "            tensor = F.pad(tensor, (0, pad_size))\n",
    "        return tensor\n",
    "    \n",
    "    def get_token_type_ids(self, tensor):\n",
    "        return torch.zeros_like(tensor)\n",
    "\n",
    "\n",
    "num_segments = 2\n",
    "num_mem_tokens = 10\n",
    "# device = torch.device(3)\n",
    "device = 'cpu'\n",
    "\n",
    "\n",
    "from rmt_utils.encoder.memory_layers import memory_layers_forward as memory_layers_func\n",
    "# from rmt_utils.encoder.memory_layers import deberta_memory_layers_forward as memory_layers_func\n",
    "\n",
    "model_name = \"google/bert_uncased_L-4_H-256_A-4\"\n",
    "# model_name = \"microsoft/deberta-v3-base\"\n",
    "# model_name = 'google/electra-base-discriminator'\n",
    "# model_name = \"google/bert_uncased_L-4_H-256_A-4\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "rmt_config = {'num_mem_tokens': 5, \n",
    "                'max_n_segments': 3,\n",
    "               #  'segment_alignment': 'right',\n",
    "                'tokenizer': tokenizer,\n",
    "                'memory_layers': 'all', \n",
    "                'memory_forward_func': memory_layers_func,\n",
    "                'share_memory_layers': True,\n",
    "                'reconstruction_loss_coef': 1,\n",
    "                'segment_ordering': 'regular',\n",
    "                'input_size': 512, \n",
    "                'bptt_depth': -1, \n",
    "                'sum_loss': False,\n",
    "             }\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "rmt = RMTEncoderForSequenceClassification(base_model, **rmt_config)\n",
    "# rmt = RMTEncoderMemoryLayers(base_model, **rmt_config)\n",
    "# rmt = RMTEncoderMLMMemLoss(base_model, **rmt_config)\n",
    "# rmt.to(device)\n",
    "\n",
    "import inspect\n",
    "inspect.signature(rmt.forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLM head for input decoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertLayer(\n",
       "  (attention): BertAttention(\n",
       "    (self): BertSelfAttention(\n",
       "      (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (output): BertSelfOutput(\n",
       "      (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (intermediate): BertIntermediate(\n",
       "    (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "    (intermediate_act_fn): GELUActivation()\n",
       "  )\n",
       "  (output): BertOutput(\n",
       "    (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "    (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.model.bert.encoder.layer[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.rec_attn = copy.deepcopy(self.model.base_model.encoder.layer[-1])\n",
    "self.rec_cls = torch.nn.Linear(self.model.config.hidden_size, self.model.config.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_kwargs = dict(**kwargs)\n",
    "rec_kwargs.pop('labels')\n",
    "# rec_kwargs.pop('token_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_prob = 0.15\n",
    "\n",
    "segmented = self.pad_and_segment(sample_input_ids)\n",
    "previous_input_ids = segmented[0]\n",
    "\n",
    "inputs = torch.stack(previous_input_ids)\n",
    "input_embeddings = self.model.embeddings(inputs)\n",
    "\n",
    "out = self.model(inputs_embeds=input_embeddings, output_hidden_states=True)\n",
    "memory_outputs = out['hidden_states'][-1][:, self.memory_position]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_forward(self, memory_outputs, previous_input_ids):\n",
    "    \n",
    "    inputs = torch.stack(previous_input_ids)\n",
    "    input_embeddings = self.model.embeddings(inputs)\n",
    "    input_embeddings[:, self.memory_position] = memory_outputs\n",
    "\n",
    "    token_inds = list(range(self.num_mem_tokens + 2, input_embeddings.shape[1] - 1))\n",
    "    mask_inds = np.random.choice(token_inds, round(len(token_inds) * mlm_prob))\n",
    "    attention_mask = torch.ones(input_embeddings.shape[1])\n",
    "    attention_mask[mask_inds] = 0\n",
    "\n",
    "    rec_attn_out = self.rec_attn(input_embeddings)\n",
    "    rec_logits = self.rec_cls(rec_attn_out[0])\n",
    "\n",
    "    loss_fct = CrossEntropyLoss(ignore_index=-100)\n",
    "    reconstruction_loss = loss_fct(rec_logits.view(-1, rec_logits.size(-1)), inputs.view(-1))\n",
    "    \n",
    "    return reconstruction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.4209, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruction_forward(self, memory_outputs=memory_outputs, previous_input_ids=segmented[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_inds = list(range(self.num_mem_tokens + 2, input_embeddings.shape[1] - 1))\n",
    "mask_inds = np.random.choice(token_inds, round(len(token_inds) * mlm_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = torch.ones(input_embeddings.shape[1])\n",
    "attention_mask[mask_inds] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512, 30527])"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_attn_out = self.rec_attn(input_embeddings)\n",
    "rec_logits = self.rec_cls(rec_attn_out[0])\n",
    "rec_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.4209, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fct = CrossEntropyLoss(ignore_index=-100)\n",
    "reconstruction_loss = loss_fct(rec_logits.view(-1, rec_logits.size(-1)), inputs.view(-1))\n",
    "reconstruction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512, 256])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_inds = list(range(self.num_mem_tokens + 2, input_embeddings.shape[1] - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(token_inds)\n",
    "mask_inds = token_inds[: round(len(token_inds) * mlm_prob) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_mask_inds = torch.randa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0159,  0.0027,  0.0078,  ...,  0.0175, -0.0240,  0.0109],\n",
       "         [ 0.0315, -0.0055,  0.0017,  ...,  0.0132, -0.0214, -0.0158],\n",
       "         [ 0.0016, -0.0042, -0.0412,  ...,  0.0192,  0.0082, -0.0007],\n",
       "         ...,\n",
       "         [-0.0986,  0.0014, -0.0430,  ..., -0.0016, -0.0158, -0.0046],\n",
       "         [-0.0986,  0.0014, -0.0430,  ..., -0.0016, -0.0158, -0.0046],\n",
       "         [-0.0986,  0.0014, -0.0430,  ..., -0.0016, -0.0158, -0.0046]],\n",
       "\n",
       "        [[-0.0159,  0.0027,  0.0078,  ...,  0.0175, -0.0240,  0.0109],\n",
       "         [ 0.0315, -0.0055,  0.0017,  ...,  0.0132, -0.0214, -0.0158],\n",
       "         [ 0.0016, -0.0042, -0.0412,  ...,  0.0192,  0.0082, -0.0007],\n",
       "         ...,\n",
       "         [-0.0226,  0.0497,  0.0308,  ..., -0.0470, -0.0116,  0.0216],\n",
       "         [-0.0381, -0.0252,  0.0037,  ...,  0.0464,  0.0336,  0.0329],\n",
       "         [-0.0637, -0.0239,  0.0430,  ..., -0.0894,  0.0181,  0.0181]]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1536])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_kwargs['token_type_ids'].shapea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 4909, 2283,  ...,    0,    0,    0],\n",
       "        [ 101, 4909, 2283,  ..., 2023, 3820,  102]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.nn import CrossEntropyLoss\n",
    "# # def segment_reconstruction_forward(self, segmented, hidden_states):\n",
    "\n",
    "# hidden_states = rec_kwargs['inputs_embeds']\n",
    "# previous_input_ids = segmented[-2]\n",
    "# non_empty_mask = [s is not None for s in previous_input_ids]\n",
    "# if sum(non_empty_mask) == 0:\n",
    "#     raise ValueError\n",
    "\n",
    "# previous_input_ids = torch.stack(previous_input_ids)[non_empty_mask]\n",
    "# reconstructor_input = hidden_states[non_empty_mask]\n",
    "\n",
    "# rec_attn_out = self.rec_attn(reconstructor_input)\n",
    "# rec_logits = self.rec_cls(rec_attn_out[0])\n",
    "\n",
    "# loss_fct = CrossEntropyLoss(ignore_index=-100)\n",
    "# reconstruction_loss = loss_fct(rec_logits.view(-1, rec_logits.size(-1)), previous_input_ids.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### segment to memory attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = rmt\n",
    "input_ids = sample_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory = self.set_memory()\n",
    "# memory = memory.repeat(input_ids.shape[0], 1, 1)\n",
    "# segmented = self.pad_and_segment(input_ids)\n",
    "\n",
    "# losses = {}\n",
    "# memories = []\n",
    "# inputs = []\n",
    "# non_memory_position = [i for i in range(self.rmt_config['input_size']) if i not in self.memory_position]\n",
    "\n",
    "# for seg_num, segment_input_ids in enumerate(segmented):\n",
    "#     if (self.rmt_config['bptt_depth'] > -1) and (len(segmented) - seg_num > self.rmt_config['bptt_depth']): \n",
    "#         memory = memory.detach()\n",
    "\n",
    "#     seg_kwargs = dict(**kwargs)\n",
    "#     seg_kwargs['output_hidden_states'] = True\n",
    "    \n",
    "#     non_empty_mask = [s is not None for s in segment_input_ids]\n",
    "#     if sum(non_empty_mask) == 0:\n",
    "#         continue\n",
    "#     input_ids = torch.stack(segment_input_ids)[non_empty_mask]\n",
    "#     attention_mask = self.get_attention_mask(input_ids)\n",
    "#     token_type_ids = self.get_token_type_ids(input_ids)\n",
    "#     seg_kwargs['labels'] = seg_kwargs['labels'][non_empty_mask]\n",
    "\n",
    "#     inputs_embeds = self.embeddings(input_ids)\n",
    "#     inputs_embeds[:, self.memory_position] = memory[non_empty_mask]\n",
    "\n",
    "#     seg_kwargs['inputs_embeds'] = inputs_embeds\n",
    "#     seg_kwargs['attention_mask'] = attention_mask\n",
    "        \n",
    "#     out = self.model.forward(**seg_kwargs)\n",
    "\n",
    "#     memory[non_empty_mask] = out.encoder_hidden_states[-1][:, self.memory_position]\n",
    "    \n",
    "#     memories.append(torch.clone(memory[non_empty_mask].detach()))\n",
    "#     inputs.append(out.encoder_hidden_states[-1][:, non_memory_position])\n",
    "\n",
    "#     losses[f'loss_{seg_num}'] = out['loss']\n",
    "\n",
    "# memory_out = out.encoder_last_hidden_state[:, self.memory_position]\n",
    "# reconstruction_loss = self.segment_reconstruction_forward(segmented, memory_out)\n",
    "# out['reconstruction_loss'] = reconstruction_loss\n",
    "\n",
    "# # drop unnecessary hiddens to save memory\n",
    "# # if not kwargs.get('output_hidden_states'):\n",
    "# #     for key in out.keys():\n",
    "# #         if 'hidden_state' in key:\n",
    "# #             out[key] = None\n",
    "            \n",
    "# for k, loss in losses.items():\n",
    "#     out[k] = loss\n",
    "\n",
    "# if self.rmt_config['sum_loss']:\n",
    "#     out['loss'] = torch.stack(losses).sum(dim=0)\n",
    "\n",
    "# rec_coef = self.rmt_config['reconstruction_loss_coef']\n",
    "# out['loss'] = reconstruction_loss * rec_coef + out['loss'] * (1 - rec_coef)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hvdenv",
   "language": "python",
   "name": "hvdenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "d97c21bf6e0a282ef40de70ecfbea211641441d72159ac4287fbae50906df39f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
