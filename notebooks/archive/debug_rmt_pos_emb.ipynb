{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers import PreTrainedModel, AutoModelForSequenceClassification, T5ForConditionalGeneration\n",
    "from transformers import AutoTokenizer\n",
    "import datasets\n",
    "\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers import BertForSequenceClassification\n",
    "import transformers\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "# from modeling_rmt import RMTEncoderForSequenceClassification\n",
    "# from modeling_rmt_enc_dec import RMTEncoderDecoderForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RMTBaseModel(torch.nn.Module):\n",
    "    def __init__(self, base_model, **rmt_kwargs):\n",
    "        super().__init__()\n",
    "        self.model = base_model\n",
    "        self.set_params(**rmt_kwargs)\n",
    "\n",
    "    def set_params(self, num_mem_tokens, tokenizer, **rmt_config):\n",
    "        self.rmt_config = rmt_config\n",
    "        self.extract_special_tokens(tokenizer)\n",
    "        self.extend_word_embeddings(num_mem_tokens, tokenizer)\n",
    "\n",
    "        self.segment_size = rmt_config['input_size'] - num_mem_tokens - tokenizer.num_special_tokens_to_add()\n",
    "        if 'sep_token' in tokenizer.special_tokens_map:\n",
    "            self.segment_size -= 1\n",
    "\n",
    "    def set_memory(self, input_shape):\n",
    "        memory = self.model.embeddings(self.mem_token_ids)\n",
    "        memory = memory.repeat(input_shape[0], 1, 1)\n",
    "        return memory\n",
    "\n",
    "    def extract_special_tokens(self, tokenizer):\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "        self.special_token_ids = [tokenizer.pad_token_id]\n",
    "        for token in ['cls_token', 'sep_token', 'eos_token', 'bos_token']:\n",
    "            token_id = getattr(tokenizer, f'{token}_id')\n",
    "            if token_id is not None:\n",
    "                self.register_buffer(token, torch.tensor([token_id]))\n",
    "                self.special_token_ids.append(token_id)\n",
    "            else:\n",
    "                setattr(self, token, None)\n",
    "\n",
    "    def extend_word_embeddings(self, num_mem_tokens, tokenizer):\n",
    "            \n",
    "        vocab_size = self.model.config.vocab_size\n",
    "        extended_vocab_size = vocab_size + num_mem_tokens\n",
    "        self.num_mem_tokens = num_mem_tokens\n",
    "        self.register_buffer('mem_token_ids', torch.arange(vocab_size, vocab_size + num_mem_tokens))\n",
    "        self.model.resize_token_embeddings(extended_vocab_size)\n",
    "\n",
    "        special_tokens = tokenizer.special_tokens_map\n",
    "        mem_start_ind = int('cls_token' in special_tokens or 'bos_token' in special_tokens)\n",
    "        self.memory_position = range(mem_start_ind, mem_start_ind + num_mem_tokens)\n",
    "        \n",
    "        if hasattr(self.model.base_model, 'embeddings'): # enc-only\n",
    "            self.model.embeddings = self.model.base_model.embeddings.word_embeddings\n",
    "        elif hasattr(self.model.encoder, 'embed_tokens'): # enc-dec\n",
    "            self.model.embeddings = self.model.encoder.embed_tokens\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "       raise NotImplementedError\n",
    "\n",
    "    def pad_and_segment(self, input_ids):\n",
    "        segmented_batch = []\n",
    "        for seq in input_ids:\n",
    "            drop_mask = sum([seq == t for t in self.special_token_ids])\n",
    "            seq = seq[(1 - drop_mask).bool()]\n",
    "            seq = seq[:self.segment_size * self.rmt_config['max_n_segments']]\n",
    "\n",
    "            align = self.rmt_config.get('segment_alignment')\n",
    "            if align in {'right', None}:\n",
    "                split_inds = (list(range(len(seq), 0, -self.segment_size)) + [0])[::-1]\n",
    "                input_segments = [seq[start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "            elif align == 'left':\n",
    "                split_inds = list(range(0, len(seq), self.segment_size)) + [len(seq)]\n",
    "                input_segments = [seq[start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "            else:\n",
    "                n_seg = math.ceil(len(seq) / self.segment_size)\n",
    "                input_segments = torch.chunk(seq, n_seg)\n",
    "\n",
    "            input_segments = [self.pad_add_special_tokens(t, self.rmt_config['input_size']) for t in input_segments]\n",
    "\n",
    "            # add empty segment markers if needed\n",
    "            n_empty_segments = self.rmt_config['max_n_segments'] - len(input_segments)\n",
    "            input_segments = [None] * n_empty_segments + input_segments\n",
    "\n",
    "            segmented_batch.append(input_segments)\n",
    "\n",
    "        segmented_batch = [[sample[seg_num] for sample in segmented_batch] \\\n",
    "                            for seg_num in range(self.rmt_config['max_n_segments'])]\n",
    "        return segmented_batch\n",
    "\n",
    "    def pad_add_special_tokens(self, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def prepare_kwargs(self, segment_input_ids, kwargs):\n",
    "        seg_kwargs = dict(**kwargs)\n",
    "        non_empty_mask = [s is not None for s in segment_input_ids]\n",
    "        if sum(non_empty_mask) == 0:\n",
    "            return None, non_empty_mask\n",
    "            \n",
    "        input_ids = torch.stack([s for s in segment_input_ids if s is not None])\n",
    "        inputs_embeds = self.model.embeddings(input_ids)\n",
    "\n",
    "        seg_kwargs['input_ids'] = None\n",
    "        seg_kwargs['inputs_embeds'] = inputs_embeds\n",
    "        if seg_kwargs.get('labels') is not None:\n",
    "            seg_kwargs['labels'] = seg_kwargs['labels'][non_empty_mask]\n",
    "        seg_kwargs['attention_mask'] = self.get_attention_mask(input_ids)\n",
    "        if seg_kwargs.get('token_type_ids') is not None:\n",
    "            seg_kwargs['token_type_ids'] = self.get_token_type_ids(input_ids)\n",
    "        seg_kwargs['output_hidden_states'] = True\n",
    "\n",
    "        return seg_kwargs, non_empty_mask\n",
    "\n",
    "    def process_outputs(self, model_outputs, output_attentions, output_hidden_states):\n",
    "        rmt_out = model_outputs[-1]\n",
    "\n",
    "        segment_keys = ['loss']\n",
    "        if output_attentions:\n",
    "            segment_keys.append('attentions')\n",
    "        if output_hidden_states:\n",
    "            segment_keys.append('hidden_states')\n",
    "\n",
    "        extracted = {}\n",
    "        for seg_num, out in enumerate(model_outputs):\n",
    "            for key, value in out.items():\n",
    "                if any([sk in key for sk in segment_keys]):\n",
    "                    extracted[f'{key}_{seg_num}'] = value\n",
    "\n",
    "        if self.rmt_config['sum_loss']:\n",
    "            losses = [out['loss'] for out in model_outputs]\n",
    "            extracted['loss'] = torch.stack(losses).mean(dim=0)\n",
    "\n",
    "        for key, value in extracted.items():\n",
    "            rmt_out[key] = value\n",
    "        \n",
    "        # drop unnecessary hiddens to save memory\n",
    "        if not output_hidden_states:\n",
    "            for key in rmt_out.keys():\n",
    "                if 'hidden_state' in key:\n",
    "                    rmt_out[key] = None\n",
    "\n",
    "        return rmt_out \n",
    "        \n",
    "    def get_token_type_ids(self, tensor):\n",
    "        return torch.zeros_like(tensor)\n",
    "\n",
    "    def get_attention_mask(self, tensor):\n",
    "        mask = torch.ones_like(tensor)\n",
    "        mask[tensor == self.pad_token_id] = 0\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_segments = 2\n",
    "num_mem_tokens = 10\n",
    "# device = torch.device(3)\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions\n",
    "\n",
    "def memory_layers_forward_pe_mod(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        head_mask=None,\n",
    "        cross_attn_head_mask=None,\n",
    "        past_key_values=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        rmt_parent=None,\n",
    "    ):\n",
    "    # Model parallel\n",
    "    if self.model_parallel:\n",
    "        torch.cuda.set_device(self.first_device)\n",
    "        self.embed_tokens = self.embed_tokens.to(self.first_device)\n",
    "    use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "    output_hidden_states = (\n",
    "        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "    )\n",
    "    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "    if input_ids is not None and inputs_embeds is not None:\n",
    "        err_msg_prefix = \"decoder_\" if self.is_decoder else \"\"\n",
    "        raise ValueError(\n",
    "            f\"You cannot specify both {err_msg_prefix}input_ids and {err_msg_prefix}inputs_embeds at the same time\"\n",
    "        )\n",
    "    elif input_ids is not None:\n",
    "        input_shape = input_ids.size()\n",
    "        input_ids = input_ids.view(-1, input_shape[-1])\n",
    "    elif inputs_embeds is not None:\n",
    "        input_shape = inputs_embeds.size()[:-1]\n",
    "    else:\n",
    "        err_msg_prefix = \"decoder_\" if self.is_decoder else \"\"\n",
    "        raise ValueError(f\"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds\")\n",
    "\n",
    "    if inputs_embeds is None:\n",
    "        assert self.embed_tokens is not None, \"You have to initialize the model with valid token embeddings\"\n",
    "        inputs_embeds = self.embed_tokens(input_ids)\n",
    "\n",
    "    batch_size, seq_length = input_shape\n",
    "\n",
    "    # required mask seq length can be calculated via length of past\n",
    "    mask_seq_length = past_key_values[0][0].shape[2] + seq_length if past_key_values is not None else seq_length\n",
    "    \n",
    "\n",
    "    if use_cache is True:\n",
    "        assert self.is_decoder, f\"`use_cache` can only be set to `True` if {self} is used as a decoder\"\n",
    "\n",
    "    if attention_mask is None:\n",
    "        attention_mask = torch.ones(batch_size, mask_seq_length).to(inputs_embeds.device)\n",
    "    if self.is_decoder and encoder_attention_mask is None and encoder_hidden_states is not None:\n",
    "        encoder_seq_length = encoder_hidden_states.shape[1]\n",
    "        encoder_attention_mask = torch.ones(\n",
    "            batch_size, encoder_seq_length, device=inputs_embeds.device, dtype=torch.long\n",
    "        )\n",
    "\n",
    "    # initialize past_key_values with `None` if past does not exist\n",
    "    if past_key_values is None:\n",
    "        past_key_values = [None] * len(self.block)\n",
    "\n",
    "    # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
    "    # ourselves in which case we just need to make it broadcastable to all heads.\n",
    "    extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_shape, inputs_embeds.device)\n",
    "\n",
    "    # If a 2D or 3D attention mask is provided for the cross-attention\n",
    "    # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "    if self.is_decoder and encoder_hidden_states is not None:\n",
    "        encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
    "        encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
    "        if encoder_attention_mask is None:\n",
    "            encoder_attention_mask = torch.ones(encoder_hidden_shape, device=inputs_embeds.device)\n",
    "        encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
    "    else:\n",
    "        encoder_extended_attention_mask = None\n",
    "\n",
    "    # Prepare head mask if needed\n",
    "    head_mask = self.get_head_mask(head_mask, self.config.num_layers)\n",
    "    cross_attn_head_mask = self.get_head_mask(cross_attn_head_mask, self.config.num_layers)\n",
    "    present_key_value_states = () if use_cache else None\n",
    "    all_hidden_states = () if output_hidden_states else None\n",
    "    all_attentions = () if output_attentions else None\n",
    "    all_cross_attentions = () if (output_attentions and self.is_decoder) else None\n",
    "    # position_bias = None\n",
    "    encoder_decoder_position_bias = None\n",
    "\n",
    "    hidden_states = self.dropout(inputs_embeds)\n",
    "    position_bias_ = rmt_parent.model.encoder.block[0].layer[0].SelfAttention.compute_bias(hidden_states.shape[1], hidden_states.shape[1])\n",
    "    neighbour_bias = position_bias_[0, :, 1, 0]\n",
    "    position_bias = torch.clone(position_bias_)\n",
    "\n",
    "    for head_num, head_bias in enumerate(neighbour_bias):\n",
    "        position_bias[:, head_num, rmt_parent.memory_position] = head_bias\n",
    "        position_bias[:, head_num, :, rmt_parent.memory_position] = head_bias\n",
    "\n",
    "    rmt_parent.encoder_position_bias = position_bias\n",
    "\n",
    "    for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):\n",
    "        layer_head_mask = head_mask[i]\n",
    "        cross_attn_layer_head_mask = cross_attn_head_mask[i]\n",
    "        # Model parallel\n",
    "        if self.model_parallel:\n",
    "            torch.cuda.set_device(hidden_states.device)\n",
    "            # Ensure that attention_mask is always on the same device as hidden_states\n",
    "            if attention_mask is not None:\n",
    "                attention_mask = attention_mask.to(hidden_states.device)\n",
    "            if position_bias is not None:\n",
    "                position_bias = position_bias.to(hidden_states.device)\n",
    "            if encoder_hidden_states is not None:\n",
    "                encoder_hidden_states = encoder_hidden_states.to(hidden_states.device)\n",
    "            if encoder_extended_attention_mask is not None:\n",
    "                encoder_extended_attention_mask = encoder_extended_attention_mask.to(hidden_states.device)\n",
    "            if encoder_decoder_position_bias is not None:\n",
    "                encoder_decoder_position_bias = encoder_decoder_position_bias.to(hidden_states.device)\n",
    "            if layer_head_mask is not None:\n",
    "                layer_head_mask = layer_head_mask.to(hidden_states.device)\n",
    "            if cross_attn_layer_head_mask is not None:\n",
    "                cross_attn_layer_head_mask = cross_attn_layer_head_mask.to(hidden_states.device)\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if self.gradient_checkpointing and self.training:\n",
    "            if use_cache:\n",
    "                raise NotImplementedError\n",
    "        else:\n",
    "            layer_outputs = layer_module(\n",
    "                hidden_states,\n",
    "                attention_mask=extended_attention_mask,\n",
    "                position_bias=position_bias,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                encoder_attention_mask=encoder_extended_attention_mask,\n",
    "                encoder_decoder_position_bias=encoder_decoder_position_bias,\n",
    "                layer_head_mask=layer_head_mask,\n",
    "                cross_attn_layer_head_mask=cross_attn_layer_head_mask,\n",
    "                past_key_value=past_key_value,\n",
    "                use_cache=use_cache,\n",
    "                output_attentions=output_attentions,\n",
    "            )\n",
    "            \n",
    "            ### Update memory\n",
    "            memory_layer = rmt_parent.memory_layers[i]\n",
    "            memory_layer_out = memory_layer(hidden_states, \n",
    "                                            attention_mask=extended_attention_mask,\n",
    "                                            position_bias=position_bias,\n",
    "                                            encoder_hidden_states=encoder_hidden_states,\n",
    "                                            encoder_attention_mask=encoder_extended_attention_mask,\n",
    "                                            encoder_decoder_position_bias=encoder_decoder_position_bias,\n",
    "                                            layer_head_mask=layer_head_mask,\n",
    "                                            cross_attn_layer_head_mask=cross_attn_layer_head_mask,\n",
    "                                            past_key_value=past_key_value,\n",
    "                                            use_cache=use_cache,\n",
    "                                            output_attentions=output_attentions,\n",
    "            )\n",
    "            memory = memory_layer_out[0][:, rmt_parent.memory_position]\n",
    "\n",
    "        # layer_outputs is a tuple with:\n",
    "        # hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\n",
    "        if use_cache is False:\n",
    "            layer_outputs = layer_outputs[:1] + (None,) + layer_outputs[1:]\n",
    "\n",
    "        hidden_states, present_key_value_state = layer_outputs[:2]\n",
    "        \n",
    "        hidden_states[:, rmt_parent.memory_position] = memory\n",
    "\n",
    "        # We share the position biases between the layers - the first layer store them\n",
    "        # layer_outputs = hidden-states, key-value-states (self-attention position bias), (self-attention weights),\n",
    "        # (cross-attention position bias), (cross-attention weights)\n",
    "        position_bias = layer_outputs[2]\n",
    "        if self.is_decoder and encoder_hidden_states is not None:\n",
    "            encoder_decoder_position_bias = layer_outputs[4 if output_attentions else 3]\n",
    "        # append next layer key value states\n",
    "        if use_cache:\n",
    "            present_key_value_states = present_key_value_states + (present_key_value_state,)\n",
    "\n",
    "        if output_attentions:\n",
    "            all_attentions = all_attentions + (layer_outputs[3],)\n",
    "            if self.is_decoder:\n",
    "                all_cross_attentions = all_cross_attentions + (layer_outputs[5],)\n",
    "\n",
    "        # Model Parallel: If it's the last layer for that device, put things on the next device\n",
    "        if self.model_parallel:\n",
    "            for k, v in self.device_map.items():\n",
    "                if i == v[-1] and \"cuda:\" + str(k) != self.last_device:\n",
    "                    hidden_states = hidden_states.to(\"cuda:\" + str(k + 1))\n",
    "\n",
    "    hidden_states = self.final_layer_norm(hidden_states)\n",
    "    hidden_states = self.dropout(hidden_states)\n",
    "\n",
    "    # Add last layer\n",
    "    if output_hidden_states:\n",
    "        all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "    if not return_dict:\n",
    "        return tuple(\n",
    "            v\n",
    "            for v in [\n",
    "                hidden_states,\n",
    "                present_key_value_states,\n",
    "                all_hidden_states,\n",
    "                all_attentions,\n",
    "                all_cross_attentions,\n",
    "            ]\n",
    "            if v is not None\n",
    "        )\n",
    "    return BaseModelOutputWithPastAndCrossAttentions(\n",
    "        last_hidden_state=hidden_states,\n",
    "        past_key_values=present_key_value_states,\n",
    "        hidden_states=all_hidden_states,\n",
    "        attentions=all_attentions,\n",
    "        cross_attentions=all_cross_attentions,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling_rmt import RMTEncoderDecoderMemoryLayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/cryptography/hazmat/backends/openssl/x509.py:14: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# from rmt_utils.encoder_decoder.horizontal_memory import horizontal_memory_forward as memory_forward_func\n",
    "# model_name = \"facebook/bart-base\"\n",
    "model_name = 't5-small'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "rmt_config = {'num_mem_tokens': 5, \n",
    "                'max_n_segments': 3,\n",
    "               #  'segment_alignment': 'right',\n",
    "                'tokenizer': tokenizer,\n",
    "                'memory_layers': 'all', \n",
    "                'memory_forward_func': memory_layers_forward_pe_mod,\n",
    "                'share_memory_layers': True,\n",
    "                'reconstruction_loss_coef': 0.1,\n",
    "                'segment_ordering': 'regular',\n",
    "                'input_size': 512, \n",
    "                'bptt_depth': -1, \n",
    "                'sum_loss': False,\n",
    "             }\n",
    "\n",
    "base_model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "rmt = RMTEncoderDecoderMemoryLayers(base_model, **rmt_config)\n",
    "rmt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['loss', 'logits', 'past_key_values', 'decoder_hidden_states', 'encoder_last_hidden_state', 'encoder_hidden_states', 'loss_0', 'loss_1', 'loss_2'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = rmt(sample_input_ids, **kwargs, output_hidden_states=False, output_attentions=False)\n",
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 512, 512])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmt.encoder_position_bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f79a405d070>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(rmt.encoder_position_bias[0, 0].detach(), cmap='Reds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f79a403c1c0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(rmt.encoder_position_bias[0, 0, :40, :40].detach(), cmap='Reds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_bias = rmt.model.encoder.block[0].layer[0].SelfAttention.compute_bias(512, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_bias = torch.clone(pos_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbour_bias = pos_bias[0, :, 1, 0]\n",
    "# pos_bias[:, :, rmt.memory_position] = neighbour_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbour_bias = pos_bias[0, :, 1, 0]\n",
    "new_bias = torch.clone(pos_bias)\n",
    "\n",
    "for head_num, head_bias in enumerate(neighbour_bias):\n",
    "    new_bias[:, head_num, rmt.memory_position] = head_bias\n",
    "    new_bias[:, head_num, :, rmt.memory_position] = head_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_bias = new_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 5])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_bias[:, head_num, :, rmt.memory_position].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Output 0 of UnbindBackward is a view and its base or another view of its base has been modified inplace. This view is the output of a function that returns multiple views. Such functions do not allow the output views to be modified inplace. You should replace the inplace operation by an out-of-place one.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_pos_emb.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu11/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_pos_emb.ipynb#Y144sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m pos_bias[:, head_num, :, rmt\u001b[39m.\u001b[39mmemory_position] \u001b[39m=\u001b[39m head_bias\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Output 0 of UnbindBackward is a view and its base or another view of its base has been modified inplace. This view is the output of a function that returns multiple views. Such functions do not allow the output views to be modified inplace. You should replace the inplace operation by an out-of-place one."
     ]
    }
   ],
   "source": [
    "pos_bias[:, head_num, :, rmt.memory_position] = head_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'set_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_pos_emb.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu11/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_pos_emb.ipynb#Y143sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m pos_bias\u001b[39m.\u001b[39;49mset_index\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'set_index'"
     ]
    }
   ],
   "source": [
    "torch.index_fill(pos_bias, 1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbour_bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.5156, -5.0000, -4.7500,  ..., -5.4375, -5.4375, -5.4375],\n",
       "        [ 5.4375, -2.5156, -5.0000,  ..., -5.4375, -5.4375, -5.4375],\n",
       "        [ 4.1250,  5.4375, -2.5156,  ..., -5.4375, -5.4375, -5.4375],\n",
       "        ...,\n",
       "        [-9.9375, -9.9375, -9.9375,  ..., -2.5156, -5.0000, -4.7500],\n",
       "        [-9.9375, -9.9375, -9.9375,  ...,  5.4375, -2.5156, -5.0000],\n",
       "        [-9.9375, -9.9375, -9.9375,  ...,  4.1250,  5.4375, -2.5156]],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos_bias[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0199,  4.4688,  2.8906,  ..., -1.5078, -1.5078, -1.5078],\n",
       "        [ 1.3203,  0.0199,  4.4688,  ..., -1.5078, -1.5078, -1.5078],\n",
       "        [ 1.0078,  1.3203,  0.0199,  ..., -1.5078, -1.5078, -1.5078],\n",
       "        ...,\n",
       "        [-1.3047, -1.3047, -1.3047,  ...,  0.0199,  4.4688,  2.8906],\n",
       "        [-1.3047, -1.3047, -1.3047,  ...,  1.3203,  0.0199,  4.4688],\n",
       "        [-1.3047, -1.3047, -1.3047,  ...,  1.0078,  1.3203,  0.0199]],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_bias[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-10.8125,  -1.4375,   0.6719,  ...,   1.0938,   1.0938,   1.0938],\n",
       "        [  0.1201, -10.8125,  -1.4375,  ...,   1.0938,   1.0938,   1.0938],\n",
       "        [  0.9531,   0.1201, -10.8125,  ...,   1.0938,   1.0938,   1.0938],\n",
       "        ...,\n",
       "        [  1.0703,   1.0703,   1.0703,  ..., -10.8125,  -1.4375,   0.6719],\n",
       "        [  1.0703,   1.0703,   1.0703,  ...,   0.1201, -10.8125,  -1.4375],\n",
       "        [  1.0703,   1.0703,   1.0703,  ...,   0.9531,   0.1201, -10.8125]],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_bias[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2480,  0.5781,  0.7930,  ..., -0.0454, -0.0454, -0.0454],\n",
       "        [-0.0613, -0.2480,  0.5781,  ..., -0.0454, -0.0454, -0.0454],\n",
       "        [ 0.5273, -0.0613, -0.2480,  ..., -0.0454, -0.0454, -0.0454],\n",
       "        ...,\n",
       "        [ 1.0391,  1.0391,  1.0391,  ..., -0.2480,  0.5781,  0.7930],\n",
       "        [ 1.0391,  1.0391,  1.0391,  ..., -0.0613, -0.2480,  0.5781],\n",
       "        [ 1.0391,  1.0391,  1.0391,  ...,  0.5273, -0.0613, -0.2480]],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_bias[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_bias[0, :, 256].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([255, 257, 241,   0, 254, 255, 258, 257])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(pos_bias[0, :, 256], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.5156, -5.0000, -4.7500,  ..., -5.4375, -5.4375, -5.4375],\n",
       "        [ 5.4375, -2.5156, -5.0000,  ..., -5.4375, -5.4375, -5.4375],\n",
       "        [ 4.1250,  5.4375, -2.5156,  ..., -5.4375, -5.4375, -5.4375],\n",
       "        ...,\n",
       "        [-9.9375, -9.9375, -9.9375,  ..., -2.5156, -5.0000, -4.7500],\n",
       "        [-9.9375, -9.9375, -9.9375,  ...,  5.4375, -2.5156, -5.0000],\n",
       "        [-9.9375, -9.9375, -9.9375,  ...,  4.1250,  5.4375, -2.5156]],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_bias[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc79e4b54f0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAAD8CAYAAACVSwr3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAAXaUlEQVR4nO2df4wc5XnHP8/e3tmJjc74J9R2MVHcRvzREmQRokRRCqECjGKkAiKKGquyZKmhEhGVEqNKlfJf0j9CEqlK6pSojpQfEJIKx4amFIiqSsXGJEAgLuGgkLNlctiYixsLbLxP/5h3z3ve3duZvdmdd2a/H+l0M+/M7D777u3n5nnnnWfN3RFCiFZqRQcghIgPiUEI0YbEIIRoQ2IQQrQhMQgh2pAYhBBtDEQMZnaDmb1oZlNmtmsQzyGEGByW9zwGMxsDfg1cDxwBngI+5e6/yvWJhBADYxBnDFcDU+7+irufAX4AbBvA8wghBkR9AI+5HphuWT8CfGihA1ZN1P3yP1ibrJjN/91Kp7audNs36xmSdX8oIUrE0y9PH3f3NWn2HYQYUmFmO4GdAKusxuN/dCnLbt8KY2NQG4OagdUSGdRqrQf2FoTVzh/fCW9kDLaWUUpCxEf9L+56Le2+g0gljgIbW9Y3hLZ5uPtud9/i7ltWjY2x7+A0v39gP7x7FhrnoOHJB9gdGo3WA5OfhfDG+eM70U0YCz2e7ikRI8QgxPAUsNnMLjezCeAOYO9CB9RrxrrxcfYfnOb3P3wEzp3rLYdeSA5C9E3uYnD3d4G/AX4KHAYecPcXFjpmbMy45KIJ1o6Pnz9z6CWHRooPquQgRF8MZIzB3R8GHk67f61mLF8+wSUAp2D/wWm2sp9lt90YIhyDWgOoJUJojjk0P6QL5f/egEYtOb6TCKyWbczBQxwacxAVJoqZj1arsXxZPZFDOHNQWiFEccQhhnqN8eVL2+SgtEKIYohDDAb1Fe9tk8PcgGQaOehqhRC5EYUYwKhN1DvKYW1WOfRCchCiJ3GIoV6DsVoih5XLlFYIUTBxiMFq1FdehNVr1Opj+aQVvZAchOhKHGIAGB9n7OIghzzSikaXD3wrkoMQHYlDDM05ARfKoVda0Wv6tNIKIfoiDjGMjcH4eLLcKodeaYXmOQgxEOIQgwGTk53loLRCiKEThxggOWtYSA5KK4QYGvGIARaWg9IKIYZGXGKArnKYm+egtEKIgROHGC68U7GDHObmOSitEGLgRCEGq4/DxMT8RqUVQhRGFGIAsFVrs8lB06eFGBjRiIGxsexyqI91lMO6LGmF7soUoo14xAD9y0FphRC5EpcYoD85KK0QIlfiEwMkcli5Jre0Yr/SCiEyEacYAOr13NIK1ZAUIhtxiKE+3rm937SiixwypRW9kBxEhYlDDOMTsPbSziXZ+0krusghU1qhMQcxwsQhBsBWb4A1l3Te2E9a0Wv6tNIKIboSjRioWSKHtZd23q6rFUIMjXjEAOnk0M/VCtWQFCITcYkBesthEGmFBiSFmEd8YoDi0goNSAoBxCoGmC+HblcrNH1aiIEQrxjgvBy6Xa3IeZ6D0gohEuIWAyituPDxJAcxBOIXA8SdVkgOooLEIYY0H45Y0wqNOYgKEocYxpfAkvf23k9pxfzHkxzEgOj512hm3zazGTN7vqVtpZk9amYvhd8Xh3Yzs6+b2ZSZPWdmV6WKwgxbe1lxctDVCiHmkeYv8V+AGy5o2wU85u6bgcfCOsCNwObwsxP4RtpAbHxJsXLI665MnTmICtDzr9Dd/xN484LmbcCesLwHuKWl/Tue8CSwwsy6fHrbGYQcbMmStvasaYVqSIpRo98xhnXufiwsvw6sC8vrgemW/Y6EtjbMbKeZHTKzQ28cP3G+fXwJtuYPc5MDed9bobRCjACLHnx0dwcy/0W6+2533+LuW9asXjVvm00sVVoBkoMojH7F8NtmihB+z4T2o8DGlv02hLbMRDHmoLRCjCj9imEvsD0sbwceamn/TLg6cQ0w25JyZKZwOSitECNKmsuV3wf+G/hjMztiZjuALwHXm9lLwCfCOsDDwCvAFPAt4LOLDTDvMYcscuj1RbqqISmqSr3XDu7+qS6bruuwrwN3LjaoC7GJpbD2MnzmNXjn9MI7Bzk4wEyHk5VQ7MXffAPOnJnXzuQkzM7C2bNzX6R77uQpaiRyAFjO2wBcAnAK9h+cZiv7WXbbjaE3x6DWAGqJEGrhA91oJNO5O03pbuINaNSS4zuJwGrdxdHt8agt/JxCdCCOmY8pyDWtUA1JIRakNGKACMYcVENSjAilEgMMQA6qISlEG6UTA+Q8IKkakkK0UUoxQCSToJRWiIpSWjHAItKKvL7xKq+0QnIQkVFqMUCfacUwv/FKYw6ihJReDFCStELFXkSJqIQYoARphWpIihJRGTFAn3JQWiE5iDYqJQYoySQopRUicionBhjAjVcq9iJGjEqKAXIekCxyEpTOHEQBVFYMEG8NyUxphQYkRQFUWgygGpLnO0JyEOmpvBggpBUFFXtJU0Py9IMP694KERUjIQaIdxLUuvFxfvLkb3S1QkTFyIgBIriUqbRClISREgOohuT5jpAcRHdGTgwwgLQi5YBkfeXCYw6ZStMrrRADZCTFAPEWe1FaIWJgZMUA8Q5IqtiLKJqRFgOUpIZkmklQvZAcRAZGXgwQb1qRqTS9BiRFjkgMAaUVzY6QHITEMI8o0oouk6AypRWSg1gkEsMFFJ5WdBlz0DdeiWEiMXRAaUWzIySHUUVi6EL0NSRVml4MEIlhAVRDstkRksOoITH0oPAbr1RDUhSAxJAC1ZBsdoTkMCpIDCnJtdiLakiKyOn5LpvZRjN7wsx+ZWYvmNldoX2lmT1qZi+F3xeHdjOzr5vZlJk9Z2ZXDfpFDItBXK1QDUkRI2ne4XeBv3X3K4BrgDvN7ApgF/CYu28GHgvrADcCm8PPTuAbuUddIKoh2ewIyaHK9Hx33f2Yu/88LJ8CDgPrgW3AnrDbHuCWsLwN+I4nPAmsMLMun4xyohqSzY6QHKpKpnfWzDYBHwQOAOvc/VjY9DqwLiyvB6ZbDjsS2ipFrJOgVENS5EHqd9XMlgM/Aj7n7r9r3ebuDmR6t81sp5kdMrNDbxw/keXQaCj8UqbSCjEgUr2jZjZOIoXvuvuPQ/NvmylC+D0T2o8CG1sO3xDa5uHuu919i7tvWbN6Vb/xF45qSDY7QnKoEmmuShhwH3DY3b/SsmkvsD0sbwceamn/TLg6cQ0w25JyVBLVkGx2hORQFdK8kx8B/hK41syeCT83AV8Crjezl4BPhHWAh4FXgCngW8Bn8w87Pgq/K1M1JEWO1Hvt4O7/BXS4MwiA6zrs78Cdi4yrlNjEUlh7GT7zGrxzeuGdgxwcYKbDCVVIK/zEDJw5M6+dyUmYnYWzZ+fkcO7kKWokA5IAy3kbgEsATsG+g9PczH6W3b61+UBQawC1RA618IFuNJIbwTrdDNbEG9CoJcd3EoHVuouj2+NRW/g5xVDRzMeciaLYSx41JJVWjDQSwwCINa1QsReRFolhQMQ6z0HFXkQaJIYBEkVaoRqSog8khgFTeFqhGpKiDySGIaC0otkRkkNZkBiGhGpINjtCcigDEsMQUQ3JZkdIDrEjMQyZIm+8mru3QmmF6IHEUABF1ZCcu7cir7SiF5JDaZEYCqIyNSR7ITmUEomhQEpxtUJ3ZY4kEkPBFFnPQcVeRDckhgiIuYbkftWQHEkkhkiIOa1QDcnRQ2KICNWQbHaE5FA0EkNkqIZksyMkhyKRGCJENSSbHSE5FIXEECmF35WpGpIjjcQQMTEPSGr6dLWRGCIn5gFJfZFudZEYSkDhk6CUVowcEkNJUFrR7AjJYRhIDCVCNSSbHSE5DBqJoWQUfrVCNSRHAomhhCitaHaE5DAoJIaSohqSzY6QHAaBxFBiVEOy2RGSQ95IDCVHNSSbHSE55InEUAFUQ7LZEZJDXkgMFUE1JJsdITnkgcRQIUpxtUJ3ZZYCiaFiFJVWqNhLtZAYKojSimZHSA790rPnzGypmR00s2fN7AUz+2Jov9zMDpjZlJndb2YToX1JWJ8K2zcN+DWIDiitaHaE5NAPaXrtHeBad/9T4ErgBjO7BvgycK+7vx84CewI++8ATob2e8N+ogBivmVbaUXc9OwxT/i/sDoefhy4FngwtO8BbgnL28I6Yft1Zp2m24lhEHMNSaUV8ZKqt8xszMyeAWaAR4GXgbfc/d2wyxFgfVheD0wDhO2zwKoOj7nTzA6Z2aE3jp9Y1IsQC1NUWjGvhqTSilKRqqfc/Zy7XwlsAK4GPrDYJ3b33e6+xd23rFnd5g2RM4UXe1FaUSoy9ZK7vwU8AXwYWGFm9bBpA3A0LB8FNgKE7ZOATgkioBQDkpo+HQVprkqsMbMVYfk9wPXAYRJB3Bp22w48FJb3hnXC9sfdR6xXIyaKAckuxV5UQzIe0vTOpcATZvYc8BTwqLvvA74A3G1mUyRjCPeF/e8DVoX2u4Fd+YctFoPSimZHSA7dqPfawd2fAz7Yof0VkvGGC9vfBm7LJToxMGxiKay9DJ95Dd45vfDOQQ4OMHOsfXuQg5+YgTNn5rUzOQmzs3D27Jwczp08RY0krQBYztsAXAJwCvYdnOZm9rPs9q3NB4JaA6glcqiFD3SjkdSXWOiilzegUUuO7yQCq3UXR7fHo7bwc1YAzXwcYVRDstkROnO4EIlhxFENyWZHSA6tSAwijqsV+iLdqJAYBBBBDcleX2qjSVBDRWIQc0RfQ1JpxdCQGMQ8Cp/noElQUSAxiDaiuFqhGpKFIjGIjhR+tUJ3ZRaKxCC6EsXVCt2VWQgSg1iQ3K9WaPp0KZAYRE/6Siu6Xa3IeZ6D0orBIDGIVCitaHbEaMhBYhCpqUxaITn0RGIQmch1ElRRaYXGHHoiMYjMlGISlNKKRSExiL4oXA66WjFQJAbRN4XLIa+7MnXm0IbEIBbFIORgS5a0tWdNK1RDcnFIDGLR5F1DEn2RbuFIDCIXopjnoLQiNyQGkRtRjDkorcgFiUHkSuFyUFqRCxKDyJ2Yv0hXNSTTITGIgZD7mEPKAcl5X6SrGpJ9IzGIgZFrWqEakkNFYhADpfAxB9WQ7AuJQQwc1ZBsdkR55CAxiKGgGpLNjiiHHCQGMTSimASltCIVEoMYKoV/41VeaUXF5SAxiKGTaw3JotKKio85SAyiEEqRVozwPAeJQRRG9GnFCNeQTB2RmY2Z2S/MbF9Yv9zMDpjZlJndb2YToX1JWJ8K2zcNKHZRAaL/It0RTSuyRHMXcLhl/cvAve7+fuAksCO07wBOhvZ7w35CdKUUk6BGLK1IFYmZbQC2Av8c1g24Fngw7LIHuCUsbwvrhO3Xhf2F6EruN16p2MuiSBvFV4HPA81Xswp4y93fDetHgPVheT0wDRC2z4b952FmO83skJkdeuP4if6iF5Ui1wHJIidBVeDMoWcEZnYzMOPuT+f5xO6+2923uPuWNavbvCFGlFhrSGZKKyowIJnm2T8CfNLMXgV+QJJCfA1YYWb1sM8G4GhYPgpsBAjbJwGdEojUqIZksyOKk0PPZ3b3e9x9g7tvAu4AHnf3TwNPALeG3bYDD4XlvWGdsP1x9whuMBelwiaWFlbsJU0NydMPPlzpeysWM4/hC8DdZjZFMoZwX2i/D1gV2u8Gdi0uRDGqxDoJat34OD958jeVvlpR771Ly/O5/wz4WVh+Bbi6wz5vA7ctKiohAja+BNZehs+8Bu+cXnjnIAcHmDnWvj3IwU/MwJkz89qZnITZWTh7dk4O506eogbUVyRiWs7bAFwCcAr2H5xmK4+w7PatzQeCWgOoJUKohQ+0e+dJWa14Axq15PhOIrBad3F0ezxqvZ+3C5r5KKJHNSSbHTG8MweJQZSCmGtIVnESlMQgSkOsxV6qeLVCYhClItYByVIUe8mAxCBKRylqSKaZBNWLvOWQAYlBlJJY04pMpemLGJBMicQgSovSimZH5P8xlhhEqYkiregyCSpTWhGZHCQGUXoKTyu6jDlE/41XCyAxiEqgtKLZEfl8pCUGURmiryEZa2n6DkgMolKohmSzIxb30ZYYROVQDclmR/T/8ZYYRCVRDclmR/T3EZcYRGXJtdjLKNWQRGIQFWcQVytGoYakxCAqj2pIZkdiECOBakhmQ2IQI0Osk6CGXkMyBRKDGCkKv5RZkrRCYhAjh2pIpnjZmY8QogKohmSPl5xpbyEqROF3ZQ67hmQGJAYx0sQ6IDmQuzIzIDGIkSeKYi951JAc8pfaClF5Yk0rci/2khKJQYjAyKQVKZAYhGghirRiGDUke720RR0tRAUpPK0YVg3JhV7Woo4WoqKMelohMQjRhZGpIdnp5WQ+QogRYmRqSF74UjIfIcSIUeSNV3P3VuSRVmRAYhAiBUXVkJy7tyKPtCIDqcRgZq+a2S/N7BkzOxTaVprZo2b2Uvh9cWg3M/u6mU2Z2XNmdlVfkQkRGZWoIZmSLGcMf+buV7r7lrC+C3jM3TcDj4V1gBuBzeFnJ/CNzFEJESmluFrRa55DChaTSmwD9oTlPcAtLe3f8YQngRVm1qVnhCgfRdZzWFSxlwykFYMD/25mT5vZztC2zt2PheXXgXVheT0w3XLskdA2DzPbaWaHzOzQG8dPZApaiKKJuYbk/m41JDOQVgwfdferSNKEO83sY60b3d1J5JEad9/t7lvcfcua1auyHCpEFMScVnSsIZmBVGJw96Ph9wzwr8DVwG+bKUL4PRN2PwpsbDl8Q2gTonKUqoZkBnqKwcyWmdlFzWXgz4Hngb3A9rDbduChsLwX+Ey4OnENMNuScghROUpTQzID9RT7rAP+1ZJpnnXge+7+b2b2FPCAme0AXgNuD/s/DNwETAGngb/KFJEQJcQmlsLay/DX/xfOvr3wzkEO3nA4/nr79jDPwd98A86cmdfO5CTMzsLZs3PzHN5989ScHACWkzz/ugbwe9h74DdsG3sk2+vxHIs79IuZnQJeLDqOlKwGjhcdRArKEieUJ9ayxAmdY73M3dekOTjNGcMweLFlfkTUmNmhMsRaljihPLGWJU5YfKyaEi2EaENiEEK0EYsYdhcdQAbKEmtZ4oTyxFqWOGGRsUYx+CiEiItYzhiEEBFRuBjM7AYzezHcpr2r9xEDjeXbZjZjZs+3tEV5e7mZbTSzJ8zsV2b2gpndFWO8ZrbUzA6a2bMhzi+G9svN7ECI534zmwjtS8L6VNi+aRhxtsQ7Zma/MLN9kcc52FII7l7YDzAGvAy8D5gAngWuKDCejwFXAc+3tP0DsCss7wK+HJZvAh4BDLgGODDkWC8FrgrLFwG/Bq6ILd7wfMvD8jhwIDz/A8Adof2bwF+H5c8C3wzLdwD3D7lf7wa+B+wL67HG+Sqw+oK23N77ob2QLi/uw8BPW9bvAe4pOKZNF4jhReDSsHwpyZwLgH8CPtVpv4Lifgi4PuZ4gfcCPwc+RDL5pn7h3wHwU+DDYbke9rMhxbeBpLbItcC+8EGKLs7wnJ3EkNt7X3QqkeoW7YJZ1O3lwyCcxn6Q5L9xdPGG0/NnSG60e5TkLPEtd3+3QyxzcYbts8Cwbr/9KvB5oHmP8qpI44QBlEJoJZaZj6XA3d3MorqMY2bLgR8Bn3P331lL6fJY4nX3c8CVZraC5O7cDxQbUTtmdjMw4+5Pm9nHCw4nDR9196NmthZ41Mz+p3XjYt/7os8YynCLdrS3l5vZOIkUvuvuPw7N0cbr7m8BT5Cckq8ws+Y/ptZY5uIM2yeBYVTy+QjwSTN7FfgBSTrxtQjjBAZfCqFoMTwFbA4jvxMkgzh7C47pQqK8vdySU4P7gMPu/pVY4zWzNeFMATN7D8k4yGESQdzaJc5m/LcCj3tIjAeJu9/j7hvcfRPJ3+Hj7v7p2OKEIZVCGNZgyQKDKDeRjKi/DPxdwbF8HzgGnCXJw3aQ5I2PAS8B/wGsDPsa8I8h7l8CW4Yc60dJ8szngGfCz02xxQv8CfCLEOfzwN+H9vcBB0luz/8hsCS0Lw3rU2H7+wr4O/g4569KRBdniOnZ8PNC83OT53uvmY9CiDaKTiWEEBEiMQgh2pAYhBBtSAxCiDYkBiFEGxKDEKINiUEI0YbEIIRo4/8BJAATQA93myMAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(pos_bias[0][0].detach(), cmap='Reds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_bias[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmt.memory_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 32099, 32131, 32131, 32131, 32131, 32131, 32131, 32131, 32131,\n",
       "         32131, 32131, 32131, 32131, 32131, 32131, 32131, 32131, 32131, 32131],\n",
       "        [    0, 32099, 32128, 32131, 32128, 32131, 32128, 32131, 32128, 32131,\n",
       "         32128, 32131, 32128, 32131, 32128, 32131, 32128, 32131, 32128, 32131]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmt.generate(sample_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmt.memory_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['loss', 'logits', 'past_key_values', 'decoder_hidden_states', 'decoder_attentions', 'cross_attentions', 'encoder_last_hidden_state', 'encoder_hidden_states', 'encoder_attentions', 'loss_0', 'decoder_hidden_states_0', 'decoder_attentions_0', 'cross_attentions_0', 'encoder_hidden_states_0', 'encoder_attentions_0', 'loss_1', 'decoder_hidden_states_1', 'decoder_attentions_1', 'cross_attentions_1', 'encoder_hidden_states_1', 'encoder_attentions_1', 'loss_2', 'decoder_hidden_states_2', 'decoder_attentions_2', 'cross_attentions_2', 'encoder_hidden_states_2', 'encoder_attentions_2'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = rmt(sample_input_ids, **kwargs, output_hidden_states=True, output_attentions=True)\n",
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1291, -0.0159, -0.0419,  ..., -0.0337, -0.2189, -0.0095],\n",
       "        [-0.0350, -0.0167,  0.0482,  ..., -0.1350, -0.2535, -0.0246],\n",
       "        [-0.0531,  0.0108, -0.0384,  ..., -0.1367, -0.1615, -0.0312],\n",
       "        ...,\n",
       "        [-0.1999,  0.0540, -0.0645,  ..., -0.1501, -0.1413, -0.0326],\n",
       "        [-0.2171,  0.0586, -0.0722,  ..., -0.1467, -0.1235, -0.0330],\n",
       "        [-0.2195,  0.0811, -0.0253,  ..., -0.0988, -0.1056, -0.0275]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmt.model.base_model.encoder.get_rel_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory_input = self.pad_add_special_tokens(self.mem_token_ids, self.input_size)[:self.num_mem_tokens + ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mem_out = self.model.base_model(memory_input.reshape((1, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_input_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m sample_input_ids\u001b[39m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_input_ids' is not defined"
     ]
    }
   ],
   "source": [
    "sample_input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['loss', 'logits', 'hidden_states', 'loss_0', 'loss_1', 'loss_2'])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = rmt(sample_input_ids, **kwargs, output_hidden_states=False, output_attentions=False)\n",
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segmented = rmt.pad_and_segment(sample_input_ids[:, :600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n, p in rmt.named_parameters():\n",
    "#     if 'memory' in n:\n",
    "#         print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(rmt.memory_decoders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n, p in rmt.named_parameters():\n",
    "#     print(n, p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Holder:\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq_len = 1536\n",
    "target_seq_len = 1024\n",
    "batch_size = 2\n",
    "\n",
    "args = Holder\n",
    "args.target_seq_len = target_seq_len\n",
    "args.input_seq_len = input_seq_len\n",
    "args.input_prefix = ''\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_attention_first_token = False  # should be True for LED\n",
    "encode_plus_kwargs = {'truncation': True, 'padding': 'longest', 'pad_to_multiple_of': 1}\n",
    "# generate_kwargs = {'max_length': args.target_seq_len, 'min_length': args.target_seq_len}\n",
    "generate_kwargs = {}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # cut too long strings because they may slow down tokenization\n",
    "    inputs = [b['input'][:args.input_seq_len * 10] for b in batch]\n",
    "    if 'outputs' in batch[0]:\n",
    "        # if we have more than 1 label per example (only in valid) take only one of them\n",
    "        # to compute loss on valid\n",
    "        labels = [b['outputs'][0][:args.target_seq_len * 10] for b in batch]\n",
    "    else:\n",
    "        labels = [b['output'][:args.target_seq_len * 10] for b in batch]\n",
    "    if args.input_prefix:\n",
    "        inputs = [args.input_prefix + inp for inp in inputs]\n",
    "    features = tokenizer.batch_encode_plus(list(inputs), max_length=args.input_seq_len, return_tensors='pt',\n",
    "                                           **encode_plus_kwargs)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer.batch_encode_plus(list(labels), max_length=args.target_seq_len, return_tensors='pt',\n",
    "                                             **encode_plus_kwargs).input_ids\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    features['labels'] = labels\n",
    "    features['id'] = [b['id'] for b in batch]\n",
    "    if 'outputs' in batch[0]:\n",
    "        features['target_text'] = [b['outputs'] for b in batch]\n",
    "    else:\n",
    "        features['target_text'] = [b['output'] for b in batch]\n",
    "    if 'global_attention_mask' in features:\n",
    "        raise RuntimeError('What global attention mask for Longformer and LongformerEncoder-Decoder should be?')\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset scrolls (/home/bulatov/.cache/huggingface/datasets/tau___scrolls/qasper/1.0.0/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a57b5a912871483d84ede5990df841e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_name = 'qasper'\n",
    "dataset = datasets.load_dataset('tau/scrolls', task_name)\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "train_sampler = RandomSampler(train_dataset,)\n",
    "kwargs = {'pin_memory': True, 'num_workers': 0}\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler,\n",
    "                                collate_fn=collate_fn, **kwargs)\n",
    "\n",
    "valid_dataset = dataset['validation']\n",
    "valid_sampler = RandomSampler(valid_dataset)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, sampler=train_sampler,\n",
    "                                collate_fn=collate_fn, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = iter(train_dataloader)\n",
    "sample = next(gen)\n",
    "\n",
    "if 'id' in sample:\n",
    "    id = sample.pop('id')\n",
    "if 'target_text' in sample:\n",
    "    tgt_text = sample.pop('target_text')\n",
    "\n",
    "rmt.to(device)\n",
    "for k in sample:\n",
    "    sample[k] = sample[k].to(device)\n",
    "    \n",
    "sample_input_ids = sample.pop('input_ids').to(device)\n",
    "kwargs = sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq_len = 1536\n",
    "target_seq_len = 3\n",
    "batch_size = 2\n",
    "\n",
    "args = Holder\n",
    "args.target_seq_len = target_seq_len\n",
    "args.input_seq_len = input_seq_len\n",
    "args.input_prefix = ''\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_plus_kwargs = {'max_length': args.input_seq_len,\n",
    "                        'truncation': True,\n",
    "                        'padding': 'longest',\n",
    "                        'pad_to_multiple_of': 1}\n",
    "generate_kwargs = {}\n",
    "labels_map = {'Contradiction': 0, 'Entailment': 1, 'Not mentioned': 2}\n",
    "num_labels = len(labels_map)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # cut too long strings because they may slow down tokenization\n",
    "    inputs = [b['input'][:args.input_seq_len * 10] for b in batch]\n",
    "    labels = [b['output'][:args.target_seq_len * 10] for b in batch]\n",
    "    if args.input_prefix:\n",
    "        inputs = [args.input_prefix + inp for inp in inputs]\n",
    "    features = tokenizer.batch_encode_plus(list(inputs), return_tensors='pt', **encode_plus_kwargs)\n",
    "    labels = np.array([labels_map[t] for t in labels])\n",
    "    features['labels'] = torch.from_numpy(labels)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset scrolls (/home/bulatov/.cache/huggingface/datasets/tau___scrolls/contract_nli/1.0.0/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "756239b0889d473ca9d2eb8cde3cd904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_name = 'contract_nli'\n",
    "dataset = datasets.load_dataset('tau/scrolls', task_name)\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "train_sampler = RandomSampler(train_dataset,)\n",
    "kwargs = {'pin_memory': True, 'num_workers': 0}\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler,\n",
    "                                collate_fn=collate_fn, **kwargs)\n",
    "\n",
    "valid_dataset = dataset['validation']\n",
    "valid_sampler = RandomSampler(valid_dataset)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, sampler=train_sampler,\n",
    "                                collate_fn=collate_fn, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = iter(train_dataloader)\n",
    "sample = next(gen)\n",
    "\n",
    "if 'id' in sample:\n",
    "    id = sample.pop('id')\n",
    "if 'target_text' in sample:\n",
    "    tgt_text = sample.pop('target_text')\n",
    "\n",
    "rmt.to(device)\n",
    "for k in sample:\n",
    "    sample[k] = sample[k].to(device)\n",
    "    \n",
    "sample_input_ids = sample.pop('input_ids').to(device)\n",
    "kwargs = sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleAttributeError",
     "evalue": "'BertForSequenceClassification' object has no attribute 'encoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb Cell 37\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m out \u001b[39m=\u001b[39m rmt(sample_input_ids, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs, output_hidden_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, output_attentions \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m out\u001b[39m.\u001b[39mkeys()\n",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb Cell 37\u001b[0m in \u001b[0;36mRMTEncoderHorizontalMemory.__call__\u001b[0;34m(self, input_ids, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     memory \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mset_memory()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     memory \u001b[39m=\u001b[39m memory\u001b[39m.\u001b[39mrepeat(input_ids\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m     segmented \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad_and_segment(input_ids)\n",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb Cell 37\u001b[0m in \u001b[0;36mRMTEncoderHorizontalMemory.set_memory\u001b[0;34m(self, memory)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# fill layer memories \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m memory_input \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad_add_special_tokens(mem_token_ids, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_mem_tokens)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m mem_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mencoder(memory_input\u001b[39m.\u001b[39mreshape((\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mreturn\u001b[39;00m memory\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/nn/modules/module.py:778\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m    777\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m--> 778\u001b[0m \u001b[39mraise\u001b[39;00m ModuleAttributeError(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    779\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mModuleAttributeError\u001b[0m: 'BertForSequenceClassification' object has no attribute 'encoder'"
     ]
    }
   ],
   "source": [
    "out = rmt(sample_input_ids, **kwargs, output_hidden_states=True, output_attentions = True)\n",
    "out.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### replace forward signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect \n",
    "from functools import wraps\n",
    "\n",
    "model_name = \"google/bert_uncased_L-4_H-256_A-4\"\n",
    "\n",
    "def decorate(func, source):\n",
    "    @wraps(source)\n",
    "    def decorated(*args, **kwargs):\n",
    "        return func(*args, **kwargs)\n",
    "    return decorated\n",
    "\n",
    "class RMT(torch.nn.Module):\n",
    "    def __init__(self, base_model, **rmt_kwargs):\n",
    "        super().__init__()\n",
    "        self.model = base_model\n",
    "        self.forward = decorate(self.forward, self.model.forward)\n",
    "\n",
    "    def forward(self, new_rmt_arg, input_ids, **kwargs):\n",
    "        pass\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "rmt = RMT(base_model, **rmt_config)\n",
    "inspect.signature(rmt.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bert_uncased_L-4_H-256_A-4 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-4_H-256_A-4 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Signature (input_ids, **kwargs)>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import wraps\n",
    "\n",
    "def wrap_func(method):\n",
    "    @wraps(method)\n",
    "    def _impl(self, *method_args, **method_kwargs):\n",
    "        method_output = method(self, *method_args, **method_kwargs)\n",
    "        return method_output\n",
    "    return _impl\n",
    "    # return \n",
    "\n",
    "class RMTEncoderForSequenceClassification(RMTBaseModel):\n",
    "    def __init__(self, base_model, **rmt_kwargs):\n",
    "        super().__init__(base_model, **rmt_kwargs)\n",
    "\n",
    "    @wrap_func\n",
    "    def forward(self, input_ids, **kwargs):\n",
    "        memory = self.set_memory()\n",
    "        memory = memory.repeat(input_ids.shape[0], 1, 1)\n",
    "        segmented = self.pad_and_segment(input_ids)\n",
    "\n",
    "        losses = []\n",
    "        for seg_num, segment_input_ids in enumerate(segmented):                \n",
    "            if (self.rmt_config['bptt_depth'] > -1) and (len(segmented) - seg_num > self.rmt_config['bptt_depth']): \n",
    "                memory = memory.detach()\n",
    "\n",
    "            seg_kwargs = dict(**kwargs)\n",
    "            seg_kwargs['output_hidden_states'] = True\n",
    "\n",
    "            non_empty_mask = [s is not None for s in segment_input_ids]\n",
    "            if sum(non_empty_mask) == 0:\n",
    "                continue\n",
    "            input_ids = torch.stack([s for s in segment_input_ids if s is not None])\n",
    "            attention_mask = self.get_attention_mask(input_ids)\n",
    "            token_type_ids = self.get_token_type_ids(input_ids)\n",
    "            seg_kwargs['labels'] = seg_kwargs['labels'][non_empty_mask]\n",
    "\n",
    "            inputs_embeds = self.model.embeddings(input_ids)\n",
    "            inputs_embeds[:, self.memory_position] = memory[non_empty_mask]\n",
    "\n",
    "            seg_kwargs['input_ids'] = None\n",
    "            seg_kwargs['inputs_embeds'] = inputs_embeds\n",
    "            seg_kwargs['attention_mask'] = attention_mask\n",
    "            seg_kwargs['token_type_ids'] = token_type_ids\n",
    "\n",
    "            out = self.model(**seg_kwargs)\n",
    "            memory[non_empty_mask] = out.hidden_states[-1][:, self.memory_position]\n",
    "\n",
    "            losses.append(out['loss'])\n",
    "\n",
    "        # drop unnecessary hiddens to save memory\n",
    "        if not kwargs.get('output_hidden_states'):\n",
    "            for key in out.keys():\n",
    "                if 'hidden_state' in key:\n",
    "                    out[key] = None\n",
    "\n",
    "        for i, l in enumerate(losses):\n",
    "            out[f'loss_{i}'] = l.mean()\n",
    "\n",
    "        if self.rmt_config['sum_loss']:\n",
    "            out['loss'] = torch.stack(losses).sum(dim=0)\n",
    "\n",
    "        return out\n",
    "        \n",
    "    def pad_add_special_tokens(self, tensor, segment_size):\n",
    "        input_elements = []\n",
    "        input_elements += [self.cls_token, self.mem_token_ids, self.sep_token, tensor, self.sep_token]\n",
    "        tensor = torch.cat(input_elements)\n",
    "\n",
    "        pad_size = segment_size - tensor.shape[0]\n",
    "        if pad_size > 0:\n",
    "            tensor = F.pad(tensor, (0, pad_size))\n",
    "        return tensor\n",
    "    \n",
    "    def get_token_type_ids(self, tensor):\n",
    "        return torch.zeros_like(tensor)\n",
    "\n",
    "\n",
    "num_segments = 2\n",
    "num_mem_tokens = 10\n",
    "# device = torch.device(3)\n",
    "device = 'cpu'\n",
    "\n",
    "\n",
    "from rmt_utils.encoder.memory_layers import memory_layers_forward as memory_layers_func\n",
    "# from rmt_utils.encoder.memory_layers import deberta_memory_layers_forward as memory_layers_func\n",
    "\n",
    "model_name = \"google/bert_uncased_L-4_H-256_A-4\"\n",
    "# model_name = \"microsoft/deberta-v3-base\"\n",
    "# model_name = 'google/electra-base-discriminator'\n",
    "# model_name = \"google/bert_uncased_L-4_H-256_A-4\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "rmt_config = {'num_mem_tokens': 5, \n",
    "                'max_n_segments': 3,\n",
    "               #  'segment_alignment': 'right',\n",
    "                'tokenizer': tokenizer,\n",
    "                'memory_layers': 'all', \n",
    "                'memory_forward_func': memory_layers_func,\n",
    "                'share_memory_layers': True,\n",
    "                'reconstruction_loss_coef': 1,\n",
    "                'segment_ordering': 'regular',\n",
    "                'input_size': 512, \n",
    "                'bptt_depth': -1, \n",
    "                'sum_loss': False,\n",
    "             }\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "rmt = RMTEncoderForSequenceClassification(base_model, **rmt_config)\n",
    "# rmt = RMTEncoderMemoryLayers(base_model, **rmt_config)\n",
    "# rmt = RMTEncoderMLMMemLoss(base_model, **rmt_config)\n",
    "# rmt.to(device)\n",
    "\n",
    "import inspect\n",
    "inspect.signature(rmt.forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLM head for input decoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertLayer(\n",
       "  (attention): BertAttention(\n",
       "    (self): BertSelfAttention(\n",
       "      (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (output): BertSelfOutput(\n",
       "      (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (intermediate): BertIntermediate(\n",
       "    (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "    (intermediate_act_fn): GELUActivation()\n",
       "  )\n",
       "  (output): BertOutput(\n",
       "    (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "    (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.model.bert.encoder.layer[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.rec_attn = copy.deepcopy(self.model.base_model.encoder.layer[-1])\n",
    "self.rec_cls = torch.nn.Linear(self.model.config.hidden_size, self.model.config.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_kwargs = dict(**kwargs)\n",
    "rec_kwargs.pop('labels')\n",
    "# rec_kwargs.pop('token_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_prob = 0.15\n",
    "\n",
    "segmented = self.pad_and_segment(sample_input_ids)\n",
    "previous_input_ids = segmented[0]\n",
    "\n",
    "inputs = torch.stack(previous_input_ids)\n",
    "input_embeddings = self.model.embeddings(inputs)\n",
    "\n",
    "out = self.model(inputs_embeds=input_embeddings, output_hidden_states=True)\n",
    "memory_outputs = out['hidden_states'][-1][:, self.memory_position]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_forward(self, memory_outputs, previous_input_ids):\n",
    "    \n",
    "    inputs = torch.stack(previous_input_ids)\n",
    "    input_embeddings = self.model.embeddings(inputs)\n",
    "    input_embeddings[:, self.memory_position] = memory_outputs\n",
    "\n",
    "    token_inds = list(range(self.num_mem_tokens + 2, input_embeddings.shape[1] - 1))\n",
    "    mask_inds = np.random.choice(token_inds, round(len(token_inds) * mlm_prob))\n",
    "    attention_mask = torch.ones(input_embeddings.shape[1])\n",
    "    attention_mask[mask_inds] = 0\n",
    "\n",
    "    rec_attn_out = self.rec_attn(input_embeddings)\n",
    "    rec_logits = self.rec_cls(rec_attn_out[0])\n",
    "\n",
    "    loss_fct = CrossEntropyLoss(ignore_index=-100)\n",
    "    reconstruction_loss = loss_fct(rec_logits.view(-1, rec_logits.size(-1)), inputs.view(-1))\n",
    "    \n",
    "    return reconstruction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.4209, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruction_forward(self, memory_outputs=memory_outputs, previous_input_ids=segmented[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_inds = list(range(self.num_mem_tokens + 2, input_embeddings.shape[1] - 1))\n",
    "mask_inds = np.random.choice(token_inds, round(len(token_inds) * mlm_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = torch.ones(input_embeddings.shape[1])\n",
    "attention_mask[mask_inds] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512, 30527])"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_attn_out = self.rec_attn(input_embeddings)\n",
    "rec_logits = self.rec_cls(rec_attn_out[0])\n",
    "rec_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.4209, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fct = CrossEntropyLoss(ignore_index=-100)\n",
    "reconstruction_loss = loss_fct(rec_logits.view(-1, rec_logits.size(-1)), inputs.view(-1))\n",
    "reconstruction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512, 256])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_inds = list(range(self.num_mem_tokens + 2, input_embeddings.shape[1] - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(token_inds)\n",
    "mask_inds = token_inds[: round(len(token_inds) * mlm_prob) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_mask_inds = torch.randa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0159,  0.0027,  0.0078,  ...,  0.0175, -0.0240,  0.0109],\n",
       "         [ 0.0315, -0.0055,  0.0017,  ...,  0.0132, -0.0214, -0.0158],\n",
       "         [ 0.0016, -0.0042, -0.0412,  ...,  0.0192,  0.0082, -0.0007],\n",
       "         ...,\n",
       "         [-0.0986,  0.0014, -0.0430,  ..., -0.0016, -0.0158, -0.0046],\n",
       "         [-0.0986,  0.0014, -0.0430,  ..., -0.0016, -0.0158, -0.0046],\n",
       "         [-0.0986,  0.0014, -0.0430,  ..., -0.0016, -0.0158, -0.0046]],\n",
       "\n",
       "        [[-0.0159,  0.0027,  0.0078,  ...,  0.0175, -0.0240,  0.0109],\n",
       "         [ 0.0315, -0.0055,  0.0017,  ...,  0.0132, -0.0214, -0.0158],\n",
       "         [ 0.0016, -0.0042, -0.0412,  ...,  0.0192,  0.0082, -0.0007],\n",
       "         ...,\n",
       "         [-0.0226,  0.0497,  0.0308,  ..., -0.0470, -0.0116,  0.0216],\n",
       "         [-0.0381, -0.0252,  0.0037,  ...,  0.0464,  0.0336,  0.0329],\n",
       "         [-0.0637, -0.0239,  0.0430,  ..., -0.0894,  0.0181,  0.0181]]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1536])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_kwargs['token_type_ids'].shapea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 4909, 2283,  ...,    0,    0,    0],\n",
       "        [ 101, 4909, 2283,  ..., 2023, 3820,  102]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.nn import CrossEntropyLoss\n",
    "# # def segment_reconstruction_forward(self, segmented, hidden_states):\n",
    "\n",
    "# hidden_states = rec_kwargs['inputs_embeds']\n",
    "# previous_input_ids = segmented[-2]\n",
    "# non_empty_mask = [s is not None for s in previous_input_ids]\n",
    "# if sum(non_empty_mask) == 0:\n",
    "#     raise ValueError\n",
    "\n",
    "# previous_input_ids = torch.stack(previous_input_ids)[non_empty_mask]\n",
    "# reconstructor_input = hidden_states[non_empty_mask]\n",
    "\n",
    "# rec_attn_out = self.rec_attn(reconstructor_input)\n",
    "# rec_logits = self.rec_cls(rec_attn_out[0])\n",
    "\n",
    "# loss_fct = CrossEntropyLoss(ignore_index=-100)\n",
    "# reconstruction_loss = loss_fct(rec_logits.view(-1, rec_logits.size(-1)), previous_input_ids.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### segment to memory attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = rmt\n",
    "input_ids = sample_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory = self.set_memory()\n",
    "# memory = memory.repeat(input_ids.shape[0], 1, 1)\n",
    "# segmented = self.pad_and_segment(input_ids)\n",
    "\n",
    "# losses = {}\n",
    "# memories = []\n",
    "# inputs = []\n",
    "# non_memory_position = [i for i in range(self.rmt_config['input_size']) if i not in self.memory_position]\n",
    "\n",
    "# for seg_num, segment_input_ids in enumerate(segmented):\n",
    "#     if (self.rmt_config['bptt_depth'] > -1) and (len(segmented) - seg_num > self.rmt_config['bptt_depth']): \n",
    "#         memory = memory.detach()\n",
    "\n",
    "#     seg_kwargs = dict(**kwargs)\n",
    "#     seg_kwargs['output_hidden_states'] = True\n",
    "    \n",
    "#     non_empty_mask = [s is not None for s in segment_input_ids]\n",
    "#     if sum(non_empty_mask) == 0:\n",
    "#         continue\n",
    "#     input_ids = torch.stack(segment_input_ids)[non_empty_mask]\n",
    "#     attention_mask = self.get_attention_mask(input_ids)\n",
    "#     token_type_ids = self.get_token_type_ids(input_ids)\n",
    "#     seg_kwargs['labels'] = seg_kwargs['labels'][non_empty_mask]\n",
    "\n",
    "#     inputs_embeds = self.embeddings(input_ids)\n",
    "#     inputs_embeds[:, self.memory_position] = memory[non_empty_mask]\n",
    "\n",
    "#     seg_kwargs['inputs_embeds'] = inputs_embeds\n",
    "#     seg_kwargs['attention_mask'] = attention_mask\n",
    "        \n",
    "#     out = self.model.forward(**seg_kwargs)\n",
    "\n",
    "#     memory[non_empty_mask] = out.encoder_hidden_states[-1][:, self.memory_position]\n",
    "    \n",
    "#     memories.append(torch.clone(memory[non_empty_mask].detach()))\n",
    "#     inputs.append(out.encoder_hidden_states[-1][:, non_memory_position])\n",
    "\n",
    "#     losses[f'loss_{seg_num}'] = out['loss']\n",
    "\n",
    "# memory_out = out.encoder_last_hidden_state[:, self.memory_position]\n",
    "# reconstruction_loss = self.segment_reconstruction_forward(segmented, memory_out)\n",
    "# out['reconstruction_loss'] = reconstruction_loss\n",
    "\n",
    "# # drop unnecessary hiddens to save memory\n",
    "# # if not kwargs.get('output_hidden_states'):\n",
    "# #     for key in out.keys():\n",
    "# #         if 'hidden_state' in key:\n",
    "# #             out[key] = None\n",
    "            \n",
    "# for k, loss in losses.items():\n",
    "#     out[k] = loss\n",
    "\n",
    "# if self.rmt_config['sum_loss']:\n",
    "#     out['loss'] = torch.stack(losses).sum(dim=0)\n",
    "\n",
    "# rec_coef = self.rmt_config['reconstruction_loss_coef']\n",
    "# out['loss'] = reconstruction_loss * rec_coef + out['loss'] * (1 - rec_coef)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hvdenv",
   "language": "python",
   "name": "hvdenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "d97c21bf6e0a282ef40de70ecfbea211641441d72159ac4287fbae50906df39f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
