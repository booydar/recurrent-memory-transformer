{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers import PreTrainedModel, AutoModelForSequenceClassification, T5ForConditionalGeneration\n",
    "from transformers import AutoTokenizer\n",
    "import datasets\n",
    "\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers import BertForSequenceClassification\n",
    "import transformers\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### toy example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMT(torch.nn.Module):\n",
    "    def __init__(self, mem_size=10, input_size=512, dim=100, out_size=2, \n",
    "                #  k1=None, # backward is done every k1 segments\n",
    "                #  k2=None, # backward uses k2 last segments\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(input_size, dim)\n",
    "        self.weight = torch.nn.Linear(dim, dim)\n",
    "        self.cls = torch.nn.Linear(dim, out_size)\n",
    "        \n",
    "        memory = torch.nn.Parameter(torch.randn(mem_size, dim), requires_grad=True)\n",
    "        self.register_parameter('memory', memory)\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.mem_size = mem_size\n",
    "        self.out_size = out_size\n",
    "        # self.k1 = k1\n",
    "        # self.k2 = k2\n",
    "    \n",
    "    def _forward(self, embedded, labels):\n",
    "        hiddens = self.weight(embedded)        \n",
    "        processed_hiddens = hiddens.mean(dim=1)\n",
    "        logits = self.cls(processed_hiddens)\n",
    "                \n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(logits.softmax(dim=1), labels.squeeze(1))\n",
    "        \n",
    "        return loss, hiddens\n",
    "\n",
    "    def forward(self, segments, labels):\n",
    "        \n",
    "        init_memory = self.memory.unsqueeze(0).repeat(segments[0].shape[0], 1, 1)\n",
    "\n",
    "        memory_states = [(None, init_memory)]\n",
    "        losses = []\n",
    "        for seg_num, X in enumerate(segments):\n",
    "            memory = memory_states[-1][1].detach()\n",
    "            memory.requires_grad = True\n",
    "\n",
    "            embedded = self.embedding(X)\n",
    "            embedded[:, :self.mem_size] = memory\n",
    "            \n",
    "            loss, hiddens = self._forward(embedded, labels)\n",
    "            new_memory = hiddens[:, :self.mem_size]\n",
    "\n",
    "            losses.append(loss)\n",
    "\n",
    "            memory_states.append((memory, new_memory))\n",
    "\n",
    "        self.memory_states = memory_states\n",
    "        \n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "batch_size = 2\n",
    "\n",
    "n_segments = 5\n",
    "mem_size = 10\n",
    "\n",
    "input_size = 512\n",
    "# out_size = 1\n",
    "dim = 100\n",
    "\n",
    "labels = torch.randint(num_classes, (batch_size, 1))\n",
    "\n",
    "\n",
    "segments = [torch.randint(100, (batch_size, input_size)) for _ in range(n_segments)]\n",
    "\n",
    "rmt = RMT(mem_size, input_size=input_size, dim=dim, out_size=num_classes)#, k1=n_segments, k2=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = rmt(segments, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = n_segments\n",
    "k2 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncated_backward(losses, memory_states,\n",
    "                        k1=-1, # update is done every k1 segments\n",
    "                        k2=-1, # backward uses k2 last segments\n",
    "                        ):\n",
    "    losses[-1].backward(retain_graph=False)\n",
    "    for i in range(k2 - 1 if k2 != -1 else len(rmt.memory_states) - 1):\n",
    "        # if we get all the way back to the \"init_memory\", stop\n",
    "        if rmt.memory_states[-i-2][0] is None:\n",
    "            break\n",
    "        curr_grad = rmt.memory_states[-i-1][0].grad\n",
    "        memory_states[-i-2][1].backward(curr_grad, retain_graph=False)\n",
    "\n",
    "    if k1 != -1:\n",
    "        raise NotImplementedError\n",
    "        # if (j+1)%self.k1 == 0:\n",
    "        #     loss = self.loss_module(output, target)\n",
    "\n",
    "        #     optimizer.zero_grad()\n",
    "        #     # backprop last module (keep graph only if they ever overlap)\n",
    "        #     start = time.time()\n",
    "        #     loss.backward(retain_graph=self.retain_graph)\n",
    "        #     for i in range(self.k2-1):\n",
    "        #         # if we get all the way back to the \"init_state\", stop\n",
    "        #         if states[-i-2][0] is None:\n",
    "        #             break\n",
    "        #         curr_grad = states[-i-1][0].grad\n",
    "        #         states[-i-2][1].backward(curr_grad, retain_graph=self.retain_graph)\n",
    "        #     optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmt.memory.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_bptt.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu11/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_bptt.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m losses \u001b[39m=\u001b[39m rmt(segments, labels)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu11/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_bptt.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m losses[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu11/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_bptt.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mmemory gradient: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mweight grad: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mcls grad: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(rmt\u001b[39m.\u001b[39;49mmemory\u001b[39m.\u001b[39;49mgrad\u001b[39m.\u001b[39;49mmean(), rmt\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mmean(), rmt\u001b[39m.\u001b[39mcls\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mmean()))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'mean'"
     ]
    }
   ],
   "source": [
    "losses = rmt(segments, labels)\n",
    "losses[0].backward()\n",
    "print('memory gradient: {}\\nweight grad: {}\\ncls grad: {}'.format(rmt.memory.grad.mean(), rmt.weight.weight.grad.mean(), rmt.cls.weight.grad.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmt.zero_grad()\n",
    "grad_from_loss = []\n",
    "for i, seg in enumerate(segments):\n",
    "    losses = rmt(segments, labels)\n",
    "    losses[i].backward()\n",
    "    grad_from_loss.append(torch.clone(rmt.memory.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.0285e-05), tensor(2.1588e-05))"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(grad_from_loss[0] + grad_from_loss[1] - grad_from_loss[2]).abs().mean(), grad_from_loss[0].abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory gradient: 0.0\n",
      "weight grad: 0.0\n",
      "cls grad: 0.0\n"
     ]
    }
   ],
   "source": [
    "rmt.zero_grad()\n",
    "print('memory gradient: {}\\nweight grad: {}\\ncls grad: {}'.format(rmt.memory.grad.mean(), rmt.weight.weight.grad.mean(), rmt.cls.weight.grad.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory gradient: 9.23935294849798e-06\n",
      "weight grad: -0.00039320866926573217\n",
      "cls grad: -1.19209286886246e-09\n"
     ]
    }
   ],
   "source": [
    "losses = rmt(segments, labels)\n",
    "losses[-1].backward()\n",
    "print('memory gradient: {}\\nweight grad: {}\\ncls grad: {}'.format(rmt.memory.grad.mean(), rmt.weight.weight.grad.mean(), rmt.cls.weight.grad.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.6648, grad_fn=<NllLossBackward>),\n",
       " tensor(0.6615, grad_fn=<NllLossBackward>),\n",
       " tensor(0.6628, grad_fn=<NllLossBackward>),\n",
       " tensor(0.6644, grad_fn=<NllLossBackward>),\n",
       " tensor(0.6562, grad_fn=<NllLossBackward>)]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory gradient: -9.052487257577013e-07\n",
      "weight grad: 5.410353696788661e-05\n",
      "cls grad: 3.725290215195187e-11\n"
     ]
    }
   ],
   "source": [
    "rmt = RMT(mem_size, input_size=input_size, dim=dim, out_size=num_classes)\n",
    "losses = rmt(segments, labels)\n",
    "losses[0].backward()\n",
    "print('memory gradient: {}\\nweight grad: {}\\ncls grad: {}'.format(rmt.memory.grad.mean(), rmt.weight.weight.grad.mean(), rmt.cls.weight.grad.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory gradient: -9.052487257577013e-07\n",
      "weight grad: 5.410353696788661e-05\n",
      "cls grad: 3.725290215195187e-11\n"
     ]
    }
   ],
   "source": [
    "rmt = RMT(mem_size, input_size=input_size, dim=dim, out_size=num_classes)\n",
    "losses = rmt(segments, labels)\n",
    "losses[0].backward()\n",
    "print('memory gradient: {}\\nweight grad: {}\\ncls grad: {}'.format(rmt.memory.grad.mean(), rmt.weight.weight.grad.mean(), rmt.cls.weight.grad.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory gradient: -9.052487257577013e-07\n",
      "weight grad: 0.0\n",
      "cls grad: 0.0\n"
     ]
    }
   ],
   "source": [
    "print('memory gradient: {}\\nweight grad: {}\\ncls grad: {}'.format(rmt.memory.grad.mean(), rmt.weight.weight.grad.mean(), rmt.cls.weight.grad.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, T5ForConditionalGeneration\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from modeling_rmt import RMTEncoderForSequenceClassification\n",
    "from modeling_rmt import RMTEncoderDecoderForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from modeling_rmt.base import RMTBaseModel\n",
    "class RMTEncoderTBPTT(RMTEncoderForSequenceClassification):\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\n",
    "                inputs_embeds=None, labels=None, output_attentions=None, output_hidden_states=None, return_dict=None):\n",
    "        kwargs = {'attention_mask': attention_mask, 'token_type_ids': token_type_ids,\n",
    "                  'position_ids': position_ids, 'inputs_embeds': inputs_embeds,\n",
    "                  'labels': labels, 'output_attentions': output_attentions,\n",
    "                  'output_hidden_states': output_hidden_states, 'return_dict': return_dict,\n",
    "                  }\n",
    "\n",
    "        init_memory = self.set_memory(input_ids.shape)\n",
    "        segmented = self.pad_and_segment(input_ids)\n",
    "        if self.num_mem_tokens == 0:\n",
    "            segmented = segmented[-1:]\n",
    "\n",
    "        memory_states = [(None, init_memory)]\n",
    "        base_model_outputs = []\n",
    "        for seg_num, segment_input_ids in enumerate(segmented):\n",
    "            seg_kwargs, non_empty_mask = self.prepare_kwargs(segment_input_ids, kwargs)\n",
    "            if sum(non_empty_mask) < 1:\n",
    "                raise NotImplementedError\n",
    "            \n",
    "            if sum(non_empty_mask) == 0:\n",
    "                continue\n",
    "            \n",
    "            memory = memory_states[-1][1].detach()\n",
    "            memory.requires_grad = True\n",
    "            # new_memory = memory\n",
    "            # new_memory.requires_grad = True, False\n",
    "            # print('memory', memory.mean(), memory.std(), memory)\n",
    "            seg_kwargs['inputs_embeds'][:, self.memory_position] = memory[non_empty_mask]\n",
    "            out = self.model(**seg_kwargs)\n",
    "            base_model_outputs.append(out)\n",
    "            \n",
    "            self.memory_states = memory_states\n",
    "            new_memory = out.hidden_states[-1][:, self.memory_position]\n",
    "            memory_states.append((memory, new_memory))\n",
    "        \n",
    "        self.memory_states = memory_states\n",
    "\n",
    "        out = self.process_outputs(base_model_outputs, output_attentions, output_hidden_states)\n",
    "        return out\n",
    "    \n",
    "    def truncated_backward(self, k1, k2):\n",
    "        memory_states = self.memory_states\n",
    "        if k1 != -1:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        for i in range(k2 - 1 if k2 != -1 else len(memory_states)):\n",
    "            curr_grad = memory_states[-i-1][0].grad\n",
    "            memory_states[-i-2][1].backward(curr_grad, retain_graph=False)\n",
    "\n",
    "            # if we get all the way back to the \"init_memory\", stop\n",
    "            if memory_states[-i-2][0] is None:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(0):\n",
    "    print(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_segments = 2\n",
    "num_mem_tokens = 10\n",
    "# device = torch.device(3)\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/cryptography/hazmat/backends/openssl/x509.py:14: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-cased'\n",
    "# model_name = 't5-base'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "rmt_config = {'num_mem_tokens': num_mem_tokens, \n",
    "                'max_n_segments': num_segments,\n",
    "                'tokenizer': tokenizer,\n",
    "               #  'memory_layers': 'all', \n",
    "               #  'share_memory_layers': True,\n",
    "               #  'reconstruction_loss_coef': 0.1,\n",
    "                'segment_ordering': 'regular',\n",
    "                'input_size': 512, \n",
    "                'bptt_depth': -1, \n",
    "                'sum_loss': False,\n",
    "             }\n",
    "\n",
    "base_model1 = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "base_model2 = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "# base_model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "# rmt = RMTEncoderDecoderForConditionalGeneration(base_model, **rmt_config)\n",
    "rmt1 = RMTEncoderForSequenceClassification(base_model1, **rmt_config)\n",
    "# rmt2 = RMTEncoderForSequenceClassification(base_model2, **rmt_config)\n",
    "rmt2 = RMTEncoderTBPTT(base_model2, **rmt_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmt2.memory_states[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmt1.model.embeddings.weight.data = rmt2.model.embeddings.weight.data\n",
    "rmt1.model.classifier.weight.data = rmt2.model.classifier.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cpt_path = \"../../runs/framework/qasper/t5-base/lr5e-05_constant_with_warmup_adamw_wd1e-03_512-1024-{1}seg_memNA_bs32_iters5000_regular/run_10/\"\n",
    "# model_cpt = os.path.join(cpt_path, \"model_best.pth\")\n",
    "# cpt = torch.load(model_cpt, map_location='cpu')\n",
    "# base_model.load_state_dict(cpt['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for (n1, p1), (n2, p2) in zip(rmt1.named_parameters(), rmt2.named_parameters()):\n",
    "#     # if getattr(rmt2, n) != p:\n",
    "#     #     print(n)\n",
    "#     if (p1 == p2).float().mean() < 1:\n",
    "#         print(n1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9533, grad_fn=<NllLossBackward>),\n",
       " tensor(0.9533, grad_fn=<NllLossBackward>))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1 = rmt1(sample_input_ids, **kwargs, output_hidden_states=False, output_attentions=False)\n",
    "out2 = rmt2(sample_input_ids, **kwargs, output_hidden_states=False, output_attentions=False)\n",
    "out1.loss, out2.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3574384/3896507179.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  rmt2.memory_states[-2][1].grad\n"
     ]
    }
   ],
   "source": [
    "rmt2.memory_states[-2][1].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = rmt(sample_input_ids, **kwargs, output_hidden_states=False, output_attentions=False)\n",
    "# out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1.loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2.loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9897e-03, -2.3091e-03,  1.8419e-03,  ...,  2.9464e-03,\n",
       "         -6.8678e-04, -5.5236e-06],\n",
       "        [ 4.1039e-04, -2.2907e-03, -4.0704e-03,  ..., -2.6411e-03,\n",
       "         -4.5289e-04,  1.3317e-03],\n",
       "        [-4.1274e-04, -3.8551e-04, -2.8217e-03,  ...,  4.2580e-04,\n",
       "         -1.1218e-03, -1.6782e-03],\n",
       "        ...,\n",
       "        [ 4.0664e-05, -2.9215e-03,  1.5561e-03,  ...,  8.5625e-04,\n",
       "          3.1595e-04, -1.4923e-03],\n",
       "        [ 2.7264e-03,  4.9132e-04, -6.6655e-03,  ...,  4.7209e-03,\n",
       "         -1.2776e-03, -4.0603e-03],\n",
       "        [ 3.0761e-03, -3.3016e-03, -6.4506e-03,  ..., -3.9267e-03,\n",
       "         -7.8163e-03, -9.2225e-04]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmt1.model.embeddings.weight.grad[rmt1.mem_token_ids] - rmt2.model.embeddings.weight.grad[rmt1.mem_token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmt2.truncated_backward(k1=-1, k2=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmt1.model.embeddings.weight.grad[rmt1.mem_token_ids] - rmt2.model.embeddings.weight.grad[rmt1.mem_token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(memory_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmt.truncated_backward(k1=-1, k2=1)\n",
    "\n",
    "k1 = -1\n",
    "k2 = 1\n",
    "self = rmt\n",
    "memory_states = self.memory_states\n",
    "if k1 != -1:\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_states[-i-2][0].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "curr_grad = memory_states[-i-1][0].grad\n",
    "\n",
    "memory_states[-i-2][1].backward(curr_grad, retain_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 8.7065e-04,  1.6680e-04, -1.2168e-03,  ..., -1.7849e-04,\n",
       "           5.4806e-04,  2.0493e-03],\n",
       "         [-1.5455e-03, -1.3318e-03, -6.9536e-03,  ..., -2.6797e-03,\n",
       "          -6.9923e-03,  1.3776e-03],\n",
       "         [ 7.5854e-03,  3.5826e-03, -2.6303e-03,  ..., -2.0050e-03,\n",
       "           1.8138e-03, -3.5395e-03],\n",
       "         ...,\n",
       "         [-2.2143e-03, -1.2456e-03,  3.8501e-03,  ...,  2.7255e-03,\n",
       "          -1.9535e-03,  2.2969e-04],\n",
       "         [ 6.5699e-03,  3.4641e-03, -2.5996e-03,  ..., -3.7634e-03,\n",
       "           4.2661e-03, -3.9519e-03],\n",
       "         [ 5.8664e-03, -5.6410e-03,  4.6935e-03,  ...,  1.3502e-04,\n",
       "          -4.6932e-03, -3.7547e-03]],\n",
       "\n",
       "        [[-1.1527e-03, -1.1127e-06,  2.6598e-03,  ..., -7.1365e-03,\n",
       "           2.9748e-04,  5.6316e-03],\n",
       "         [ 1.7341e-03,  5.5398e-03,  6.3473e-03,  ...,  1.0567e-03,\n",
       "           2.5097e-03,  4.1832e-03],\n",
       "         [-2.7789e-03, -1.8359e-03,  2.4206e-03,  ...,  1.8355e-03,\n",
       "           9.1351e-04,  1.0039e-02],\n",
       "         ...,\n",
       "         [-1.6219e-02, -1.3918e-02, -1.1967e-02,  ...,  2.2558e-03,\n",
       "          -1.0039e-02,  5.6604e-03],\n",
       "         [-6.2256e-03, -2.8997e-03,  5.5948e-03,  ...,  7.8800e-03,\n",
       "          -4.1549e-03,  1.3720e-02],\n",
       "         [ 2.8358e-03,  9.1771e-04, -3.1378e-03,  ...,  7.9243e-04,\n",
       "           8.8960e-05,  7.3513e-03]]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_states[-i-2][0].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(k2 - 1 if k2 != -1 else len(memory_states)):\n",
    "    print()\n",
    "    curr_grad = memory_states[-i-1][0].grad\n",
    "    memory_states[-i-2][1].backward(curr_grad, retain_graph=False)\n",
    "\n",
    "    # if we get all the way back to the \"init_memory\", stop\n",
    "    if memory_states[-i-2][0] is None:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([29006, 768])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmt.model.embeddings.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmt.model.embeddings.weight.grad[rmt.mem_token_ids]#.abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-7.0348e-04, -5.1936e-03, -1.7022e-03,  ..., -4.8533e-04,\n",
       "          -1.4215e-03,  2.2574e-03],\n",
       "         [-4.5974e-03,  5.1353e-03, -5.1554e-05,  ...,  9.8471e-03,\n",
       "          -8.8535e-04,  6.8188e-03],\n",
       "         [-1.9443e-03,  7.3353e-04, -3.4847e-03,  ...,  3.4695e-04,\n",
       "           1.9930e-03, -3.3250e-03],\n",
       "         ...,\n",
       "         [ 1.4958e-03, -1.0761e-02,  2.7553e-02,  ...,  1.3110e-02,\n",
       "           8.6204e-03,  1.2122e-02],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [-4.0009e-03,  5.6785e-03, -3.3421e-03,  ..., -1.6336e-02,\n",
       "          -6.6896e-04,  8.5934e-03]],\n",
       "\n",
       "        [[-7.0348e-04, -5.1936e-03, -1.7022e-03,  ..., -4.8533e-04,\n",
       "          -1.4215e-03,  2.2574e-03],\n",
       "         [-8.5180e-04,  4.4820e-04, -3.6088e-04,  ..., -7.8554e-05,\n",
       "          -2.9245e-05, -6.2273e-04],\n",
       "         [-1.1675e-03,  9.3001e-04, -1.3836e-04,  ...,  5.5215e-04,\n",
       "           1.0142e-03,  3.8922e-04],\n",
       "         ...,\n",
       "         [-1.2561e-03, -3.9388e-04,  1.4310e-03,  ...,  7.5279e-03,\n",
       "          -7.2989e-05,  3.1132e-03],\n",
       "         [ 3.7742e-04,  5.2707e-05, -7.0312e-04,  ...,  2.0590e-03,\n",
       "          -1.0606e-03,  1.3453e-04],\n",
       "         [-4.0009e-03,  5.6785e-03, -3.3421e-03,  ..., -1.6336e-02,\n",
       "          -6.6896e-04,  8.5934e-03]]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmt.model.embeddings.weight.grad[sample_input_ids]#.abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(251.0535)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rmt.model.embeddings.weight.grad.abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(16.0130, grad_fn=<NllLossBackward>),\n",
       " tensor(15.4830, grad_fn=<NllLossBackward>),\n",
       " tensor(14.7046, grad_fn=<NllLossBackward>))"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['loss_0'], out['loss_1'], out['loss_2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Holder:\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq_len = 512\n",
    "target_seq_len = 1024\n",
    "batch_size = 2\n",
    "\n",
    "args = Holder\n",
    "args.target_seq_len = target_seq_len\n",
    "args.input_seq_len = input_seq_len\n",
    "args.input_prefix = ''\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_attention_first_token = False  # should be True for LED\n",
    "encode_plus_kwargs = {'truncation': True, 'padding': 'longest', 'pad_to_multiple_of': 1}\n",
    "# generate_kwargs = {'max_length': args.target_seq_len, 'min_length': args.target_seq_len}\n",
    "generate_kwargs = {}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # cut too long strings because they may slow down tokenization\n",
    "    inputs = [b['input'][:args.input_seq_len * 10] for b in batch]\n",
    "    if 'outputs' in batch[0]:\n",
    "        # if we have more than 1 label per example (only in valid) take only one of them\n",
    "        # to compute loss on valid\n",
    "        labels = [b['outputs'][0][:args.target_seq_len * 10] for b in batch]\n",
    "    else:\n",
    "        labels = [b['output'][:args.target_seq_len * 10] for b in batch]\n",
    "    if args.input_prefix:\n",
    "        inputs = [args.input_prefix + inp for inp in inputs]\n",
    "    features = tokenizer.batch_encode_plus(list(inputs), max_length=args.input_seq_len, return_tensors='pt',\n",
    "                                           **encode_plus_kwargs)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer.batch_encode_plus(list(labels), max_length=args.target_seq_len, return_tensors='pt',\n",
    "                                             **encode_plus_kwargs).input_ids\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    features['labels'] = labels\n",
    "    features['id'] = [b['id'] for b in batch]\n",
    "    if 'outputs' in batch[0]:\n",
    "        features['target_text'] = [b['outputs'] for b in batch]\n",
    "    else:\n",
    "        features['target_text'] = [b['output'] for b in batch]\n",
    "    if 'global_attention_mask' in features:\n",
    "        raise RuntimeError('What global attention mask for Longformer and LongformerEncoder-Decoder should be?')\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/cryptography/hazmat/backends/openssl/x509.py:14: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.\n",
      "  warnings.warn(\n",
      "Reusing dataset scrolls (/home/bulatov/.cache/huggingface/datasets/tau___scrolls/qasper/1.0.0/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3acd0cf87d0d4d25a1f3608a7d13d68a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_name = 'qasper'\n",
    "dataset = datasets.load_dataset('tau/scrolls', task_name)\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "train_sampler = RandomSampler(train_dataset,)\n",
    "kwargs = {'pin_memory': True, 'num_workers': 0}\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler,\n",
    "                                collate_fn=collate_fn, **kwargs)\n",
    "\n",
    "valid_dataset = dataset['validation']\n",
    "valid_sampler = RandomSampler(valid_dataset)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, sampler=train_sampler,\n",
    "                                collate_fn=collate_fn, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = iter(train_dataloader)\n",
    "sample = next(gen)\n",
    "\n",
    "if 'id' in sample:\n",
    "    id = sample.pop('id')\n",
    "if 'target_text' in sample:\n",
    "    tgt_text = sample.pop('target_text')\n",
    "\n",
    "for k in sample:\n",
    "    sample[k] = sample[k].to(device)\n",
    "    \n",
    "sample_input_ids = sample.pop('input_ids').to(device)\n",
    "kwargs = sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq_len = 1536\n",
    "target_seq_len = 3\n",
    "batch_size = 2\n",
    "\n",
    "args = Holder\n",
    "args.target_seq_len = target_seq_len\n",
    "args.input_seq_len = input_seq_len\n",
    "args.input_prefix = ''\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_plus_kwargs = {'max_length': args.input_seq_len,\n",
    "                        'truncation': True,\n",
    "                        'padding': 'longest',\n",
    "                        'pad_to_multiple_of': 1}\n",
    "generate_kwargs = {}\n",
    "labels_map = {'Contradiction': 0, 'Entailment': 1, 'Not mentioned': 2}\n",
    "num_labels = len(labels_map)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # cut too long strings because they may slow down tokenization\n",
    "    inputs = [b['input'][:args.input_seq_len * 10] for b in batch]\n",
    "    labels = [b['output'][:args.target_seq_len * 10] for b in batch]\n",
    "    if args.input_prefix:\n",
    "        inputs = [args.input_prefix + inp for inp in inputs]\n",
    "    features = tokenizer.batch_encode_plus(list(inputs), return_tensors='pt', **encode_plus_kwargs)\n",
    "    labels = np.array([labels_map[t] for t in labels])\n",
    "    features['labels'] = torch.from_numpy(labels)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset scrolls (/home/bulatov/.cache/huggingface/datasets/tau___scrolls/contract_nli/1.0.0/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c91f96fda4f14decb9112b653512e5f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "task_name = 'contract_nli'\n",
    "dataset = datasets.load_dataset('tau/scrolls', task_name)\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "train_sampler = RandomSampler(train_dataset,)\n",
    "kwargs = {'pin_memory': True, 'num_workers': 0}\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler,\n",
    "                                collate_fn=collate_fn, **kwargs)\n",
    "\n",
    "valid_dataset = dataset['validation']\n",
    "valid_sampler = RandomSampler(valid_dataset)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, sampler=train_sampler,\n",
    "                                collate_fn=collate_fn, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = iter(train_dataloader)\n",
    "sample = next(gen)\n",
    "\n",
    "if 'id' in sample:\n",
    "    id = sample.pop('id')\n",
    "if 'target_text' in sample:\n",
    "    tgt_text = sample.pop('target_text')\n",
    "\n",
    "# rmt.to(device)\n",
    "for k in sample:\n",
    "    sample[k] = sample[k].to(device)\n",
    "    \n",
    "sample_input_ids = sample.pop('input_ids').to(device)\n",
    "kwargs = sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (2) to match target batch_size (12).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_bptt.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu11/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_bptt.ipynb#Y131sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m out \u001b[39m=\u001b[39m rmt(sample_input_ids, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs, output_hidden_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, output_attentions \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu11/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_bptt.ipynb#Y131sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m out\u001b[39m.\u001b[39mkeys()\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/RMT_light/framework/notebooks/../modeling_rmt/sequence_classification.py:29\u001b[0m, in \u001b[0;36mRMTEncoderForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m     28\u001b[0m seg_kwargs[\u001b[39m'\u001b[39m\u001b[39minputs_embeds\u001b[39m\u001b[39m'\u001b[39m][:, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory_position] \u001b[39m=\u001b[39m memory[non_empty_mask]\n\u001b[0;32m---> 29\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mseg_kwargs)\n\u001b[1;32m     30\u001b[0m base_model_outputs\u001b[39m.\u001b[39mappend(out)\n\u001b[1;32m     32\u001b[0m memory[non_empty_mask] \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mhidden_states[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][:, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory_position]\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:1580\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1578\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mproblem_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msingle_label_classification\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1579\u001b[0m     loss_fct \u001b[39m=\u001b[39m CrossEntropyLoss()\n\u001b[0;32m-> 1580\u001b[0m     loss \u001b[39m=\u001b[39m loss_fct(logits\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_labels), labels\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[1;32m   1581\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mproblem_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmulti_label_classification\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1582\u001b[0m     loss_fct \u001b[39m=\u001b[39m BCEWithLogitsLoss()\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/nn/modules/loss.py:961\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    960\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 961\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    962\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/nn/functional.py:2468\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2466\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2467\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 2468\u001b[0m \u001b[39mreturn\u001b[39;00m nll_loss(log_softmax(\u001b[39minput\u001b[39;49m, \u001b[39m1\u001b[39;49m), target, weight, \u001b[39mNone\u001b[39;49;00m, ignore_index, \u001b[39mNone\u001b[39;49;00m, reduction)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/nn/functional.py:2261\u001b[0m, in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2258\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mExpected 2 or more dimensions (got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(dim))\n\u001b[1;32m   2260\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39m!=\u001b[39m target\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m):\n\u001b[0;32m-> 2261\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mExpected input batch_size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) to match target batch_size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m).\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   2262\u001b[0m                      \u001b[39m.\u001b[39mformat(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), target\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)))\n\u001b[1;32m   2263\u001b[0m \u001b[39mif\u001b[39;00m dim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m   2264\u001b[0m     ret \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39mnll_loss(\u001b[39minput\u001b[39m, target, weight, _Reduction\u001b[39m.\u001b[39mget_enum(reduction), ignore_index)\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (2) to match target batch_size (12)."
     ]
    }
   ],
   "source": [
    "out = rmt(sample_input_ids, **kwargs, output_hidden_states=True, output_attentions = True)\n",
    "out.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# toy T-BPTT example from source\n",
    "https://discuss.pytorch.org/t/truncated-backprop-data-clarification/34854"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_bptt.ipynb Cell 44\u001b[0m in \u001b[0;36m<cell line: 49>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu11/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_bptt.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m layer_size \u001b[39m=\u001b[39m \u001b[39m50\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu11/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_bptt.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m idx \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu11/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_bptt.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mMyMod\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu11/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_bptt.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu11/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_bptt.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m         \u001b[39msuper\u001b[39m(MyMod, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class TBPTT():\n",
    "    def __init__(self, one_step_module, loss_module, k1, k2, optimizer):\n",
    "        self.one_step_module = one_step_module\n",
    "        self.loss_module = loss_module\n",
    "        self.k1 = k1 # update period\n",
    "        self.k2 = k2 # steps to use in bptt\n",
    "        self.retain_graph = k1 < k2\n",
    "        # You can also remove all the optimizer code here, and the\n",
    "        # train function will just accumulate all the gradients in\n",
    "        # one_step_module parameters\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def train(self, input_sequence, init_state):\n",
    "        states = [(None, init_state)]\n",
    "        for j, (inp, target) in enumerate(input_sequence):\n",
    "\n",
    "            state = states[-1][1].detach()\n",
    "            state.requires_grad=True\n",
    "            output, new_state = self.one_step_module(inp, state)\n",
    "            states.append((state, new_state))\n",
    "\n",
    "            while len(states) > self.k2:\n",
    "                # Delete stuff that is too old\n",
    "                del states[0]\n",
    "\n",
    "            if (j+1)%self.k1 == 0:\n",
    "                loss = self.loss_module(output, target)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                # backprop last module (keep graph only if they ever overlap)\n",
    "                start = time.time()\n",
    "                loss.backward(retain_graph=self.retain_graph)\n",
    "                for i in range(self.k2-1):\n",
    "                    # if we get all the way back to the \"init_state\", stop\n",
    "                    if states[-i-2][0] is None:\n",
    "                        break\n",
    "                    curr_grad = states[-i-1][0].grad\n",
    "                    states[-i-2][1].backward(curr_grad, retain_graph=self.retain_graph)\n",
    "                print(\"bw: {}\".format(time.time()-start))\n",
    "                optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "seq_len = 20\n",
    "layer_size = 50\n",
    "\n",
    "idx = 0\n",
    "\n",
    "class MyMod(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyMod, self).__init__()\n",
    "        self.lin = nn.Linear(2*layer_size, 2*layer_size)\n",
    "\n",
    "    def forward(self, inp, state):\n",
    "        global idx\n",
    "        full_out = self.lin(torch.cat([inp, state], 1))\n",
    "        # out, new_state = full_out.chunk(2, dim=1)\n",
    "        out = full_out.narrow(1, 0, layer_size)\n",
    "        new_state = full_out.narrow(1, layer_size, layer_size)\n",
    "        def get_pr(idx_val):\n",
    "            def pr(*args):\n",
    "                print(\"doing backward {}\".format(idx_val))\n",
    "            return pr\n",
    "        new_state.register_hook(get_pr(idx))\n",
    "        out.register_hook(get_pr(idx))\n",
    "        print(\"doing fw {}\".format(idx))\n",
    "        idx += 1\n",
    "        return out, new_state\n",
    "\n",
    "\n",
    "one_step_module = MyMod()\n",
    "loss_module = nn.MSELoss()\n",
    "input_sequence = [(torch.rand(200, layer_size), torch.rand(200, layer_size))] * seq_len\n",
    "\n",
    "optimizer = torch.optim.SGD(one_step_module.parameters(), lr=1e-3)\n",
    "\n",
    "runner = TBPTT(one_step_module, loss_module, 5, 7, optimizer)\n",
    "\n",
    "runner.train(input_sequence, torch.zeros(200, layer_size))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hvdenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
