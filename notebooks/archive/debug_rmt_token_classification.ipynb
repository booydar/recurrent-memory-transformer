{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers import PreTrainedModel, AutoModelForSequenceClassification, T5ForConditionalGeneration\n",
    "from transformers import AutoTokenizer\n",
    "import datasets\n",
    "\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers import BertForSequenceClassification\n",
    "import transformers\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "# from modeling_rmt import RMTEncoderForSequenceClassification\n",
    "# from modeling_rmt_enc_dec import RMTEncoderDecoderForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RMTBaseModel(torch.nn.Module):\n",
    "    def __init__(self, base_model, **rmt_kwargs):\n",
    "        super().__init__()\n",
    "        self.model = base_model\n",
    "        self.set_params(**rmt_kwargs)\n",
    "\n",
    "    def set_params(self, num_mem_tokens, tokenizer, **rmt_config):\n",
    "        self.rmt_config = rmt_config\n",
    "        self.extract_special_tokens(tokenizer)\n",
    "        self.extend_word_embeddings(num_mem_tokens, tokenizer)\n",
    "\n",
    "        self.segment_size = rmt_config['input_size'] - num_mem_tokens - tokenizer.num_special_tokens_to_add()\n",
    "        if 'sep_token' in tokenizer.special_tokens_map:\n",
    "            self.segment_size -= 1\n",
    "\n",
    "    def set_memory(self, input_shape):\n",
    "        memory = self.model.embeddings(self.mem_token_ids)\n",
    "        memory = memory.repeat(input_shape[0], 1, 1)\n",
    "        return memory\n",
    "\n",
    "    def extract_special_tokens(self, tokenizer):\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "        self.special_token_ids = [tokenizer.pad_token_id]\n",
    "        for token in ['cls_token', 'sep_token', 'eos_token', 'bos_token']:\n",
    "            token_id = getattr(tokenizer, f'{token}_id')\n",
    "            if token_id is not None:\n",
    "                self.register_buffer(token, torch.tensor([token_id]))\n",
    "                self.special_token_ids.append(token_id)\n",
    "            else:\n",
    "                setattr(self, token, None)\n",
    "\n",
    "    def extend_word_embeddings(self, num_mem_tokens, tokenizer):\n",
    "            \n",
    "        vocab_size = self.model.config.vocab_size\n",
    "        extended_vocab_size = vocab_size + num_mem_tokens\n",
    "        self.num_mem_tokens = num_mem_tokens\n",
    "        self.register_buffer('mem_token_ids', torch.arange(vocab_size, vocab_size + num_mem_tokens))\n",
    "        self.model.resize_token_embeddings(extended_vocab_size)\n",
    "\n",
    "        special_tokens = tokenizer.special_tokens_map\n",
    "        mem_start_ind = int('cls_token' in special_tokens or 'bos_token' in special_tokens)\n",
    "        self.memory_position = range(mem_start_ind, mem_start_ind + num_mem_tokens)\n",
    "        \n",
    "        if hasattr(self.model.base_model, 'embeddings'): # enc-only\n",
    "            self.model.embeddings = self.model.base_model.embeddings.word_embeddings\n",
    "        elif hasattr(self.model.encoder, 'embed_tokens'): # enc-dec\n",
    "            self.model.embeddings = self.model.encoder.embed_tokens\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "       raise NotImplementedError\n",
    "\n",
    "    def pad_and_segment(self, input_ids):\n",
    "        segmented_batch = []\n",
    "        for seq in input_ids:\n",
    "            drop_mask = sum([seq == t for t in self.special_token_ids])\n",
    "            seq = seq[(1 - drop_mask).bool()]\n",
    "            seq = seq[:self.segment_size * self.rmt_config['max_n_segments']]\n",
    "\n",
    "            align = self.rmt_config.get('segment_alignment')\n",
    "            if align in {'right', None}:\n",
    "                split_inds = (list(range(len(seq), 0, -self.segment_size)) + [0])[::-1]\n",
    "                input_segments = [seq[start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "            elif align == 'left':\n",
    "                split_inds = list(range(0, len(seq), self.segment_size)) + [len(seq)]\n",
    "                input_segments = [seq[start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "            else:\n",
    "                n_seg = math.ceil(len(seq) / self.segment_size)\n",
    "                input_segments = torch.chunk(seq, n_seg)\n",
    "\n",
    "            input_segments = [self.pad_add_special_tokens(t, self.rmt_config['input_size']) for t in input_segments]\n",
    "\n",
    "            # add empty segment markers if needed\n",
    "            n_empty_segments = self.rmt_config['max_n_segments'] - len(input_segments)\n",
    "            input_segments = [None] * n_empty_segments + input_segments\n",
    "\n",
    "            segmented_batch.append(input_segments)\n",
    "\n",
    "        segmented_batch = [[sample[seg_num] for sample in segmented_batch] \\\n",
    "                            for seg_num in range(self.rmt_config['max_n_segments'])]\n",
    "        return segmented_batch\n",
    "\n",
    "    def pad_add_special_tokens(self, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def prepare_kwargs(self, segment_input_ids, kwargs):\n",
    "        seg_kwargs = dict(**kwargs)\n",
    "        non_empty_mask = [s is not None for s in segment_input_ids]\n",
    "        if sum(non_empty_mask) == 0:\n",
    "            return None, non_empty_mask\n",
    "            \n",
    "        input_ids = torch.stack([s for s in segment_input_ids if s is not None])\n",
    "        inputs_embeds = self.model.embeddings(input_ids)\n",
    "\n",
    "        seg_kwargs['input_ids'] = None\n",
    "        seg_kwargs['inputs_embeds'] = inputs_embeds\n",
    "        if seg_kwargs.get('labels') is not None:\n",
    "            seg_kwargs['labels'] = seg_kwargs['labels'][non_empty_mask]\n",
    "        seg_kwargs['attention_mask'] = self.get_attention_mask(input_ids)\n",
    "        if seg_kwargs.get('token_type_ids') is not None:\n",
    "            seg_kwargs['token_type_ids'] = self.get_token_type_ids(input_ids)\n",
    "        seg_kwargs['output_hidden_states'] = True\n",
    "\n",
    "        return seg_kwargs, non_empty_mask\n",
    "\n",
    "    def process_outputs(self, model_outputs, output_attentions, output_hidden_states):\n",
    "        rmt_out = model_outputs[-1]\n",
    "\n",
    "        segment_keys = ['loss']\n",
    "        if output_attentions:\n",
    "            segment_keys.append('attentions')\n",
    "        if output_hidden_states:\n",
    "            segment_keys.append('hidden_states')\n",
    "\n",
    "        extracted = {}\n",
    "        for seg_num, out in enumerate(model_outputs):\n",
    "            for key, value in out.items():\n",
    "                if any([sk in key for sk in segment_keys]):\n",
    "                    extracted[f'{key}_{seg_num}'] = value\n",
    "\n",
    "        if self.rmt_config['sum_loss']:\n",
    "            losses = [out['loss'] for out in model_outputs]\n",
    "            extracted['loss'] = torch.stack(losses).mean(dim=0)\n",
    "\n",
    "        for key, value in extracted.items():\n",
    "            rmt_out[key] = value\n",
    "        \n",
    "        # drop unnecessary hiddens to save memory\n",
    "        if not output_hidden_states:\n",
    "            for key in rmt_out.keys():\n",
    "                if 'hidden_state' in key:\n",
    "                    rmt_out[key] = None\n",
    "\n",
    "        return rmt_out \n",
    "        \n",
    "    def get_token_type_ids(self, tensor):\n",
    "        return torch.zeros_like(tensor)\n",
    "\n",
    "    def get_attention_mask(self, tensor):\n",
    "        mask = torch.ones_like(tensor)\n",
    "        mask[tensor == self.pad_token_id] = 0\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# from .base import RMTBaseModel\n",
    "\n",
    "class RMTEncoderForSequenceClassification(RMTBaseModel):\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\n",
    "                inputs_embeds=None, labels=None, output_attentions=None, output_hidden_states=None, return_dict=None):\n",
    "        kwargs = {'attention_mask': attention_mask, 'token_type_ids': token_type_ids,\n",
    "                  'position_ids': position_ids, 'inputs_embeds': inputs_embeds,\n",
    "                  'labels': labels, 'output_attentions': output_attentions,\n",
    "                  'output_hidden_states': output_hidden_states, 'return_dict': return_dict,\n",
    "                  }\n",
    "\n",
    "        memory = self.set_memory(input_ids.shape)\n",
    "        segmented = self.pad_and_segment(input_ids)\n",
    "\n",
    "        base_model_outputs = []\n",
    "        for seg_num, segment_input_ids in enumerate(segmented):                \n",
    "            if self.rmt_config['bptt_depth'] != -1:\n",
    "                raise NotImplementedError\n",
    "\n",
    "            seg_kwargs, non_empty_mask = self.prepare_kwargs(segment_input_ids, kwargs)\n",
    "            if sum(non_empty_mask) == 0:\n",
    "                continue\n",
    "            \n",
    "            seg_kwargs['inputs_embeds'][:, self.memory_position] = memory[non_empty_mask]\n",
    "            out = self.model(**seg_kwargs)\n",
    "            base_model_outputs.append(out)\n",
    "            \n",
    "            memory[non_empty_mask] = out.hidden_states[-1][:, self.memory_position]\n",
    "\n",
    "        out = self.process_outputs(base_model_outputs, output_attentions, output_hidden_states)\n",
    "        return out\n",
    "\n",
    "    def pad_add_special_tokens(self, tensor, segment_size):\n",
    "        input_elements = []\n",
    "        input_elements += [self.cls_token, self.mem_token_ids, self.sep_token, tensor, self.sep_token]\n",
    "        tensor = torch.cat(input_elements)\n",
    "\n",
    "        pad_size = segment_size - tensor.shape[0]\n",
    "        if pad_size > 0:\n",
    "            tensor = F.pad(tensor, (0, pad_size))\n",
    "        return tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# from .sequence_classification import RMTEncoderForSequenceClassification\n",
    "\n",
    "class RMTEncoderForTokenClassification(RMTEncoderForSequenceClassification):\n",
    "    # todo: move segment looping into RMT class, also move help functions into RMT class\n",
    "    def __init__(self, base_model, **rmt_kwargs):\n",
    "        super().__init__(base_model, **rmt_kwargs)\n",
    "        self.rmt_config['sum_loss'] = True\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\n",
    "                inputs_embeds=None, labels=None, labels_mask=None, pos_weight=None, output_attentions=None,\n",
    "                output_hidden_states=None, return_dict=None):\n",
    "        # todo: currently output from RMT model is not the same like from backbone model with 1 segment\n",
    "        # because of inserted memory tokens and operations with cls/sep/pad in pad_and_segment\n",
    "        # need to impl such that output from forward is like output from backbone model:\n",
    "        # input -> segmented_inp -> segmented_logits -> output\n",
    "        #                               | -> loss         | -> metrics\n",
    "        #                           segmented_labels <- labels\n",
    "\n",
    "        kwargs = {'input_ids': input_ids, 'attention_mask': attention_mask, 'token_type_ids': token_type_ids,\n",
    "                  'position_ids': position_ids, 'head_mask': head_mask, 'inputs_embeds': inputs_embeds,\n",
    "                  'labels': labels, 'labels_mask': labels_mask, 'pos_weight': pos_weight,\n",
    "                  'output_attentions': output_attentions, 'output_hidden_states': output_hidden_states,\n",
    "                  'return_dict': return_dict,\n",
    "                  }\n",
    "        memory = self.set_memory(input_ids.shape)\n",
    "        segmented = self.pad_and_segment(input_ids, labels, labels_mask)\n",
    "\n",
    "        base_model_outputs = []\n",
    "        for seg_num, segment in enumerate(zip(*segmented)):\n",
    "            if self.rmt_config['bptt_depth'] > -1:\n",
    "                raise NotImplementedError\n",
    "\n",
    "            seg_kwargs, non_empty_mask = self.prepare_kwargs(segment, kwargs)\n",
    "            seg_kwargs['inputs_embeds'][:, self.memory_position] = memory[non_empty_mask]\n",
    "\n",
    "            out = self.model(**seg_kwargs)\n",
    "            memory[non_empty_mask] = out.hidden_states[-1][:, self.memory_position]\n",
    "            \n",
    "            out['seg_kwargs'] = seg_kwargs\n",
    "            base_model_outputs.append(out)\n",
    "\n",
    "        out = self.process_outputs(input_ids, base_model_outputs, output_attentions, output_hidden_states)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def prepare_kwargs(self, segment, kwargs):\n",
    "        segment_input_ids, segment_labels, segment_labels_mask = segment\n",
    "        seg_kwargs = dict(**kwargs)\n",
    "        non_empty_mask = [s is not None for s in segment_input_ids]\n",
    "        if sum(non_empty_mask) == 0:\n",
    "            return None, non_empty_mask\n",
    "            \n",
    "        input_ids = torch.stack([s for s in segment_input_ids if s is not None])\n",
    "        inputs_embeds = self.model.embeddings(input_ids)\n",
    "\n",
    "        seg_kwargs['input_ids'] = None\n",
    "        seg_kwargs['inputs_embeds'] = inputs_embeds\n",
    "        if seg_kwargs.get('labels') is not None:\n",
    "            seg_kwargs['labels'] = seg_kwargs['labels'][non_empty_mask]\n",
    "        seg_kwargs['attention_mask'] = self.get_attention_mask(input_ids)\n",
    "        if seg_kwargs.get('token_type_ids') is not None:\n",
    "            seg_kwargs['token_type_ids'] = self.get_token_type_ids(input_ids)\n",
    "        seg_kwargs['output_hidden_states'] = True\n",
    "        if seg_kwargs['labels'] is not None:\n",
    "            seg_kwargs['labels'] = torch.stack([el for el, m in zip(segment_labels, non_empty_mask) if m])\n",
    "        if seg_kwargs['labels_mask'] is not None:\n",
    "            seg_kwargs['labels_mask'] = torch.stack([el for el, m in zip(segment_labels_mask, non_empty_mask) if m])\n",
    "        if pos_weight is not None:\n",
    "            # all values in the second dimension of pos_weight should be the same\n",
    "            pos_weight = pos_weight[0, 0, :][None, None, :]\n",
    "            segm_bs, segm_seq_len, _ = seg_kwargs['labels'].shape\n",
    "            seg_kwargs['pos_weight'] = pos_weight.repeat(segm_bs, segm_seq_len, 1)\n",
    "\n",
    "        return seg_kwargs, non_empty_mask\n",
    "\n",
    "\n",
    "    def process_outputs(self, input_ids, model_outputs, output_attentions, output_hidden_states):\n",
    "        rmt_out = model_outputs[-1]\n",
    "\n",
    "        bs, seq_len = input_ids.shape\n",
    "\n",
    "        seg_kwargs = out['seg_kwargs']\n",
    "        losses = []\n",
    "        logits = []\n",
    "        logits_masks = []\n",
    "        labels_segm = []\n",
    "        for out in model_outputs:\n",
    "            losses.append(out['loss'])\n",
    "            logits.append(out['logits'].detach())\n",
    "            labels_segm += [seg_kwargs['labels']]\n",
    "\n",
    "            if seg_kwargs['labels_mask'] is not None:\n",
    "                logits_masks.append(seg_kwargs['labels_mask'])\n",
    "\n",
    "        # drop unnecessary hiddens to save memory\n",
    "        if not output_hidden_states:\n",
    "            for key in out.keys():\n",
    "                if 'hidden_state' in key:\n",
    "                    out[key] = None\n",
    "\n",
    "        for i, l in enumerate(losses):\n",
    "            out[f'loss_{i}'] = l.mean()\n",
    "\n",
    "        # aggregate losses from all segments\n",
    "        out['loss'] = torch.stack(losses).mean()\n",
    "\n",
    "        # some sequences are skipped in some batches if they are empty, we need to put dummy predictions for them.\n",
    "        # this may lead to different order of samples in the batch, but we modify order of labels and masks as well\n",
    "        for i in range(len(logits)):\n",
    "            logits[i] = F.pad(logits[i], (0, 0, 0, 0, 0, bs - logits[i].shape[0]))\n",
    "            labels_segm[i] = F.pad(labels_segm[i], (0, 0, 0, 0, 0, bs - labels_segm[i].shape[0]))\n",
    "            if len(logits_masks) > 0:\n",
    "                logits_masks[i] = F.pad(logits_masks[i], (0, 0, 0, bs - logits_masks[i].shape[0]))\n",
    "\n",
    "        out['logits'] = torch.cat(logits, dim=1)\n",
    "        # Warning: rmt logits, labels, masks are not in the same order as in input data:\n",
    "        # the first dimension is number of segments!\n",
    "        # so, torch.cat will result in segm0, segm0,.. and only after all segm0 will come segm1, ... .\n",
    "        # not segm0, segm1, segm0, segm1 as in input data\n",
    "        out['logits_segm'] = [logits]\n",
    "        out['labels_segm'] = [labels_segm]\n",
    "        if len(logits_masks) > 0:\n",
    "            out['rmt_logits_masks'] = torch.cat(logits_masks, dim=1)\n",
    "            out['rmt_logits_masks_segm'] = [logits_masks]\n",
    "\n",
    "        return rmt_out \n",
    "\n",
    "    def pad_and_segment(self, input_ids, labels=None, labels_mask=None):\n",
    "        segmented_batch = []\n",
    "        segmented_batch_labels = []\n",
    "        segmented_batch_labels_mask = []\n",
    "\n",
    "        if labels is None:\n",
    "            labels = [None] * input_ids.shape[0]\n",
    "        batch_labels = labels\n",
    "\n",
    "        if labels_mask is None:\n",
    "            labels_mask = [None] * input_ids.shape[0]\n",
    "        batch_labels_mask = labels_mask\n",
    "\n",
    "        for seq, labels, labels_mask in zip(input_ids, batch_labels, batch_labels_mask):\n",
    "            content_tokens_mask = (seq != self.pad_token_id) & (seq != self.cls_token.item()) & (seq != self.sep_token.item())\n",
    "            seq = seq[content_tokens_mask]\n",
    "            seq = seq[:self.segment_size * self.rmt_config['max_n_segments']]\n",
    "            if labels is not None:\n",
    "                labels = labels[content_tokens_mask]\n",
    "                labels = labels[:self.segment_size * self.rmt_config['max_n_segments']]\n",
    "            if labels_mask is not None:\n",
    "                labels_mask = labels_mask[content_tokens_mask]\n",
    "                labels_mask = labels_mask[:self.segment_size * self.rmt_config['max_n_segments']]\n",
    "\n",
    "            # n_seg = math.ceil(len(seq) / self.segment_size)\n",
    "            # input_segments = torch.chunk(seq, n_seg)\n",
    "            align = self.rmt_config.get('segment_alignment')\n",
    "            if align in {'right', None}:\n",
    "                split_inds = (list(range(len(seq), 0, -self.segment_size)) + [0])[::-1]\n",
    "            elif align == 'left':\n",
    "                split_inds = list(range(0, len(seq), self.segment_size)) + [len(seq)]\n",
    "            elif align == 'center':\n",
    "                n_seg = math.ceil(len(seq) / self.segment_size)\n",
    "                split_inds = list(range(0, len(seq), math.ceil(len(seq) / n_seg))) + [len(seq)]\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "            input_segments = [seq[start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "            input_segments = [self.pad_add_special_tokens(t, self.rmt_config['input_size']) for t in input_segments]\n",
    "            segmented_batch.append(input_segments)\n",
    "\n",
    "            if labels is not None:\n",
    "                labels_segments = [labels[start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "                labels_segments = [self.pad_add_special_tokens(t, self.rmt_config['input_size'], add_to='labels') for t in labels_segments]\n",
    "                segmented_batch_labels.append(labels_segments)\n",
    "\n",
    "            if labels_mask is not None:\n",
    "                labels_mask_segments = [labels_mask[start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "                labels_mask_segments = [self.pad_add_special_tokens(t, self.rmt_config['input_size'], add_to='labels_mask') for t in labels_mask_segments]\n",
    "                segmented_batch_labels_mask.append(labels_mask_segments)\n",
    "\n",
    "        # # batch of segments -> segmented batch\n",
    "        # # + align segments to right border\n",
    "        # # so that the last segment is always non-empty\n",
    "        # segmented_batch = [[s[::-1][i] if len(s) > i else None for s in segmented_batch]\n",
    "        #                    for i in range(self.rmt_config['max_n_segments'])][::-1]\n",
    "\n",
    "        # if len(segmented_batch_labels) > 0:\n",
    "        #     segmented_batch_labels = [[s[::-1][i] if len(s) > i else None for s in segmented_batch_labels]\n",
    "        #                               for i in range(self.rmt_config['max_n_segments'])][::-1]\n",
    "\n",
    "        # if len(segmented_batch_labels_mask) > 0:\n",
    "        #     segmented_batch_labels_mask = [[s[::-1][i] if len(s) > i else None for s in segmented_batch_labels_mask]\n",
    "        #                                    for i in range(self.rmt_config['max_n_segments'])][::-1]\n",
    "\n",
    "        return segmented_batch, segmented_batch_labels, segmented_batch_labels_mask\n",
    "\n",
    "    def pad_add_special_tokens(self, tensor, segment_size, add_to='inputs'):\n",
    "        input_elements = []\n",
    "        if add_to == 'inputs':\n",
    "            input_elements += [self.cls_token, self.mem_token_ids, self.sep_token, tensor, self.sep_token]\n",
    "        elif add_to == 'labels':\n",
    "            masked_labels = torch.zeros((1, tensor.shape[-1]), device=tensor.device)\n",
    "            input_elements += [masked_labels, masked_labels.repeat(self.num_mem_tokens, 1), masked_labels, tensor, masked_labels]\n",
    "        elif add_to == 'labels_mask':\n",
    "            mask_value = torch.zeros((1), device=tensor.device)\n",
    "            input_elements += [mask_value, mask_value.repeat(self.num_mem_tokens), mask_value, tensor, mask_value]\n",
    "\n",
    "        tensor = torch.cat(input_elements)\n",
    "\n",
    "        pad_size = segment_size - tensor.shape[0]\n",
    "        if pad_size > 0:\n",
    "            if add_to == 'inputs':\n",
    "                tensor = F.pad(tensor, (0, pad_size), value=self.pad_token_id)\n",
    "            elif add_to == 'labels':\n",
    "                # todo: labels pad value should be specified, if not multilable classification it could be just -100\n",
    "                tensor = F.pad(tensor, (0, 0, 0, pad_size), value=0)\n",
    "            elif add_to == 'labels_mask':\n",
    "                tensor = F.pad(tensor, (0, pad_size), value=0)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_segments = 2\n",
    "num_mem_tokens = 10\n",
    "# device = torch.device(3)\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForTokenClassification: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'deberta.embeddings.position_embeddings.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# from rmt_utils.encoder.memory_layers import memory_layers_forward as memory_layers_func\n",
    "# from rmt_utils.encoder.memory_layers import deberta_memory_layers_forward as memory_layers_func\n",
    "# memory_forward_func = deberta_memory_layers_forward\n",
    "model_name = \"google/bert_uncased_L-4_H-256_A-4\"\n",
    "# model_name = \"microsoft/deberta-v3-base\"\n",
    "# model_name = 'google/electra-base-discriminator'\n",
    "# model_name = \"google/bert_uncased_L-4_H-256_A-4\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "rmt_config = {'num_mem_tokens': 5, \n",
    "                'max_n_segments': 3,\n",
    "               #  'segment_alignment': 'right',\n",
    "                'tokenizer': tokenizer,\n",
    "               #  'memory_layers': 'all', \n",
    "               #  'memory_forward_func': memory_forward_func,\n",
    "               #  'share_memory_layers': True,\n",
    "                'reconstruction_loss_coef': 0.1,\n",
    "                'segment_ordering': 'regular',\n",
    "                'input_size': 512, \n",
    "                'bptt_depth': -1, \n",
    "                'sum_loss': False,\n",
    "             }\n",
    "\n",
    "base_model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=3)\n",
    "# rmt = RMTEncoderForSequenceClassification(base_model, **rmt_config)\n",
    "# rmt = RMTEncoderMemoryLayers(base_model, **rmt_config)\n",
    "# rmt = RMTEncoderMLMMemLoss(base_model, **rmt_config)\n",
    "# rmt = RMTEncoderHorizontalMemory(base_model, **rmt_config)\n",
    "rmt = RMTEncoderForTokenClassification(base_model, **rmt_config)\n",
    "# rmt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rmt_utils.encoder_decoder.horizontal_memory import horizontal_memory_forward as memory_forward_func\n",
    "# # model_name = \"facebook/bart-base\"\n",
    "# model_name = 't5-small'\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# rmt_config = {'num_mem_tokens': 5, \n",
    "#                 'max_n_segments': 3,\n",
    "#                #  'segment_alignment': 'right',\n",
    "#                 'tokenizer': tokenizer,\n",
    "#                 'memory_layers': 'all', \n",
    "#                #  'memory_forward_func': memory_layers_func,\n",
    "#                 'share_memory_layers': True,\n",
    "#                 'reconstruction_loss_coef': 0.1,\n",
    "#                 'segment_ordering': 'regular',\n",
    "#                 'input_size': 512, \n",
    "#                 'bptt_depth': -1, \n",
    "#                 'sum_loss': False,\n",
    "#              }\n",
    "\n",
    "# base_model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# # rmt = RMTEncoderDecoderForConditionalGeneration(base_model, **rmt_config)\n",
    "# # rmt = RMTEncoderDecoderMemoryLoss(base_model, **rmt_config) # does not work\n",
    "# # rmt = RMTEncoderDecoderMemoryLayers(base_model, **rmt_config)\n",
    "# rmt = RMTEncoderDecoderHorizontalMemory(base_model, **rmt_config)\n",
    "# # rmt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m out \u001b[39m=\u001b[39m rmt(sample_input_ids, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs, output_hidden_states\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, output_attentions\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m out\u001b[39m.\u001b[39mkeys()\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb Cell 13\u001b[0m in \u001b[0;36mRMTEncoderMLMMemLoss.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=146'>147</a>\u001b[0m memory[non_empty_mask] \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mhidden_states[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][:, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory_position]\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=148'>149</a>\u001b[0m input_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([s \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m segment_input_ids \u001b[39mif\u001b[39;00m s \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m])\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=149'>150</a>\u001b[0m segment_reconstruction_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msegment_reconstruction_forward(memory[non_empty_mask], input_ids)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=150'>151</a>\u001b[0m out[\u001b[39m'\u001b[39m\u001b[39mreconstruction_loss\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m segment_reconstruction_loss\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=151'>152</a>\u001b[0m base_model_outputs\u001b[39m.\u001b[39mappend(out)\n",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb Cell 13\u001b[0m in \u001b[0;36mRMTEncoderMLMMemLoss.segment_reconstruction_forward\u001b[0;34m(self, memory_outputs, previous_input_ids)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=112'>113</a>\u001b[0m attention_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones(input_embeddings\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39minput_embeddings\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=113'>114</a>\u001b[0m attention_mask[mask_inds] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=115'>116</a>\u001b[0m rec_attn_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrec_attn(input_embeddings, attention_mask\u001b[39m=\u001b[39;49mattention_mask)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=116'>117</a>\u001b[0m rec_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrec_cls(rec_attn_out[\u001b[39m0\u001b[39m])\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=118'>119</a>\u001b[0m loss_fct \u001b[39m=\u001b[39m CrossEntropyLoss(ignore_index\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m100\u001b[39m)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:341\u001b[0m, in \u001b[0;36mDebertaV2Layer.forward\u001b[0;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    333\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    334\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    339\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    340\u001b[0m ):\n\u001b[0;32m--> 341\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    342\u001b[0m         hidden_states,\n\u001b[1;32m    343\u001b[0m         attention_mask,\n\u001b[1;32m    344\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    345\u001b[0m         query_states\u001b[39m=\u001b[39;49mquery_states,\n\u001b[1;32m    346\u001b[0m         relative_pos\u001b[39m=\u001b[39;49mrelative_pos,\n\u001b[1;32m    347\u001b[0m         rel_embeddings\u001b[39m=\u001b[39;49mrel_embeddings,\n\u001b[1;32m    348\u001b[0m     )\n\u001b[1;32m    349\u001b[0m     \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[1;32m    350\u001b[0m         attention_output, att_matrix \u001b[39m=\u001b[39m attention_output\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:272\u001b[0m, in \u001b[0;36mDebertaV2Attention.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    264\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    265\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    270\u001b[0m     rel_embeddings\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    271\u001b[0m ):\n\u001b[0;32m--> 272\u001b[0m     self_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    273\u001b[0m         hidden_states,\n\u001b[1;32m    274\u001b[0m         attention_mask,\n\u001b[1;32m    275\u001b[0m         output_attentions,\n\u001b[1;32m    276\u001b[0m         query_states\u001b[39m=\u001b[39;49mquery_states,\n\u001b[1;32m    277\u001b[0m         relative_pos\u001b[39m=\u001b[39;49mrelative_pos,\n\u001b[1;32m    278\u001b[0m         rel_embeddings\u001b[39m=\u001b[39;49mrel_embeddings,\n\u001b[1;32m    279\u001b[0m     )\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[1;32m    281\u001b[0m         self_output, att_matrix \u001b[39m=\u001b[39m self_output\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:699\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelative_attention:\n\u001b[1;32m    698\u001b[0m     rel_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_dropout(rel_embeddings)\n\u001b[0;32m--> 699\u001b[0m     rel_att \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdisentangled_attention_bias(\n\u001b[1;32m    700\u001b[0m         query_layer, key_layer, relative_pos, rel_embeddings, scale_factor\n\u001b[1;32m    701\u001b[0m     )\n\u001b[1;32m    703\u001b[0m \u001b[39mif\u001b[39;00m rel_att \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     attention_scores \u001b[39m=\u001b[39m attention_scores \u001b[39m+\u001b[39m rel_att\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:745\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.disentangled_attention_bias\u001b[0;34m(self, query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)\u001b[0m\n\u001b[1;32m    742\u001b[0m att_span \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_ebd_size\n\u001b[1;32m    743\u001b[0m relative_pos \u001b[39m=\u001b[39m relative_pos\u001b[39m.\u001b[39mlong()\u001b[39m.\u001b[39mto(query_layer\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 745\u001b[0m rel_embeddings \u001b[39m=\u001b[39m rel_embeddings[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpos_ebd_size \u001b[39m-\u001b[39;49m att_span : \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpos_ebd_size \u001b[39m+\u001b[39;49m att_span, :]\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m    746\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshare_att_key:\n\u001b[1;32m    747\u001b[0m     pos_query_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\n\u001b[1;32m    748\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquery_proj(rel_embeddings), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_attention_heads\n\u001b[1;32m    749\u001b[0m     )\u001b[39m.\u001b[39mrepeat(query_layer\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_attention_heads, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "out = rmt(sample_input_ids, **kwargs, output_hidden_states=False, output_attentions=False)\n",
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmt.memory_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 32099, 32131, 32131, 32131, 32131, 32131, 32131, 32131, 32131,\n",
       "         32131, 32131, 32131, 32131, 32131, 32131, 32131, 32131, 32131, 32131],\n",
       "        [    0, 32099, 32128, 32131, 32128, 32131, 32128, 32131, 32128, 32131,\n",
       "         32128, 32131, 32128, 32131, 32128, 32131, 32128, 32131, 32128, 32131]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmt.generate(sample_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmt.memory_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:1320: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  label_index = (labels >= 0).nonzero()\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m out \u001b[39m=\u001b[39m rmt(sample_input_ids, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs, output_hidden_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, output_attentions\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m out\u001b[39m.\u001b[39mkeys()\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb Cell 17\u001b[0m in \u001b[0;36mRMTEncoderMLMMemLoss.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=146'>147</a>\u001b[0m memory[non_empty_mask] \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mhidden_states[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][:, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory_position]\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=148'>149</a>\u001b[0m input_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([s \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m segment_input_ids \u001b[39mif\u001b[39;00m s \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m])\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=149'>150</a>\u001b[0m segment_reconstruction_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msegment_reconstruction_forward(memory[non_empty_mask], input_ids)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=150'>151</a>\u001b[0m out[\u001b[39m'\u001b[39m\u001b[39mreconstruction_loss\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m segment_reconstruction_loss\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=151'>152</a>\u001b[0m base_model_outputs\u001b[39m.\u001b[39mappend(out)\n",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb Cell 17\u001b[0m in \u001b[0;36mRMTEncoderMLMMemLoss.segment_reconstruction_forward\u001b[0;34m(self, memory_outputs, previous_input_ids)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=112'>113</a>\u001b[0m attention_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones(input_embeddings\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39minput_embeddings\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=113'>114</a>\u001b[0m attention_mask[mask_inds] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=115'>116</a>\u001b[0m rec_attn_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrec_attn(input_embeddings, attention_mask\u001b[39m=\u001b[39;49mattention_mask)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=116'>117</a>\u001b[0m rec_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrec_cls(rec_attn_out[\u001b[39m0\u001b[39m])\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=118'>119</a>\u001b[0m loss_fct \u001b[39m=\u001b[39m CrossEntropyLoss(ignore_index\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m100\u001b[39m)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:341\u001b[0m, in \u001b[0;36mDebertaV2Layer.forward\u001b[0;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    333\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    334\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    339\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    340\u001b[0m ):\n\u001b[0;32m--> 341\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    342\u001b[0m         hidden_states,\n\u001b[1;32m    343\u001b[0m         attention_mask,\n\u001b[1;32m    344\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    345\u001b[0m         query_states\u001b[39m=\u001b[39;49mquery_states,\n\u001b[1;32m    346\u001b[0m         relative_pos\u001b[39m=\u001b[39;49mrelative_pos,\n\u001b[1;32m    347\u001b[0m         rel_embeddings\u001b[39m=\u001b[39;49mrel_embeddings,\n\u001b[1;32m    348\u001b[0m     )\n\u001b[1;32m    349\u001b[0m     \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[1;32m    350\u001b[0m         attention_output, att_matrix \u001b[39m=\u001b[39m attention_output\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:272\u001b[0m, in \u001b[0;36mDebertaV2Attention.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    264\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    265\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    270\u001b[0m     rel_embeddings\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    271\u001b[0m ):\n\u001b[0;32m--> 272\u001b[0m     self_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    273\u001b[0m         hidden_states,\n\u001b[1;32m    274\u001b[0m         attention_mask,\n\u001b[1;32m    275\u001b[0m         output_attentions,\n\u001b[1;32m    276\u001b[0m         query_states\u001b[39m=\u001b[39;49mquery_states,\n\u001b[1;32m    277\u001b[0m         relative_pos\u001b[39m=\u001b[39;49mrelative_pos,\n\u001b[1;32m    278\u001b[0m         rel_embeddings\u001b[39m=\u001b[39;49mrel_embeddings,\n\u001b[1;32m    279\u001b[0m     )\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[1;32m    281\u001b[0m         self_output, att_matrix \u001b[39m=\u001b[39m self_output\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:699\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelative_attention:\n\u001b[1;32m    698\u001b[0m     rel_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_dropout(rel_embeddings)\n\u001b[0;32m--> 699\u001b[0m     rel_att \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdisentangled_attention_bias(\n\u001b[1;32m    700\u001b[0m         query_layer, key_layer, relative_pos, rel_embeddings, scale_factor\n\u001b[1;32m    701\u001b[0m     )\n\u001b[1;32m    703\u001b[0m \u001b[39mif\u001b[39;00m rel_att \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     attention_scores \u001b[39m=\u001b[39m attention_scores \u001b[39m+\u001b[39m rel_att\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:745\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.disentangled_attention_bias\u001b[0;34m(self, query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)\u001b[0m\n\u001b[1;32m    742\u001b[0m att_span \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_ebd_size\n\u001b[1;32m    743\u001b[0m relative_pos \u001b[39m=\u001b[39m relative_pos\u001b[39m.\u001b[39mlong()\u001b[39m.\u001b[39mto(query_layer\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 745\u001b[0m rel_embeddings \u001b[39m=\u001b[39m rel_embeddings[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpos_ebd_size \u001b[39m-\u001b[39;49m att_span : \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpos_ebd_size \u001b[39m+\u001b[39;49m att_span, :]\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m    746\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshare_att_key:\n\u001b[1;32m    747\u001b[0m     pos_query_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\n\u001b[1;32m    748\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquery_proj(rel_embeddings), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_attention_heads\n\u001b[1;32m    749\u001b[0m     )\u001b[39m.\u001b[39mrepeat(query_layer\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_attention_heads, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "out = rmt(sample_input_ids, **kwargs, output_hidden_states=True, output_attentions=True)\n",
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1291, -0.0159, -0.0419,  ..., -0.0337, -0.2189, -0.0095],\n",
       "        [-0.0350, -0.0167,  0.0482,  ..., -0.1350, -0.2535, -0.0246],\n",
       "        [-0.0531,  0.0108, -0.0384,  ..., -0.1367, -0.1615, -0.0312],\n",
       "        ...,\n",
       "        [-0.1999,  0.0540, -0.0645,  ..., -0.1501, -0.1413, -0.0326],\n",
       "        [-0.2171,  0.0586, -0.0722,  ..., -0.1467, -0.1235, -0.0330],\n",
       "        [-0.2195,  0.0811, -0.0253,  ..., -0.0988, -0.1056, -0.0275]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmt.model.base_model.encoder.get_rel_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory_input = self.pad_add_special_tokens(self.mem_token_ids, self.input_size)[:self.num_mem_tokens + ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mem_out = self.model.base_model(memory_input.reshape((1, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_input_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m sample_input_ids\u001b[39m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_input_ids' is not defined"
     ]
    }
   ],
   "source": [
    "sample_input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['loss', 'logits', 'hidden_states', 'loss_0', 'loss_1', 'loss_2'])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = rmt(sample_input_ids, **kwargs, output_hidden_states=False, output_attentions=False)\n",
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segmented = rmt.pad_and_segment(sample_input_ids[:, :600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n, p in rmt.named_parameters():\n",
    "#     if 'memory' in n:\n",
    "#         print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(rmt.memory_decoders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n, p in rmt.named_parameters():\n",
    "#     print(n, p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Holder:\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq_len = 1536\n",
    "target_seq_len = 1024\n",
    "batch_size = 2\n",
    "\n",
    "args = Holder\n",
    "args.target_seq_len = target_seq_len\n",
    "args.input_seq_len = input_seq_len\n",
    "args.input_prefix = ''\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_attention_first_token = False  # should be True for LED\n",
    "encode_plus_kwargs = {'truncation': True, 'padding': 'longest', 'pad_to_multiple_of': 1}\n",
    "# generate_kwargs = {'max_length': args.target_seq_len, 'min_length': args.target_seq_len}\n",
    "generate_kwargs = {}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # cut too long strings because they may slow down tokenization\n",
    "    inputs = [b['input'][:args.input_seq_len * 10] for b in batch]\n",
    "    if 'outputs' in batch[0]:\n",
    "        # if we have more than 1 label per example (only in valid) take only one of them\n",
    "        # to compute loss on valid\n",
    "        labels = [b['outputs'][0][:args.target_seq_len * 10] for b in batch]\n",
    "    else:\n",
    "        labels = [b['output'][:args.target_seq_len * 10] for b in batch]\n",
    "    if args.input_prefix:\n",
    "        inputs = [args.input_prefix + inp for inp in inputs]\n",
    "    features = tokenizer.batch_encode_plus(list(inputs), max_length=args.input_seq_len, return_tensors='pt',\n",
    "                                           **encode_plus_kwargs)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer.batch_encode_plus(list(labels), max_length=args.target_seq_len, return_tensors='pt',\n",
    "                                             **encode_plus_kwargs).input_ids\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    features['labels'] = labels\n",
    "    features['id'] = [b['id'] for b in batch]\n",
    "    if 'outputs' in batch[0]:\n",
    "        features['target_text'] = [b['outputs'] for b in batch]\n",
    "    else:\n",
    "        features['target_text'] = [b['output'] for b in batch]\n",
    "    if 'global_attention_mask' in features:\n",
    "        raise RuntimeError('What global attention mask for Longformer and LongformerEncoder-Decoder should be?')\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset scrolls (/home/bulatov/.cache/huggingface/datasets/tau___scrolls/qasper/1.0.0/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f06aacacaa58439dbfab0d8c9dfd9c22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_name = 'qasper'\n",
    "dataset = datasets.load_dataset('tau/scrolls', task_name)\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "train_sampler = RandomSampler(train_dataset,)\n",
    "kwargs = {'pin_memory': True, 'num_workers': 0}\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler,\n",
    "                                collate_fn=collate_fn, **kwargs)\n",
    "\n",
    "valid_dataset = dataset['validation']\n",
    "valid_sampler = RandomSampler(valid_dataset)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, sampler=train_sampler,\n",
    "                                collate_fn=collate_fn, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = iter(train_dataloader)\n",
    "sample = next(gen)\n",
    "\n",
    "if 'id' in sample:\n",
    "    id = sample.pop('id')\n",
    "if 'target_text' in sample:\n",
    "    tgt_text = sample.pop('target_text')\n",
    "\n",
    "rmt.to(device)\n",
    "for k in sample:\n",
    "    sample[k] = sample[k].to(device)\n",
    "    \n",
    "sample_input_ids = sample.pop('input_ids').to(device)\n",
    "kwargs = sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq_len = 1536\n",
    "target_seq_len = 3\n",
    "batch_size = 2\n",
    "\n",
    "args = Holder\n",
    "args.target_seq_len = target_seq_len\n",
    "args.input_seq_len = input_seq_len\n",
    "args.input_prefix = ''\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_plus_kwargs = {'max_length': args.input_seq_len,\n",
    "                        'truncation': True,\n",
    "                        'padding': 'longest',\n",
    "                        'pad_to_multiple_of': 1}\n",
    "generate_kwargs = {}\n",
    "labels_map = {'Contradiction': 0, 'Entailment': 1, 'Not mentioned': 2}\n",
    "num_labels = len(labels_map)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # cut too long strings because they may slow down tokenization\n",
    "    inputs = [b['input'][:args.input_seq_len * 10] for b in batch]\n",
    "    labels = [b['output'][:args.target_seq_len * 10] for b in batch]\n",
    "    if args.input_prefix:\n",
    "        inputs = [args.input_prefix + inp for inp in inputs]\n",
    "    features = tokenizer.batch_encode_plus(list(inputs), return_tensors='pt', **encode_plus_kwargs)\n",
    "    labels = np.array([labels_map[t] for t in labels])\n",
    "    features['labels'] = torch.from_numpy(labels)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset scrolls (/home/bulatov/.cache/huggingface/datasets/tau___scrolls/contract_nli/1.0.0/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "756239b0889d473ca9d2eb8cde3cd904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_name = 'contract_nli'\n",
    "dataset = datasets.load_dataset('tau/scrolls', task_name)\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "train_sampler = RandomSampler(train_dataset,)\n",
    "kwargs = {'pin_memory': True, 'num_workers': 0}\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler,\n",
    "                                collate_fn=collate_fn, **kwargs)\n",
    "\n",
    "valid_dataset = dataset['validation']\n",
    "valid_sampler = RandomSampler(valid_dataset)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, sampler=train_sampler,\n",
    "                                collate_fn=collate_fn, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = iter(train_dataloader)\n",
    "sample = next(gen)\n",
    "\n",
    "if 'id' in sample:\n",
    "    id = sample.pop('id')\n",
    "if 'target_text' in sample:\n",
    "    tgt_text = sample.pop('target_text')\n",
    "\n",
    "rmt.to(device)\n",
    "for k in sample:\n",
    "    sample[k] = sample[k].to(device)\n",
    "    \n",
    "sample_input_ids = sample.pop('input_ids').to(device)\n",
    "kwargs = sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleAttributeError",
     "evalue": "'BertForSequenceClassification' object has no attribute 'encoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb Cell 37\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m out \u001b[39m=\u001b[39m rmt(sample_input_ids, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs, output_hidden_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, output_attentions \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m out\u001b[39m.\u001b[39mkeys()\n",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb Cell 37\u001b[0m in \u001b[0;36mRMTEncoderHorizontalMemory.__call__\u001b[0;34m(self, input_ids, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     memory \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mset_memory()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     memory \u001b[39m=\u001b[39m memory\u001b[39m.\u001b[39mrepeat(input_ids\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m     segmented \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad_and_segment(input_ids)\n",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb Cell 37\u001b[0m in \u001b[0;36mRMTEncoderHorizontalMemory.set_memory\u001b[0;34m(self, memory)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# fill layer memories \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m memory_input \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad_add_special_tokens(mem_token_ids, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_mem_tokens)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m mem_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mencoder(memory_input\u001b[39m.\u001b[39mreshape((\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_framework.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mreturn\u001b[39;00m memory\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/nn/modules/module.py:778\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m    777\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m--> 778\u001b[0m \u001b[39mraise\u001b[39;00m ModuleAttributeError(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    779\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mModuleAttributeError\u001b[0m: 'BertForSequenceClassification' object has no attribute 'encoder'"
     ]
    }
   ],
   "source": [
    "out = rmt(sample_input_ids, **kwargs, output_hidden_states=True, output_attentions = True)\n",
    "out.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### replace forward signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect \n",
    "from functools import wraps\n",
    "\n",
    "model_name = \"google/bert_uncased_L-4_H-256_A-4\"\n",
    "\n",
    "def decorate(func, source):\n",
    "    @wraps(source)\n",
    "    def decorated(*args, **kwargs):\n",
    "        return func(*args, **kwargs)\n",
    "    return decorated\n",
    "\n",
    "class RMT(torch.nn.Module):\n",
    "    def __init__(self, base_model, **rmt_kwargs):\n",
    "        super().__init__()\n",
    "        self.model = base_model\n",
    "        self.forward = decorate(self.forward, self.model.forward)\n",
    "\n",
    "    def forward(self, new_rmt_arg, input_ids, **kwargs):\n",
    "        pass\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "rmt = RMT(base_model, **rmt_config)\n",
    "inspect.signature(rmt.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bert_uncased_L-4_H-256_A-4 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-4_H-256_A-4 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Signature (input_ids, **kwargs)>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import wraps\n",
    "\n",
    "def wrap_func(method):\n",
    "    @wraps(method)\n",
    "    def _impl(self, *method_args, **method_kwargs):\n",
    "        method_output = method(self, *method_args, **method_kwargs)\n",
    "        return method_output\n",
    "    return _impl\n",
    "    # return \n",
    "\n",
    "class RMTEncoderForSequenceClassification(RMTBaseModel):\n",
    "    def __init__(self, base_model, **rmt_kwargs):\n",
    "        super().__init__(base_model, **rmt_kwargs)\n",
    "\n",
    "    @wrap_func\n",
    "    def forward(self, input_ids, **kwargs):\n",
    "        memory = self.set_memory()\n",
    "        memory = memory.repeat(input_ids.shape[0], 1, 1)\n",
    "        segmented = self.pad_and_segment(input_ids)\n",
    "\n",
    "        losses = []\n",
    "        for seg_num, segment_input_ids in enumerate(segmented):                \n",
    "            if (self.rmt_config['bptt_depth'] > -1) and (len(segmented) - seg_num > self.rmt_config['bptt_depth']): \n",
    "                memory = memory.detach()\n",
    "\n",
    "            seg_kwargs = dict(**kwargs)\n",
    "            seg_kwargs['output_hidden_states'] = True\n",
    "\n",
    "            non_empty_mask = [s is not None for s in segment_input_ids]\n",
    "            if sum(non_empty_mask) == 0:\n",
    "                continue\n",
    "            input_ids = torch.stack([s for s in segment_input_ids if s is not None])\n",
    "            attention_mask = self.get_attention_mask(input_ids)\n",
    "            token_type_ids = self.get_token_type_ids(input_ids)\n",
    "            seg_kwargs['labels'] = seg_kwargs['labels'][non_empty_mask]\n",
    "\n",
    "            inputs_embeds = self.model.embeddings(input_ids)\n",
    "            inputs_embeds[:, self.memory_position] = memory[non_empty_mask]\n",
    "\n",
    "            seg_kwargs['input_ids'] = None\n",
    "            seg_kwargs['inputs_embeds'] = inputs_embeds\n",
    "            seg_kwargs['attention_mask'] = attention_mask\n",
    "            seg_kwargs['token_type_ids'] = token_type_ids\n",
    "\n",
    "            out = self.model(**seg_kwargs)\n",
    "            memory[non_empty_mask] = out.hidden_states[-1][:, self.memory_position]\n",
    "\n",
    "            losses.append(out['loss'])\n",
    "\n",
    "        # drop unnecessary hiddens to save memory\n",
    "        if not kwargs.get('output_hidden_states'):\n",
    "            for key in out.keys():\n",
    "                if 'hidden_state' in key:\n",
    "                    out[key] = None\n",
    "\n",
    "        for i, l in enumerate(losses):\n",
    "            out[f'loss_{i}'] = l.mean()\n",
    "\n",
    "        if self.rmt_config['sum_loss']:\n",
    "            out['loss'] = torch.stack(losses).sum(dim=0)\n",
    "\n",
    "        return out\n",
    "        \n",
    "    def pad_add_special_tokens(self, tensor, segment_size):\n",
    "        input_elements = []\n",
    "        input_elements += [self.cls_token, self.mem_token_ids, self.sep_token, tensor, self.sep_token]\n",
    "        tensor = torch.cat(input_elements)\n",
    "\n",
    "        pad_size = segment_size - tensor.shape[0]\n",
    "        if pad_size > 0:\n",
    "            tensor = F.pad(tensor, (0, pad_size))\n",
    "        return tensor\n",
    "    \n",
    "    def get_token_type_ids(self, tensor):\n",
    "        return torch.zeros_like(tensor)\n",
    "\n",
    "\n",
    "num_segments = 2\n",
    "num_mem_tokens = 10\n",
    "# device = torch.device(3)\n",
    "device = 'cpu'\n",
    "\n",
    "\n",
    "from rmt_utils.encoder.memory_layers import memory_layers_forward as memory_layers_func\n",
    "# from rmt_utils.encoder.memory_layers import deberta_memory_layers_forward as memory_layers_func\n",
    "\n",
    "model_name = \"google/bert_uncased_L-4_H-256_A-4\"\n",
    "# model_name = \"microsoft/deberta-v3-base\"\n",
    "# model_name = 'google/electra-base-discriminator'\n",
    "# model_name = \"google/bert_uncased_L-4_H-256_A-4\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "rmt_config = {'num_mem_tokens': 5, \n",
    "                'max_n_segments': 3,\n",
    "               #  'segment_alignment': 'right',\n",
    "                'tokenizer': tokenizer,\n",
    "                'memory_layers': 'all', \n",
    "                'memory_forward_func': memory_layers_func,\n",
    "                'share_memory_layers': True,\n",
    "                'reconstruction_loss_coef': 1,\n",
    "                'segment_ordering': 'regular',\n",
    "                'input_size': 512, \n",
    "                'bptt_depth': -1, \n",
    "                'sum_loss': False,\n",
    "             }\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "rmt = RMTEncoderForSequenceClassification(base_model, **rmt_config)\n",
    "# rmt = RMTEncoderMemoryLayers(base_model, **rmt_config)\n",
    "# rmt = RMTEncoderMLMMemLoss(base_model, **rmt_config)\n",
    "# rmt.to(device)\n",
    "\n",
    "import inspect\n",
    "inspect.signature(rmt.forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLM head for input decoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertLayer(\n",
       "  (attention): BertAttention(\n",
       "    (self): BertSelfAttention(\n",
       "      (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (output): BertSelfOutput(\n",
       "      (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (intermediate): BertIntermediate(\n",
       "    (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "    (intermediate_act_fn): GELUActivation()\n",
       "  )\n",
       "  (output): BertOutput(\n",
       "    (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "    (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.model.bert.encoder.layer[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.rec_attn = copy.deepcopy(self.model.base_model.encoder.layer[-1])\n",
    "self.rec_cls = torch.nn.Linear(self.model.config.hidden_size, self.model.config.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_kwargs = dict(**kwargs)\n",
    "rec_kwargs.pop('labels')\n",
    "# rec_kwargs.pop('token_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_prob = 0.15\n",
    "\n",
    "segmented = self.pad_and_segment(sample_input_ids)\n",
    "previous_input_ids = segmented[0]\n",
    "\n",
    "inputs = torch.stack(previous_input_ids)\n",
    "input_embeddings = self.model.embeddings(inputs)\n",
    "\n",
    "out = self.model(inputs_embeds=input_embeddings, output_hidden_states=True)\n",
    "memory_outputs = out['hidden_states'][-1][:, self.memory_position]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_forward(self, memory_outputs, previous_input_ids):\n",
    "    \n",
    "    inputs = torch.stack(previous_input_ids)\n",
    "    input_embeddings = self.model.embeddings(inputs)\n",
    "    input_embeddings[:, self.memory_position] = memory_outputs\n",
    "\n",
    "    token_inds = list(range(self.num_mem_tokens + 2, input_embeddings.shape[1] - 1))\n",
    "    mask_inds = np.random.choice(token_inds, round(len(token_inds) * mlm_prob))\n",
    "    attention_mask = torch.ones(input_embeddings.shape[1])\n",
    "    attention_mask[mask_inds] = 0\n",
    "\n",
    "    rec_attn_out = self.rec_attn(input_embeddings)\n",
    "    rec_logits = self.rec_cls(rec_attn_out[0])\n",
    "\n",
    "    loss_fct = CrossEntropyLoss(ignore_index=-100)\n",
    "    reconstruction_loss = loss_fct(rec_logits.view(-1, rec_logits.size(-1)), inputs.view(-1))\n",
    "    \n",
    "    return reconstruction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.4209, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruction_forward(self, memory_outputs=memory_outputs, previous_input_ids=segmented[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_inds = list(range(self.num_mem_tokens + 2, input_embeddings.shape[1] - 1))\n",
    "mask_inds = np.random.choice(token_inds, round(len(token_inds) * mlm_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = torch.ones(input_embeddings.shape[1])\n",
    "attention_mask[mask_inds] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512, 30527])"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_attn_out = self.rec_attn(input_embeddings)\n",
    "rec_logits = self.rec_cls(rec_attn_out[0])\n",
    "rec_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.4209, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fct = CrossEntropyLoss(ignore_index=-100)\n",
    "reconstruction_loss = loss_fct(rec_logits.view(-1, rec_logits.size(-1)), inputs.view(-1))\n",
    "reconstruction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512, 256])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_inds = list(range(self.num_mem_tokens + 2, input_embeddings.shape[1] - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(token_inds)\n",
    "mask_inds = token_inds[: round(len(token_inds) * mlm_prob) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_mask_inds = torch.randa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0159,  0.0027,  0.0078,  ...,  0.0175, -0.0240,  0.0109],\n",
       "         [ 0.0315, -0.0055,  0.0017,  ...,  0.0132, -0.0214, -0.0158],\n",
       "         [ 0.0016, -0.0042, -0.0412,  ...,  0.0192,  0.0082, -0.0007],\n",
       "         ...,\n",
       "         [-0.0986,  0.0014, -0.0430,  ..., -0.0016, -0.0158, -0.0046],\n",
       "         [-0.0986,  0.0014, -0.0430,  ..., -0.0016, -0.0158, -0.0046],\n",
       "         [-0.0986,  0.0014, -0.0430,  ..., -0.0016, -0.0158, -0.0046]],\n",
       "\n",
       "        [[-0.0159,  0.0027,  0.0078,  ...,  0.0175, -0.0240,  0.0109],\n",
       "         [ 0.0315, -0.0055,  0.0017,  ...,  0.0132, -0.0214, -0.0158],\n",
       "         [ 0.0016, -0.0042, -0.0412,  ...,  0.0192,  0.0082, -0.0007],\n",
       "         ...,\n",
       "         [-0.0226,  0.0497,  0.0308,  ..., -0.0470, -0.0116,  0.0216],\n",
       "         [-0.0381, -0.0252,  0.0037,  ...,  0.0464,  0.0336,  0.0329],\n",
       "         [-0.0637, -0.0239,  0.0430,  ..., -0.0894,  0.0181,  0.0181]]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1536])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_kwargs['token_type_ids'].shapea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 4909, 2283,  ...,    0,    0,    0],\n",
       "        [ 101, 4909, 2283,  ..., 2023, 3820,  102]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.nn import CrossEntropyLoss\n",
    "# # def segment_reconstruction_forward(self, segmented, hidden_states):\n",
    "\n",
    "# hidden_states = rec_kwargs['inputs_embeds']\n",
    "# previous_input_ids = segmented[-2]\n",
    "# non_empty_mask = [s is not None for s in previous_input_ids]\n",
    "# if sum(non_empty_mask) == 0:\n",
    "#     raise ValueError\n",
    "\n",
    "# previous_input_ids = torch.stack(previous_input_ids)[non_empty_mask]\n",
    "# reconstructor_input = hidden_states[non_empty_mask]\n",
    "\n",
    "# rec_attn_out = self.rec_attn(reconstructor_input)\n",
    "# rec_logits = self.rec_cls(rec_attn_out[0])\n",
    "\n",
    "# loss_fct = CrossEntropyLoss(ignore_index=-100)\n",
    "# reconstruction_loss = loss_fct(rec_logits.view(-1, rec_logits.size(-1)), previous_input_ids.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### segment to memory attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = rmt\n",
    "input_ids = sample_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory = self.set_memory()\n",
    "# memory = memory.repeat(input_ids.shape[0], 1, 1)\n",
    "# segmented = self.pad_and_segment(input_ids)\n",
    "\n",
    "# losses = {}\n",
    "# memories = []\n",
    "# inputs = []\n",
    "# non_memory_position = [i for i in range(self.rmt_config['input_size']) if i not in self.memory_position]\n",
    "\n",
    "# for seg_num, segment_input_ids in enumerate(segmented):\n",
    "#     if (self.rmt_config['bptt_depth'] > -1) and (len(segmented) - seg_num > self.rmt_config['bptt_depth']): \n",
    "#         memory = memory.detach()\n",
    "\n",
    "#     seg_kwargs = dict(**kwargs)\n",
    "#     seg_kwargs['output_hidden_states'] = True\n",
    "    \n",
    "#     non_empty_mask = [s is not None for s in segment_input_ids]\n",
    "#     if sum(non_empty_mask) == 0:\n",
    "#         continue\n",
    "#     input_ids = torch.stack(segment_input_ids)[non_empty_mask]\n",
    "#     attention_mask = self.get_attention_mask(input_ids)\n",
    "#     token_type_ids = self.get_token_type_ids(input_ids)\n",
    "#     seg_kwargs['labels'] = seg_kwargs['labels'][non_empty_mask]\n",
    "\n",
    "#     inputs_embeds = self.embeddings(input_ids)\n",
    "#     inputs_embeds[:, self.memory_position] = memory[non_empty_mask]\n",
    "\n",
    "#     seg_kwargs['inputs_embeds'] = inputs_embeds\n",
    "#     seg_kwargs['attention_mask'] = attention_mask\n",
    "        \n",
    "#     out = self.model.forward(**seg_kwargs)\n",
    "\n",
    "#     memory[non_empty_mask] = out.encoder_hidden_states[-1][:, self.memory_position]\n",
    "    \n",
    "#     memories.append(torch.clone(memory[non_empty_mask].detach()))\n",
    "#     inputs.append(out.encoder_hidden_states[-1][:, non_memory_position])\n",
    "\n",
    "#     losses[f'loss_{seg_num}'] = out['loss']\n",
    "\n",
    "# memory_out = out.encoder_last_hidden_state[:, self.memory_position]\n",
    "# reconstruction_loss = self.segment_reconstruction_forward(segmented, memory_out)\n",
    "# out['reconstruction_loss'] = reconstruction_loss\n",
    "\n",
    "# # drop unnecessary hiddens to save memory\n",
    "# # if not kwargs.get('output_hidden_states'):\n",
    "# #     for key in out.keys():\n",
    "# #         if 'hidden_state' in key:\n",
    "# #             out[key] = None\n",
    "            \n",
    "# for k, loss in losses.items():\n",
    "#     out[k] = loss\n",
    "\n",
    "# if self.rmt_config['sum_loss']:\n",
    "#     out['loss'] = torch.stack(losses).sum(dim=0)\n",
    "\n",
    "# rec_coef = self.rmt_config['reconstruction_loss_coef']\n",
    "# out['loss'] = reconstruction_loss * rec_coef + out['loss'] * (1 - rec_coef)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hvdenv",
   "language": "python",
   "name": "hvdenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "d97c21bf6e0a282ef40de70ecfbea211641441d72159ac4287fbae50906df39f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
