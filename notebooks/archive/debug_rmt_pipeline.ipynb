{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import hf_hub_download\n",
    "# scrolls_metric_path = hf_hub_download(repo_id=\"datasets/tau/scrolls\", filename=\"metrics/scrolls.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers import PreTrainedModel, AutoModelForSequenceClassification\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import datasets\n",
    "\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers import BertForSequenceClassification\n",
    "import transformers\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Holder:\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq_len = 512\n",
    "target_seq_len = 512\n",
    "\n",
    "num_mem_tokens = 2\n",
    "input_size = 128\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "args = Holder\n",
    "args.target_seq_len = target_seq_len\n",
    "args.input_seq_len = input_seq_len\n",
    "args.num_mem_tokens = num_mem_tokens\n",
    "args.input_size = input_size\n",
    "args.input_prefix = ''\n",
    "args.block_size = None\n",
    "args.task_name = 'wikitext-2-v1'\n",
    "\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/cryptography/hazmat/backends/openssl/x509.py:14: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = 'gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/home/bulatov/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "804455bc2c9943a79ed7dee2908eb6b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/bulatov/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-389b922bfc5fe729.arrow\n",
      "Loading cached processed dataset at /home/bulatov/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-6067a66e735cfbb1.arrow\n",
      "Loading cached processed dataset at /home/bulatov/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-941845a5470f2db7.arrow\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "raw_datasets = datasets.load_dataset('wikitext', args.task_name)\n",
    "column_names = raw_datasets[\"train\"].column_names\n",
    "text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[text_column_name])\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=column_names,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "block_size = args.input_size \n",
    "if args.num_mem_tokens is not None:\n",
    "    block_size -= 2 * args.num_mem_tokens\n",
    "history_size = args.input_seq_len - block_size\n",
    "\n",
    "def group_texts(examples, block_size, history_size=None):\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "\n",
    "    if history_size is None:\n",
    "        result = {\n",
    "            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "    else:\n",
    "        result = {\n",
    "            k: [t[max({0, i - history_size}) : i + block_size] for i in range(0, total_length, block_size)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_pad_value = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124, 388)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size, history_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/bulatov/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-1504f9373e317eca.arrow\n",
      "Loading cached processed dataset at /home/bulatov/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-c6da793e710ea6d8.arrow\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     input_ids = [torch.tensor(b['input_ids'][::-1]) for b in batch]\n",
    "#     labels = [torch.tensor(b['labels'][::-1]) for b in batch]\n",
    "#     attention_mask = [torch.tensor(b['attention_mask'][::-1]) for b in batch]\n",
    "#     input_ids = pad_sequence(input_ids, padding_value=id_pad_value).T.flip(1)\n",
    "#     labels = pad_sequence(labels, padding_value=-100).T.flip(1)\n",
    "#     attention_mask = pad_sequence(attention_mask, padding_value=0).T.flip(1)\n",
    "\n",
    "#     collated = {'input_ids': input_ids,\n",
    "#                 'labels': labels, \n",
    "#                 'attention_mask': attention_mask}\n",
    "\n",
    "#     if input_ids.shape[1] != block_size:\n",
    "#         labels_mask = torch.ones_like(input_ids, dtype=bool)\n",
    "#         labels_mask[:, :-block_size] = False\n",
    "#         collated['labels_mask'] = labels_mask\n",
    "\n",
    "#     return collated\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [torch.tensor(b['input_ids']) for b in batch]\n",
    "    labels = [torch.tensor(b['labels']) for b in batch]\n",
    "    labels_mask = [torch.ones_like(l, dtype=bool) for l in labels]\n",
    "    attention_mask = [torch.tensor(b['attention_mask']) for b in batch]\n",
    "\n",
    "    input_ids = pad_sequence(input_ids, padding_value=id_pad_value).T\n",
    "    labels = pad_sequence(labels, padding_value=-100).T\n",
    "    labels_mask = pad_sequence(labels_mask, padding_value=False).T\n",
    "    attention_mask = pad_sequence(attention_mask, padding_value=0).T\n",
    "\n",
    "    collated = {'input_ids': input_ids,\n",
    "                'labels': labels, \n",
    "                'labels_mask': labels_mask,\n",
    "                'attention_mask': attention_mask}\n",
    "\n",
    "    # if args.vary_n_segments:\n",
    "    #     n_segments = np.random.randint(1, args.max_n_segments + 1)\n",
    "    #     n_tokens = n_segments * block_size\n",
    "    #     for k in collated:\n",
    "    #         collated[k] = collated[k][:, -n_tokens:]\n",
    "\n",
    "    return collated\n",
    "\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"].map(lambda x: group_texts(x, block_size, history_size), \n",
    "                                        batched=True, desc=f\"Grouping train in chunks of {block_size} and history {history_size}\")\n",
    "valid_dataset = tokenized_datasets[\"validation\"].map(lambda x: group_texts(x, block_size), \n",
    "                                        batched=True, desc=f\"Grouping valid in chunks of {block_size}\")\n",
    "\n",
    "\n",
    "# shuffle train data each epoch (one loop over train_dataset)\n",
    "# train_sampler = DistributedStrain_dataset[i] for i in range(4)ampler(train_dataset, rank=hvd.rank(), num_replicas=hvd.size(), shuffle=True,\n",
    "#                                     drop_last=False, seed=args.seed)\n",
    "# per_worker_batch_size = args.batch_size * args.gradient_accumulation_steps\n",
    "# global_batch_size = per_worker_batch_size * hvd.size()\n",
    "\n",
    "# train_sampler = RandomSampler(train_dataset)\n",
    "kwargs = {'pin_memory': True}#, 'num_workers': args.data_n_workers}\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [train_dataset[i] for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in b[0]:\n",
    "    b[0][k] = b[0][k][:124]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = collate_fn(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = iter(train_dataloader)\n",
    "batch = next(gen)\n",
    "batch = next(gen)\n",
    "batch = next(gen)\n",
    "batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_datasets['train'][1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions\n",
    "\n",
    "class MemoryCell(torch.nn.Module):\n",
    "    def __init__(self, base_model, num_mem_tokens):\n",
    "        super().__init__()\n",
    "        self.model = base_model\n",
    "        self.create_memory(num_mem_tokens)\n",
    "\n",
    "    def create_memory(self, num_mem_tokens):\n",
    "        self.num_mem_tokens = num_mem_tokens\n",
    "        embeddings = self.model.get_input_embeddings()\n",
    "        memory_weights = torch.randn((num_mem_tokens, self.model.config.n_embd)) * embeddings.weight.data.std()\n",
    "        self.register_parameter('memory', torch.nn.Parameter(memory_weights, requires_grad=True))\n",
    "\n",
    "        self.read_memory_position = range(num_mem_tokens)\n",
    "        self.write_memory_position = range(-num_mem_tokens, 0)\n",
    "\n",
    "    def set_memory(self, input_shape):\n",
    "        memory = self.memory.repeat(input_shape[0], 1, 1)\n",
    "        return memory\n",
    "\n",
    "    def forward(self, input_ids, memory_state=None, **kwargs):\n",
    "        if memory_state is None:\n",
    "            memory_state = self.set_memory(input_ids.shape)\n",
    "\n",
    "        seg_kwargs = self.process_input(input_ids, memory_state, **kwargs)\n",
    "        out = self.model(**seg_kwargs)\n",
    "        out, new_memory_state = self.process_output(out, **kwargs)\n",
    "\n",
    "        return out, new_memory_state\n",
    "    \n",
    "    def process_input(self, input_ids, memory_state, **kwargs):\n",
    "        seg_kwargs = dict(**kwargs)\n",
    "\n",
    "        inputs_embeds = kwargs.get('inputs_embeds')\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.model.get_input_embeddings()(input_ids)\n",
    "        inputs_embeds = torch.cat([memory_state, inputs_embeds, memory_state], dim=1)\n",
    "\n",
    "        seg_kwargs['input_ids'] = None\n",
    "        seg_kwargs['inputs_embeds'] = inputs_embeds\n",
    "        if kwargs.get('attention_mask') is not None:\n",
    "            seg_kwargs['attention_mask'] = self.pad_attention_mask(kwargs['attention_mask'], inputs_embeds.shape)\n",
    "        seg_kwargs['output_hidden_states'] = True\n",
    "        return seg_kwargs\n",
    "    \n",
    "    def pad_attention_mask(self, attention_mask, shape):\n",
    "        if self.num_mem_tokens in {0, None}:\n",
    "            return attention_mask\n",
    "        else:\n",
    "            mask = torch.ones(*shape[:2], dtype=torch.int64).to(attention_mask.device)\n",
    "            mask[:, self.num_mem_tokens:-self.num_mem_tokens] = attention_mask\n",
    "            return mask\n",
    "    \n",
    "    def process_output(self, model_outputs, **kwargs):\n",
    "        if self.num_mem_tokens not in {0, None}:\n",
    "            out = CausalLMOutputWithCrossAttentions()\n",
    "            memory_state = model_outputs.hidden_states[-1][:, -self.num_mem_tokens:]\n",
    "            out['logits'] = model_outputs.logits[:, self.num_mem_tokens:-self.num_mem_tokens]\n",
    "            \n",
    "            if kwargs.get('output_hidden_states'):\n",
    "                out['hidden_states'] = [lh[:, self.num_mem_tokens:-self.num_mem_tokens] for lh in model_outputs.hidden_states]\n",
    "            if kwargs.get('output_attentions'):\n",
    "                out['attentions'] = model_outputs['attentions']\n",
    "        else:\n",
    "            memory_state = None\n",
    "            out = model_outputs\n",
    "            \n",
    "        return out, memory_state \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentWrapper(torch.nn.Module):\n",
    "    def __init__(self, memory_cell, **rmt_kwargs):\n",
    "        super().__init__()\n",
    "        self.memory_cell = memory_cell\n",
    "        self.rmt_config = rmt_kwargs\n",
    "\n",
    "    def forward(self, input_ids, labels=None, labels_mask=None, inputs_embeds=None, attention_mask=None, output_attentions=None, output_hidden_states=None):\n",
    "        memory_state = None\n",
    "        segmented = self.segment(input_ids=input_ids, inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
    "\n",
    "        cell_outputs = []\n",
    "        for seg_num, segment in enumerate(segmented):\n",
    "            cell_out, memory_state = self.memory_cell(**segment, memory_state=memory_state, output_hidden_states=True)\n",
    "            cell_outputs.append(cell_out)\n",
    "            self.manage_gradients(memory_state, seg_num)\n",
    "\n",
    "        out = self.process_outputs(cell_outputs, labels=labels, \n",
    "                                   labels_mask=labels_mask,\n",
    "                                   output_attentions=output_attentions, \n",
    "                                   output_hidden_states=output_hidden_states)\n",
    "        return out\n",
    "\n",
    "    def segment(self, **kwargs):\n",
    "        segments = []\n",
    "        for k, tensor in kwargs.items():\n",
    "            if tensor is not None:\n",
    "                k_segments = self.split_tensor(tensor)\n",
    "                for s, k_seg in enumerate(k_segments):\n",
    "                    if s < len(segments):\n",
    "                        segments[s][k] = k_seg\n",
    "                    else:\n",
    "                        segments.append({k: k_seg})\n",
    "\n",
    "        return segments\n",
    "    \n",
    "    def split_tensor(self, tensor):\n",
    "        align = self.rmt_config.get('segment_alignment')\n",
    "        segment_size = self.rmt_config.get('segment_size')\n",
    "        if align in {'left', None}:\n",
    "            split_inds = list(range(0, tensor.shape[1], segment_size)) + [tensor.shape[1]]\n",
    "            segments = [tensor[:, start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "        elif align in {'right', None}:\n",
    "            split_inds = (list(range(tensor.shape[1], 0, -segment_size)) + [0])[::-1]\n",
    "            segments = [tensor[:, start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "        elif align == 'center':\n",
    "            n_seg = math.ceil(tensor.shape[1] / segment_size)\n",
    "            segments = torch.chunk(tensor, n_seg, dim=1)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return segments\n",
    "\n",
    "    def process_outputs(self, cell_outputs, **kwargs):\n",
    "        out = CausalLMOutputWithCrossAttentions()\n",
    "        full_logits = torch.cat([o.logits for o in cell_outputs], dim=1)\n",
    "        full_hidden_states = tuple([torch.cat(layer_hs, dim=1) for layer_hs in zip(*[o.hidden_states for o in cell_outputs])])\n",
    "\n",
    "        labels = kwargs.get('labels')\n",
    "        if labels is not None:\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            shift_logits = full_logits[..., :-1, :].contiguous()\n",
    "            flat_labels = shift_labels.view(-1)\n",
    "            flat_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "            \n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            labels_mask = kwargs.get('labels_mask')\n",
    "            if labels_mask is not None:\n",
    "                shift_mask = labels_mask[..., :-1].contiguous()\n",
    "\n",
    "                flat_labels = flat_labels[shift_mask.view(-1)]\n",
    "                flat_logits = flat_logits[shift_mask.view(-1)]\n",
    "                \n",
    "            out['loss'] = loss_fct(flat_logits, flat_labels)\n",
    "\n",
    "        out['logits'] = full_logits\n",
    "        segment_keys = ['loss', 'logits']\n",
    "        if kwargs.get('output_attentions'):\n",
    "            segment_keys.append('attentions')\n",
    "        if kwargs.get('output_hidden_states'):\n",
    "            segment_keys.append('hidden_states')\n",
    "            out['hidden_states'] = full_hidden_states\n",
    "\n",
    "        # for seg_num, o in enumerate(cell_outputs):\n",
    "        #     for key, value in o.items():\n",
    "        #         if any([sk in key for sk in segment_keys]):\n",
    "        #             out[f'{key}_{seg_num}'] = value\n",
    "\n",
    "        return out \n",
    "        \n",
    "    def manage_gradients(self, memory_state, seg_num):\n",
    "        k2, max_n_segments = self.rmt_config.get('k2'), self.rmt_config.get('max_n_segments')\n",
    "        if seg_num == 0 \\\n",
    "            or k2 in {-1, None} \\\n",
    "            or seg_num + k2 > max_n_segments:\n",
    "                return True\n",
    "        \n",
    "        memory_state = memory_state.detach()\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/cryptography/hazmat/backends/openssl/x509.py:14: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_mem_tokens = 10\n",
    "# device = torch.device(3)\n",
    "device = 'cpu'\n",
    "\n",
    "rmt_config = {'num_mem_tokens': 10, \n",
    "               #  'max_n_segments': 1,\n",
    "               #  'segment_alignment': 'right',\n",
    "               #  'tokenizer': tokenizer,\n",
    "               #  'memory_layers': 'all', \n",
    "               #  'share_memory_layers': True,\n",
    "               #  'reconstruction_loss_coef': 0.1,\n",
    "               #  'k1': -1, 'k2': 3,\n",
    "               #  'segment_ordering': 'regular',\n",
    "               #  'input_size': 1024, \n",
    "               #  'bptt_depth': -1, \n",
    "               #  'sum_loss': False,\n",
    "             }\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "cell = MemoryCell(base_model, num_mem_tokens=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmt = RecurrentWrapper(cell, max_n_segments=5, segment_size=124, segment_alignment='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = iter(train_dataloader)\n",
    "batch = next(gen)\n",
    "batch = next(gen)\n",
    "batch = next(gen)\n",
    "# batch.pop('labels_mask')\n",
    "# batch.pop('labels')\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmt_out = rmt(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'memory_state' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_pipeline.ipynb Cell 27\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_pipeline.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m memory_state\u001b[39m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'memory_state' is not defined"
     ]
    }
   ],
   "source": [
    "memory_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleAttributeError",
     "evalue": "'RecurrentWrapper' object has no attribute 'generate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_pipeline.ipynb Cell 27\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_pipeline.ipynb#Y100sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m gen \u001b[39m=\u001b[39m rmt\u001b[39m.\u001b[39;49mgenerate(batch[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/nn/modules/module.py:778\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m    777\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m--> 778\u001b[0m \u001b[39mraise\u001b[39;00m ModuleAttributeError(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    779\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mModuleAttributeError\u001b[0m: 'RecurrentWrapper' object has no attribute 'generate'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader._SingleProcessDataLoaderIter at 0x7f7bec027eb0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['loss', 'logits'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmt_out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(11.4472, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "for k in rmt_out:\n",
    "    if 'loss' in k:\n",
    "        print(k, rmt_out[k] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented = rmt.segment(input_ids=input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([2, 103]),\n",
       " torch.Size([2, 103]),\n",
       " torch.Size([2, 103]),\n",
       " torch.Size([2, 103]),\n",
       " torch.Size([2, 100])]"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s['input_ids'].shape for s in segmented]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmt_out = rmt(**batch)\n",
    "\n",
    "self = rmt\n",
    "input_ids = batch['input_ids']\n",
    "\n",
    "memory_state = None\n",
    "segmented = self.segment(input_ids=input_ids, inputs_embeds=None, attention_mask=None)\n",
    "\n",
    "cell_outputs = []\n",
    "for seg_num, segment in enumerate(segmented):\n",
    "    cell_out, memory_state = self.memory_cell(**segment, memory_state=memory_state, output_hidden_states=True)#**batch)\n",
    "    cell_outputs.append(cell_out)\n",
    "    self.manage_gradients(memory_state, seg_num)\n",
    "\n",
    "# out = self.process_outputs(cell_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 248])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1.]), tensor([1., 1., 1., 1.]), tensor([1., 1.]))"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.chunk(torch.ones(10), 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50258, 768)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_tokens('[GEN]', special_tokens=True)\n",
    "gen_token = tokenizer.encode('[GEN]')[0]\n",
    "\n",
    "rmt.memory_cell.model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# gen_token = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<|endoftext|>',\n",
       " 'eos_token': '<|endoftext|>',\n",
       " 'unk_token': '<|endoftext|>'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_attention_first_token = False  # should be True for LED\n",
    "encode_plus_kwargs = {'truncation': True, 'padding': 'longest', 'pad_to_multiple_of': 1}\n",
    "# generate_kwargs = {'max_length': args.targettokenizer.add_tokens('[GEN]', special_tokens=True)\n",
    "# gen_token = tokenizer.encode('[GEN]')[0]\n",
    "\n",
    "# rmt.memory_cell.model.resize_token_embeddings(len(tokenizer))_seq_len, 'min_length': args.target_seq_len}\n",
    "generate_kwargs = {}\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs = [b['input'][:args.input_seq_len * 10] for b in batch]\n",
    "    labels = [b['output'][:args.input_seq_len * 10] for b in batch]\n",
    "\n",
    "    collated = {}\n",
    "    inputs = tokenizer.batch_encode_plus(list(inputs), padding=False)\n",
    "    labels = tokenizer.batch_encode_plus(list(labels), padding=False)\n",
    "\n",
    "    full_inputs = [torch.tensor(i[:input_size - len(l) - 1] + [gen_token] + l) for i, l in zip(inputs['input_ids'], labels['input_ids'])]\n",
    "    full_inputs = pad_sequence(full_inputs, padding_value=tokenizer.pad_token_id).T\n",
    "    \n",
    "    labels_mask = torch.zeros_like(full_inputs).bool()\n",
    "    for i, l in enumerate(labels['input_ids']):\n",
    "        labels_mask[i, -len(l) -1:] = True\n",
    "\n",
    "    collated['input_ids'] = collated['labels'] = full_inputs\n",
    "    collated['labels_mask'] = labels_mask\n",
    "    collated['attention_mask'] = collated['input_ids'] != tokenizer.pad_token_id\n",
    "\n",
    "    return collated\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/cryptography/hazmat/backends/openssl/x509.py:14: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f4ee8308dad4b1ab7318719ae547b89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset scrolls (/home/bulatov/.cache/huggingface/datasets/tau___scrolls/quality/1.0.0/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8655e22455641c3950bc202c82b226f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq2seq_task_name = 'quality'\n",
    "dataset = datasets.load_dataset('tau/scrolls', seq2seq_task_name)\n",
    "train_dataset = dataset['train']\n",
    "# shuffle train data each epoch (one loop over train_dataset)\n",
    "# train_sampler = DistributedSampler(train_dataset, shuffle=True, drop_last=False, seed=args.seed)\n",
    "kwargs = {'pin_memory': True}\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2,\n",
    "                                collate_fn=collate_fn, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1291 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "gen = iter(train_dataloader)\n",
    "batch = next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'labels', 'labels_mask', 'attention_mask'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['labels'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Why is Si retirement so significant to the Space Exploration Team? \\n\\n (A) There aren’t enough working people in the world. They won’t be able to find a replacement.\\n (B) As one of two remaining spacemen, it would likely mean the defunding and shut down of the Space Exploration Team.\\n (C) Training new spacemen is costly and time consuming. They won’t have anyone else ready after him.\\n (D) His retirement may inspire others to[GEN]Training new spacemen is costly and time consuming. They won’t have anyone else ready after him.',\n",
       " 'What makes Gubelin an outlier in the present day?\\n\\n (A) He is much older than the rest of the population.\\n (B) He refuses new operations that could improve his health.\\n (C) His mind is still active, and he values hard work.\\n (D) He still wears glasses and value objects like the gold watch given to Si.\\n\\n\\nSPACEMAN ON A SPREE\\n\\n\\n\\n\\n   BY MACK REYNOLDS\\n\\n\\n\\n\\n   Illustrated by Nodel\\n\\n\\n\\n\\n[GEN]He still wears glasses and value objects like the gold watch given to Si.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(batch['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[GEN]Training new spacemen is costly and time consuming. They won’t have anyone else ready after him.',\n",
       " '[GEN]He still wears glasses and value objects like the gold watch given to Si.']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode([c[i] for c, i in zip(batch['input_ids'], batch['labels_mask'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Why is Si retirement so significant to the Space Exploration Team? \\n\\n (A) There aren’t enough working people in the world. They won’t be able to find a replacement.\\n (B) As one of two remaining spacemen, it would likely mean the defunding and shut down of the Space Exploration Team.\\n (C) Training new spacemen is costly and time consuming. They won’t have anyone else ready after him.\\n (D) His retirement may inspire others to[GEN]Training new spacemen is costly and time consuming. They won’t have anyone else ready after him.',\n",
       " 'What makes Gubelin an outlier in the present day?\\n\\n (A) He is much older than the rest of the population.\\n (B) He refuses new operations that could improve his health.\\n (C) His mind is still active, and he values hard work.\\n (D) He still wears glasses and value objects like the gold watch given to Si.\\n\\n\\nSPACEMAN ON A SPREE\\n\\n\\n\\n\\n   BY MACK REYNOLDS\\n\\n\\n\\n\\n   Illustrated by Nodel\\n\\n\\n\\n\\n[GEN]He still wears glasses and value objects like the gold watch given to Si.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode([c[i] for c, i in zip(batch['input_ids'], batch['attention_mask'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = rmt\n",
    "input_ids = batch['input_ids']\n",
    "labels = None\n",
    "labels_mask = batch['labels_mask']\n",
    "attention_mask = batch['attention_mask']\n",
    "output_attentions = output_hidden_states = True\n",
    "inputs_embeds = None\n",
    "\n",
    "memory_state = None\n",
    "segmented = self.segment(input_ids=input_ids, inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
    "\n",
    "cell_outputs = []\n",
    "for seg_num, segment in enumerate(segmented):\n",
    "    cell_out, memory_state = self.memory_cell(**segment, memory_state=memory_state, output_hidden_states=True)\n",
    "    cell_outputs.append(cell_out)\n",
    "    self.manage_gradients(memory_state, seg_num)\n",
    "\n",
    "out = self.process_outputs(cell_outputs, labels=labels, \n",
    "                            labels_mask=labels_mask,\n",
    "                            output_attentions=output_attentions, \n",
    "                            output_hidden_states=output_hidden_states)\n",
    "\n",
    "# gen = rmt.generate(batch['input_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[GEN]Training new spacemen is costly and time consuming. They won’t have anyone else ready after him.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_ids[0][labels_mask[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why is Si retirement so significant to the Space Exploration Team? \\n\\n (A) There aren’t enough working people in the world. They won’t be able to find a replacement.\\n (B) As one of two remaining spacemen, it would likely mean the defunding and shut down of the Space Exploration Team.\\n (C) Training new spacemen is costly and time consuming. They won’t have anyone else ready after him.\\n (D) His retirement may inspire others to'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_ids[0][~labels_mask[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [ids[~mask] for ids, mask in zip(input_ids, labels_mask)]\n",
    "answers = [ids[mask] for ids, mask in zip(input_ids, labels_mask)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[105, 112]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(t) for t in tasks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [torch.cat((ids[~mask], torch.tensor([gen_token]))) for ids, mask in zip(input_ids, labels_mask)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why is Si retirement so significant to the Space Exploration Team? \\n\\n (A) There aren’t enough working people in the world. They won’t be able to find a replacement.\\n (B) As one of two remaining spacemen, it would likely mean the defunding and shut down of the Space Exploration Team.\\n (C) Training new spacemen is costly and time consuming. They won’t have anyone else ready after him.\\n (D) His retirement may inspire others to[GEN]'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tasks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "        self,\n",
    "        inputs: Optional[torch.Tensor] = None,\n",
    "        max_length: Optional[int] = None,\n",
    "        min_length: Optional[int] = None,\n",
    "        do_sample: Optional[bool] = None,\n",
    "        early_stopping: Optional[bool] = None,\n",
    "        num_beams: Optional[int] = None,\n",
    "        temperature: Optional[float] = None,\n",
    "        top_k: Optional[int] = None,\n",
    "        top_p: Optional[float] = None,\n",
    "        typical_p: Optional[float] = None,\n",
    "        repetition_penalty: Optional[float] = None,\n",
    "        bad_words_ids: Optional[Iterable[int]] = None,\n",
    "        bos_token_id: Optional[int] = None,\n",
    "        pad_token_id: Optional[int] = None,\n",
    "        eos_token_id: Optional[int] = None,\n",
    "        length_penalty: Optional[float] = None,\n",
    "        no_repeat_ngram_size: Optional[int] = None,\n",
    "        encoder_no_repeat_ngram_size: Optional[int] = None,\n",
    "        num_return_sequences: Optional[int] = None,\n",
    "        max_time: Optional[float] = None,\n",
    "        max_new_tokens: Optional[int] = None,\n",
    "        decoder_start_token_id: Optional[int] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        num_beam_groups: Optional[int] = None,\n",
    "        diversity_penalty: Optional[float] = None,\n",
    "        prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None,\n",
    "        logits_processor: Optional[LogitsProcessorList] = LogitsProcessorList(),\n",
    "        stopping_criteria: Optional[StoppingCriteriaList] = StoppingCriteriaList(),\n",
    "        constraints: Optional[List[Constraint]] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_scores: Optional[bool] = None,\n",
    "        return_dict_in_generate: Optional[bool] = None,\n",
    "        forced_bos_token_id: Optional[int] = None,\n",
    "        forced_eos_token_id: Optional[int] = None,\n",
    "        remove_invalid_values: Optional[bool] = None,\n",
    "        synced_gpus: Optional[bool] = False,\n",
    "        **model_kwargs,\n",
    "    ) -> Union[GreedySearchOutput, SampleOutput, BeamSearchOutput, BeamSampleOutput, torch.LongTensor]:\n",
    "        r\"\"\"\n",
    "        Generates sequences for models with a language modeling head. The method currently supports greedy decoding,\n",
    "        multinomial sampling, beam-search decoding, and beam-search multinomial sampling.\n",
    "\n",
    "        Apart from `inputs`, all the arguments below will default to the value of the attribute of the same name inside\n",
    "        the [`PretrainedConfig`] of the model. The default values indicated are the default values of those config.\n",
    "\n",
    "        Most of these parameters are explained in more detail in [this blog\n",
    "        post](https://huggingface.co/blog/how-to-generate).\n",
    "\n",
    "        Parameters:\n",
    "            inputs (`torch.Tensor` of shape `(batch_size, sequence_length)`, `(batch_size, sequence_length,\n",
    "            feature_dim)` or `(batch_size, num_channels, height, width)`, *optional*):\n",
    "                The sequence used as a prompt for the generation or as model inputs to the encoder. If `None` the\n",
    "                method initializes it with `bos_token_id` and a batch size of 1. For decoder-only models `inputs`\n",
    "                should of in the format of `input_ids`. For encoder-decoder models *inputs* can represent any of\n",
    "                `input_ids`, `input_values`, `input_features`, or `pixel_values`.\n",
    "            max_length (`int`, *optional*, defaults to `model.config.max_length`):\n",
    "                The maximum length of the sequence to be generated.\n",
    "            max_new_tokens (`int`, *optional*, defaults to None):\n",
    "                The maximum numbers of tokens to generate, ignore the current number of tokens. Use either\n",
    "                `max_new_tokens` or `max_length` but not both, they serve the same purpose.\n",
    "            min_length (`int`, *optional*, defaults to 10):\n",
    "                The minimum length of the sequence to be generated.\n",
    "            do_sample (`bool`, *optional*, defaults to `False`):\n",
    "                Whether or not to use sampling ; use greedy decoding otherwise.\n",
    "            early_stopping (`bool`, *optional*, defaults to `False`):\n",
    "                Whether to stop the beam search when at least `num_beams` sentences are finished per batch or not.\n",
    "            num_beams (`int`, *optional*, defaults to 1):\n",
    "                Number of beams for beam search. 1 means no beam search.\n",
    "            temperature (`float`, *optional*, defaults to 1.0):\n",
    "                The value used to module the next token probabilities.\n",
    "            top_k (`int`, *optional*, defaults to 50):\n",
    "                The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
    "            top_p (`float`, *optional*, defaults to 1.0):\n",
    "                If set to float < 1, only the most probable tokens with probabilities that add up to `top_p` or higher\n",
    "                are kept for generation.\n",
    "            repetition_penalty (`float`, *optional*, defaults to 1.0):\n",
    "                The parameter for repetition penalty. 1.0 means no penalty. See [this\n",
    "                paper](https://arxiv.org/pdf/1909.05858.pdf) for more details.\n",
    "            pad_token_id (`int`, *optional*):\n",
    "                The id of the *padding* token.\n",
    "            bos_token_id (`int`, *optional*):\n",
    "                The id of the *beginning-of-sequence* token.\n",
    "            eos_token_id (`int`, *optional*):\n",
    "                The id of the *end-of-sequence* token.\n",
    "            length_penalty (`float`, *optional*, defaults to 1.0):\n",
    "                Exponential penalty to the length. 1.0 means no penalty. Set to values < 1.0 in order to encourage the\n",
    "                model to generate shorter sequences, to a value > 1.0 in order to encourage the model to produce longer\n",
    "                sequences.\n",
    "            no_repeat_ngram_size (`int`, *optional*, defaults to 0):\n",
    "                If set to int > 0, all ngrams of that size can only occur once.\n",
    "            encoder_no_repeat_ngram_size (`int`, *optional*, defaults to 0):\n",
    "                If set to int > 0, all ngrams of that size that occur in the `encoder_input_ids` cannot occur in the\n",
    "                `decoder_input_ids`.\n",
    "            bad_words_ids(`List[List[int]]`, *optional*):\n",
    "                List of token ids that are not allowed to be generated. In order to get the token ids of the words that\n",
    "                should not appear in the generated text, use `tokenizer(bad_words, add_prefix_space=True,\n",
    "                add_special_tokens=False).input_ids`.\n",
    "            num_return_sequences(`int`, *optional*, defaults to 1):\n",
    "                The number of independently computed returned sequences for each element in the batch.\n",
    "            max_time(`float`, *optional*, defaults to None):\n",
    "                The maximum amount of time you allow the computation to run for in seconds. generation will still\n",
    "                finish the current pass after allocated time has been passed.\n",
    "            attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Mask to avoid performing attention on padding token indices. Mask values are in `[0, 1]`, 1 for tokens\n",
    "                that are not masked, and 0 for masked tokens. If not provided, will default to a tensor the same shape\n",
    "                as `input_ids` that masks the pad token. [What are attention masks?](../glossary#attention-mask)\n",
    "            decoder_start_token_id (`int`, *optional*):\n",
    "                If an encoder-decoder model starts decoding with a different token than *bos*, the id of that token.\n",
    "            use_cache: (`bool`, *optional*, defaults to `True`):\n",
    "                Whether or not the model should use the past last key/values attentions (if applicable to the model) to\n",
    "                speed up decoding.\n",
    "            num_beam_groups (`int`, *optional*, defaults to 1):\n",
    "                Number of groups to divide `num_beams` into in order to ensure diversity among different groups of\n",
    "                beams. [this paper](https://arxiv.org/pdf/1610.02424.pdf) for more details.\n",
    "            diversity_penalty (`float`, *optional*, defaults to 0.0):\n",
    "                This value is subtracted from a beam's score if it generates a token same as any beam from other group\n",
    "                at a particular time. Note that `diversity_penalty` is only effective if `group beam search` is\n",
    "                enabled.\n",
    "            prefix_allowed_tokens_fn: (`Callable[[int, torch.Tensor], List[int]]`, *optional*):\n",
    "                If provided, this function constraints the beam search to allowed tokens only at each step. If not\n",
    "                provided no constraint is applied. This function takes 2 arguments: the batch ID `batch_id` and\n",
    "                `input_ids`. It has to return a list with the allowed tokens for the next generation step conditioned\n",
    "                on the batch ID `batch_id` and the previously generated tokens `inputs_ids`. This argument is useful\n",
    "                for constrained generation conditioned on the prefix, as described in [Autoregressive Entity\n",
    "                Retrieval](https://arxiv.org/abs/2010.00904).\n",
    "            logits_processor (`LogitsProcessorList`, *optional*):\n",
    "                 Custom logits processors that complement the default logits processors built from arguments and a\n",
    "                 model's config. If a logit processor is passed that is already created with the arguments or a model's\n",
    "                 config an error is thrown. This feature is intended for advanced users.\n",
    "            stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
    "                 Custom stopping criteria that complement the default stopping criteria built from arguments and a\n",
    "                 model's config. If a stopping criteria is passed that is already created with the arguments or a\n",
    "                 model's config an error is thrown. This feature is intended for advanced users.\n",
    "            constraints (`List[Constraint]`, *optional*):\n",
    "                 Custom constraints that can be added to the generation to ensure that the output will contain the use\n",
    "                 of certain tokens as defined by `Constraint` objects, in the most sensible way possible.\n",
    "            output_attentions (`bool`, *optional*, defaults to `False`):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more details.\n",
    "            output_hidden_states (`bool`, *optional*, defaults to `False`):\n",
    "                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
    "                for more details.\n",
    "            output_scores (`bool`, *optional*, defaults to `False`):\n",
    "                Whether or not to return the prediction scores. See `scores` under returned tensors for more details.\n",
    "            return_dict_in_generate (`bool`, *optional*, defaults to `False`):\n",
    "                Whether or not to return a [`~file_utils.ModelOutput`] instead of a plain tuple.\n",
    "            forced_bos_token_id (`int`, *optional*):\n",
    "                The id of the token to force as the first generated token after the `decoder_start_token_id`. Useful\n",
    "                for multilingual models like [mBART](../model_doc/mbart) where the first generated token needs to be\n",
    "                the target language token.\n",
    "            forced_eos_token_id (`int`, *optional*):\n",
    "                The id of the token to force as the last generated token when `max_length` is reached.\n",
    "            remove_invalid_values (`bool`, *optional*):\n",
    "                Whether to remove possible *nan* and *inf* outputs of the model to prevent the generation method to\n",
    "                crash. Note that using `remove_invalid_values` can slow down generation.\n",
    "            synced_gpus (`bool`, *optional*, defaults to `False`):\n",
    "                Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n",
    "            model_kwargs:\n",
    "                Additional model specific kwargs will be forwarded to the `forward` function of the model. If the model\n",
    "                is an encoder-decoder model, encoder specific kwargs should not be prefixed and decoder specific kwargs\n",
    "                should be prefixed with *decoder_*.\n",
    "\n",
    "        Return:\n",
    "            [`~file_utils.ModelOutput`] or `torch.LongTensor`: A [`~file_utils.ModelOutput`] (if\n",
    "            `return_dict_in_generate=True` or when `config.return_dict_in_generate=True`) or a `torch.FloatTensor`.\n",
    "\n",
    "                If the model is *not* an encoder-decoder model (`model.config.is_encoder_decoder=False`), the possible\n",
    "                [`~file_utils.ModelOutput`] types are:\n",
    "\n",
    "                    - [`~generation_utils.GreedySearchDecoderOnlyOutput`],\n",
    "                    - [`~generation_utils.SampleDecoderOnlyOutput`],\n",
    "                    - [`~generation_utils.BeamSearchDecoderOnlyOutput`],\n",
    "                    - [`~generation_utils.BeamSampleDecoderOnlyOutput`]\n",
    "\n",
    "                If the model is an encoder-decoder model (`model.config.is_encoder_decoder=True`), the possible\n",
    "                [`~file_utils.ModelOutput`] types are:\n",
    "\n",
    "                    - [`~generation_utils.GreedySearchEncoderDecoderOutput`],\n",
    "                    - [`~generation_utils.SampleEncoderDecoderOutput`],\n",
    "                    - [`~generation_utils.BeamSearchEncoderDecoderOutput`],\n",
    "                    - [`~generation_utils.BeamSampleEncoderDecoderOutput`]\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "\n",
    "        >>> tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "        >>> model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "        >>> # do greedy decoding without providing a prompt\n",
    "        >>> outputs = model.generate(max_length=40)\n",
    "        >>> print(\"Generated:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "        >>> tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "        >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "        >>> document = (\n",
    "        ...     \"at least two people were killed in a suspected bomb attack on a passenger bus \"\n",
    "        ...     \"in the strife-torn southern philippines on monday , the military said.\"\n",
    "        ... )\n",
    "        >>> # encode input context\n",
    "        >>> input_ids = tokenizer(document, return_tensors=\"pt\").input_ids\n",
    "        >>> # generate 3 independent sequences using beam search decoding (5 beams)\n",
    "        >>> # with T5 encoder-decoder model conditioned on short news article.\n",
    "        >>> outputs = model.generate(input_ids=input_ids, num_beams=5, num_return_sequences=3)\n",
    "        >>> print(\"Generated:\", tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "\n",
    "        >>> tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "        >>> model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "        >>> input_context = \"The dog\"\n",
    "        >>> # encode input context\n",
    "        >>> input_ids = tokenizer(input_context, return_tensors=\"pt\").input_ids\n",
    "        >>> # generate 3 candidates using sampling\n",
    "        >>> outputs = model.generate(input_ids=input_ids, max_length=20, num_return_sequences=3, do_sample=True)\n",
    "        >>> print(\"Generated:\", tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "\n",
    "        >>> tokenizer = AutoTokenizer.from_pretrained(\"ctrl\")\n",
    "        >>> model = AutoModelForCausalLM.from_pretrained(\"ctrl\")\n",
    "        >>> # \"Legal\" is one of the control codes for ctrl\n",
    "        >>> input_context = \"Legal My neighbor is\"\n",
    "        >>> # encode input context\n",
    "        >>> input_ids = tokenizer(input_context, return_tensors=\"pt\").input_ids\n",
    "        >>> outputs = model.generate(input_ids=input_ids, max_length=20, repetition_penalty=1.2)\n",
    "        >>> print(\"Generated:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "        >>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=False)\n",
    "        >>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "        >>> input_context = \"My cute dog\"\n",
    "        >>> # get tokens of words that should not be generated\n",
    "        >>> bad_words_ids = tokenizer(\n",
    "        ...     [\"idiot\", \"stupid\", \"shut up\"], add_prefix_space=True, add_special_tokens=False\n",
    "        >>> ).input_ids\n",
    "        >>> # encode input context\n",
    "        >>> input_ids = tokenizer(input_context, return_tensors=\"pt\").input_ids\n",
    "        >>> # generate sequences without allowing bad_words to be generated\n",
    "        >>> outputs = model.generate(input_ids=input_ids, max_length=20, do_sample=True, bad_words_ids=bad_words_ids)\n",
    "        >>> print(\"Generated:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "        ```\"\"\"\n",
    "        # 1. Set generation parameters if not already defined\n",
    "        bos_token_id = bos_token_id if bos_token_id is not None else self.config.bos_token_id\n",
    "        num_beams = num_beams if num_beams is not None else self.config.num_beams\n",
    "        length_penalty = length_penalty if length_penalty is not None else self.config.length_penalty\n",
    "        early_stopping = early_stopping if early_stopping is not None else self.config.early_stopping\n",
    "        num_beam_groups = num_beam_groups if num_beam_groups is not None else self.config.num_beam_groups\n",
    "        do_sample = do_sample if do_sample is not None else self.config.do_sample\n",
    "        num_return_sequences = (\n",
    "            num_return_sequences if num_return_sequences is not None else self.config.num_return_sequences\n",
    "        )\n",
    "\n",
    "        pad_token_id = pad_token_id if pad_token_id is not None else self.config.pad_token_id\n",
    "        eos_token_id = eos_token_id if eos_token_id is not None else self.config.eos_token_id\n",
    "\n",
    "        if eos_token_id is None and hasattr(self.config, \"decoder\"):\n",
    "            eos_token_id = self.config.decoder.eos_token_id\n",
    "\n",
    "        if pad_token_id is None and eos_token_id is not None:\n",
    "            # special case if pad_token_id is not defined\n",
    "            logger.warning(f\"Setting `pad_token_id` to `eos_token_id`:{eos_token_id} for open-end generation.\")\n",
    "            pad_token_id = eos_token_id\n",
    "\n",
    "        output_scores = output_scores if output_scores is not None else self.config.output_scores\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict_in_generate = (\n",
    "            return_dict_in_generate if return_dict_in_generate is not None else self.config.return_dict_in_generate\n",
    "        )\n",
    "\n",
    "        # 2. Define model inputs\n",
    "        # inputs_tensor has to be defined\n",
    "        # model_input_name is defined if model-specific keyword input is passed\n",
    "        # otherwise model_input_name is None\n",
    "        # all model-specific keyword inputs are removed from `model_kwargs`\n",
    "        inputs_tensor, model_input_name, model_kwargs = self._prepare_model_inputs(inputs, bos_token_id, model_kwargs)\n",
    "        batch_size = inputs_tensor.shape[0]\n",
    "\n",
    "        # 3. Define other model kwargs\n",
    "        model_kwargs[\"output_attentions\"] = output_attentions\n",
    "        model_kwargs[\"output_hidden_states\"] = output_hidden_states\n",
    "        model_kwargs[\"use_cache\"] = use_cache\n",
    "\n",
    "        accepts_attention_mask = \"attention_mask\" in set(inspect.signature(self.forward).parameters.keys())\n",
    "        requires_attention_mask = \"encoder_outputs\" not in model_kwargs\n",
    "\n",
    "        if model_kwargs.get(\"attention_mask\", None) is None and requires_attention_mask and accepts_attention_mask:\n",
    "            model_kwargs[\"attention_mask\"] = self._prepare_attention_mask_for_generation(\n",
    "                inputs_tensor, pad_token_id, eos_token_id\n",
    "            )\n",
    "\n",
    "        if self.config.is_encoder_decoder and \"encoder_outputs\" not in model_kwargs:\n",
    "            # if model is encoder decoder encoder_outputs are created\n",
    "            # and added to `model_kwargs`\n",
    "            model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(\n",
    "                inputs_tensor, model_kwargs, model_input_name\n",
    "            )\n",
    "\n",
    "        # 4. Prepare `input_ids` which will be used for auto-regressive generation\n",
    "        if self.config.is_encoder_decoder:\n",
    "            input_ids = self._prepare_decoder_input_ids_for_generation(\n",
    "                batch_size,\n",
    "                decoder_start_token_id=decoder_start_token_id,\n",
    "                bos_token_id=bos_token_id,\n",
    "                model_kwargs=model_kwargs,\n",
    "            )\n",
    "        else:\n",
    "            # if decoder-only then inputs_tensor has to be `input_ids`\n",
    "            input_ids = inputs_tensor\n",
    "\n",
    "        # 5. Prepare `max_length` depending on other stopping criteria\n",
    "        # if `max_new_tokens` is passed, but not `max_length` -> set `max_length = max_new_tokens`\n",
    "        if max_length is None and max_new_tokens is not None:\n",
    "            max_length = max_new_tokens + input_ids.shape[-1]\n",
    "        elif max_length is not None and max_new_tokens is not None:\n",
    "            # Both are set, this is odd, raise a warning\n",
    "            warnings.warn(\n",
    "                \"Both `max_length` and `max_new_tokens` have been set \"\n",
    "                f\"but they serve the same purpose. `max_length` {max_length} \"\n",
    "                f\"will take priority over `max_new_tokens` {max_new_tokens}.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "        # default to config if still None\n",
    "        max_length = max_length if max_length is not None else self.config.max_length\n",
    "\n",
    "        if input_ids.shape[-1] >= max_length:\n",
    "            input_ids_string = \"decoder_input_ids\" if self.config.is_encoder_decoder else \"input_ids\"\n",
    "            logger.warning(\n",
    "                f\"Input length of {input_ids_string} is {input_ids.shape[-1]}, but ``max_length`` is set to {max_length}. \"\n",
    "                \"This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\"\n",
    "            )\n",
    "\n",
    "        # 6. determine generation mode\n",
    "        is_constraint_gen_mode = constraints is not None\n",
    "        is_greedy_gen_mode = (num_beams == 1) and (num_beam_groups == 1) and do_sample is False and constraints is None\n",
    "        is_sample_gen_mode = (num_beams == 1) and (num_beam_groups == 1) and do_sample is True and constraints is None\n",
    "        is_beam_gen_mode = (num_beams > 1) and (num_beam_groups == 1) and do_sample is False and constraints is None\n",
    "        is_beam_sample_gen_mode = (\n",
    "            (num_beams > 1) and (num_beam_groups == 1) and do_sample is True and constraints is None\n",
    "        )\n",
    "        is_group_beam_gen_mode = (num_beams > 1) and (num_beam_groups > 1) and constraints is None\n",
    "\n",
    "        if num_beam_groups > num_beams:\n",
    "            raise ValueError(\"`num_beam_groups` has to be smaller or equal to `num_beams`\")\n",
    "        if is_group_beam_gen_mode and do_sample is True:\n",
    "            raise ValueError(\n",
    "                \"Diverse beam search cannot be used in sampling mode. Make sure that `do_sample` is set to `False`.\"\n",
    "            )\n",
    "\n",
    "        # 7. prepare distribution pre_processing samplers\n",
    "        logits_processor = self._get_logits_processor(\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "            encoder_no_repeat_ngram_size=encoder_no_repeat_ngram_size,\n",
    "            encoder_input_ids=inputs_tensor,\n",
    "            bad_words_ids=bad_words_ids,\n",
    "            min_length=min_length,\n",
    "            max_length=max_length,\n",
    "            eos_token_id=eos_token_id,\n",
    "            forced_bos_token_id=forced_bos_token_id,\n",
    "            forced_eos_token_id=forced_eos_token_id,\n",
    "            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n",
    "            num_beams=num_beams,\n",
    "            num_beam_groups=num_beam_groups,\n",
    "            diversity_penalty=diversity_penalty,\n",
    "            remove_invalid_values=remove_invalid_values,\n",
    "            logits_processor=logits_processor,\n",
    "        )\n",
    "\n",
    "        # 8. prepare stopping criteria\n",
    "        stopping_criteria = self._get_stopping_criteria(\n",
    "            max_length=max_length, max_time=max_time, stopping_criteria=stopping_criteria\n",
    "        )\n",
    "\n",
    "        # 9. go into different generation modes\n",
    "        if is_greedy_gen_mode:\n",
    "            if num_return_sequences > 1:\n",
    "                raise ValueError(\n",
    "                    f\"num_return_sequences has to be 1, but is {num_return_sequences} when doing greedy search.\"\n",
    "                )\n",
    "\n",
    "            # 10. run greedy search\n",
    "            return self.greedy_search(\n",
    "                input_ids,\n",
    "                logits_processor=logits_processor,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                pad_token_id=pad_token_id,\n",
    "                eos_token_id=eos_token_id,\n",
    "                output_scores=output_scores,\n",
    "                return_dict_in_generate=return_dict_in_generate,\n",
    "                synced_gpus=synced_gpus,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "        elif is_sample_gen_mode:\n",
    "            # 10. prepare logits warper\n",
    "            logits_warper = self._get_logits_warper(\n",
    "                top_k=top_k, top_p=top_p, typical_p=typical_p, temperature=temperature, num_beams=num_beams\n",
    "            )\n",
    "\n",
    "            # 11. expand input_ids with `num_return_sequences` additional sequences per batch\n",
    "            input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
    "                input_ids,\n",
    "                expand_size=num_return_sequences,\n",
    "                is_encoder_decoder=self.config.is_encoder_decoder,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "            # 12. run sample\n",
    "            return self.sample(\n",
    "                input_ids,\n",
    "                logits_processor=logits_processor,\n",
    "                logits_warper=logits_warper,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                pad_token_id=pad_token_id,\n",
    "                eos_token_id=eos_token_id,\n",
    "                output_scores=output_scores,\n",
    "                return_dict_in_generate=return_dict_in_generate,\n",
    "                synced_gpus=synced_gpus,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "        elif is_beam_gen_mode:\n",
    "            if num_return_sequences > num_beams:\n",
    "                raise ValueError(\"`num_return_sequences` has to be smaller or equal to `num_beams`.\")\n",
    "\n",
    "            if stopping_criteria.max_length is None:\n",
    "                raise ValueError(\"`max_length` needs to be a stopping_criteria for now.\")\n",
    "\n",
    "            # 10. prepare beam search scorer\n",
    "            beam_scorer = BeamSearchScorer(\n",
    "                batch_size=batch_size,\n",
    "                num_beams=num_beams,\n",
    "                device=self.device,\n",
    "                length_penalty=length_penalty,\n",
    "                do_early_stopping=early_stopping,\n",
    "                num_beam_hyps_to_keep=num_return_sequences,\n",
    "            )\n",
    "            # 11. interleave input_ids with `num_beams` additional sequences per batch\n",
    "            input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
    "                input_ids, expand_size=num_beams, is_encoder_decoder=self.config.is_encoder_decoder, **model_kwargs\n",
    "            )\n",
    "            # 12. run beam search\n",
    "            return self.beam_search(\n",
    "                input_ids,\n",
    "                beam_scorer,\n",
    "                logits_processor=logits_processor,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                pad_token_id=pad_token_id,\n",
    "                eos_token_id=eos_token_id,\n",
    "                output_scores=output_scores,\n",
    "                return_dict_in_generate=return_dict_in_generate,\n",
    "                synced_gpus=synced_gpus,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "        elif is_beam_sample_gen_mode:\n",
    "            # 10. prepare logits warper\n",
    "            logits_warper = self._get_logits_warper(\n",
    "                top_k=top_k, top_p=top_p, typical_p=typical_p, temperature=temperature, num_beams=num_beams\n",
    "            )\n",
    "\n",
    "            if stopping_criteria.max_length is None:\n",
    "                raise ValueError(\"`max_length` needs to be a stopping_criteria for now.\")\n",
    "            # 11. prepare beam search scorer\n",
    "            beam_scorer = BeamSearchScorer(\n",
    "                batch_size=batch_size * num_return_sequences,\n",
    "                num_beams=num_beams,\n",
    "                device=self.device,\n",
    "                length_penalty=length_penalty,\n",
    "                do_early_stopping=early_stopping,\n",
    "            )\n",
    "\n",
    "            # 12. interleave input_ids with `num_beams` additional sequences per batch\n",
    "            input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
    "                input_ids,\n",
    "                expand_size=num_beams * num_return_sequences,\n",
    "                is_encoder_decoder=self.config.is_encoder_decoder,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "            # 13. run beam sample\n",
    "            return self.beam_sample(\n",
    "                input_ids,\n",
    "                beam_scorer,\n",
    "                logits_processor=logits_processor,\n",
    "                logits_warper=logits_warper,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                pad_token_id=pad_token_id,\n",
    "                eos_token_id=eos_token_id,\n",
    "                output_scores=output_scores,\n",
    "                return_dict_in_generate=return_dict_in_generate,\n",
    "                synced_gpus=synced_gpus,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "        elif is_group_beam_gen_mode:\n",
    "            if num_return_sequences > num_beams:\n",
    "                raise ValueError(\"`num_return_sequences` has to be smaller or equal to `num_beams`.\")\n",
    "\n",
    "            if num_beams % num_beam_groups != 0:\n",
    "                raise ValueError(\"`num_beams` should be divisible by `num_beam_groups` for group beam search.\")\n",
    "\n",
    "            if stopping_criteria.max_length is None:\n",
    "                raise ValueError(\"`max_length` needs to be a stopping_criteria for now.\")\n",
    "\n",
    "            # 10. prepare beam search scorer\n",
    "            beam_scorer = BeamSearchScorer(\n",
    "                batch_size=batch_size,\n",
    "                num_beams=num_beams,\n",
    "                max_length=stopping_criteria.max_length,\n",
    "                device=self.device,\n",
    "                length_penalty=length_penalty,\n",
    "                do_early_stopping=early_stopping,\n",
    "                num_beam_hyps_to_keep=num_return_sequences,\n",
    "                num_beam_groups=num_beam_groups,\n",
    "            )\n",
    "            # 11. interleave input_ids with `num_beams` additional sequences per batch\n",
    "            input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
    "                input_ids, expand_size=num_beams, is_encoder_decoder=self.config.is_encoder_decoder, **model_kwargs\n",
    "            )\n",
    "            # 12. run beam search\n",
    "            return self.group_beam_search(\n",
    "                input_ids,\n",
    "                beam_scorer,\n",
    "                logits_processor=logits_processor,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                pad_token_id=pad_token_id,\n",
    "                eos_token_id=eos_token_id,\n",
    "                output_scores=output_scores,\n",
    "                return_dict_in_generate=return_dict_in_generate,\n",
    "                synced_gpus=synced_gpus,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "        elif is_constraint_gen_mode:\n",
    "            if num_return_sequences > num_beams:\n",
    "                raise ValueError(\"`num_return_sequences` has to be smaller or equal to `num_beams`.\")\n",
    "\n",
    "            if stopping_criteria.max_length is None:\n",
    "                raise ValueError(\"`max_length` needs to be a stopping_criteria for now.\")\n",
    "\n",
    "            if num_beams <= 1:\n",
    "                raise ValueError(\"`num_beams` needs to be greater than 1 for constrained genertation.\")\n",
    "\n",
    "            if do_sample:\n",
    "                raise ValueError(\"`do_sample` needs to be false for constrained generation.\")\n",
    "\n",
    "            if num_beam_groups is not None and num_beam_groups > 1:\n",
    "                raise ValueError(\"`num_beam_groups` not supported yet for constrained generation.\")\n",
    "\n",
    "            # 10. prepare beam search scorer\n",
    "            constrained_beam_scorer = ConstrainedBeamSearchScorer(\n",
    "                constraints=constraints,\n",
    "                batch_size=batch_size,\n",
    "                num_beams=num_beams,\n",
    "                device=self.device,\n",
    "                length_penalty=length_penalty,\n",
    "                do_early_stopping=early_stopping,\n",
    "                num_beam_hyps_to_keep=num_return_sequences,\n",
    "            )\n",
    "            # 11. interleave input_ids with `num_beams` additional sequences per batch\n",
    "            input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
    "                input_ids, expand_size=num_beams, is_encoder_decoder=self.config.is_encoder_decoder, **model_kwargs\n",
    "            )\n",
    "            # 12. run beam search\n",
    "            return self.constrained_beam_search(\n",
    "                input_ids,\n",
    "                constrained_beam_scorer=constrained_beam_scorer,\n",
    "                logits_processor=logits_processor,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                pad_token_id=pad_token_id,\n",
    "                eos_token_id=eos_token_id,\n",
    "                output_scores=output_scores,\n",
    "                return_dict_in_generate=return_dict_in_generate,\n",
    "                synced_gpus=synced_gpus,\n",
    "                **model_kwargs,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 5195,   318, 15638, 10737,   523,  2383,   284,   262,  4687, 36806,\n",
       "          4816,    30,   220,   628,   357,    32,     8,  1318,  3588,   447,\n",
       "           247,    83,  1576,  1762,   661,   287,   262,   995,    13,  1119,\n",
       "          1839,   447,   247,    83,   307,  1498,   284,  1064,   257,  9014,\n",
       "            13,   198,   357,    33,     8,  1081,   530,   286,   734,  5637,\n",
       "         34752,  8952,    11,   340,   561,  1884,  1612,   262, 47613,   278,\n",
       "           290,  4423,   866,   286,   262,  4687, 36806,  4816,    13,   198,\n",
       "           357,    34,     8, 13614,   649, 34752,  8952,   318, 16378,   290,\n",
       "           640, 18587,    13,  1119,  1839,   447,   247,    83,   423,  2687,\n",
       "          2073,  3492,   706,   683,    13,   198,   357,    35,     8,  2399,\n",
       "         10737,   743, 18330,  1854,   284, 50257]),\n",
       " tensor([ 2061,  1838,   402,   549, 27176,   281,   503,  2505,   287,   262,\n",
       "          1944,  1110,    30,   628,   357,    32,     8,   679,   318,   881,\n",
       "          4697,   621,   262,  1334,   286,   262,  3265,    13,   198,   357,\n",
       "            33,     8,   679, 17567,   649,  4560,   326,   714,  2987,   465,\n",
       "          1535,    13,   198,   357,    34,     8,  2399,  2000,   318,   991,\n",
       "          4075,    11,   290,   339,  3815,  1327,   670,    13,   198,   357,\n",
       "            35,     8,   679,   991, 17326, 15232,   290,  1988,  5563,   588,\n",
       "           262,  3869,  2342,  1813,   284, 15638,    13,   628,   198,  4303,\n",
       "          2246,  3620,  1565,  6177,   317,  6226, 11587,   628,   628,   198,\n",
       "           220,   220, 11050,   337,  8120,  4526, 40760,  3535,  5258,   628,\n",
       "           628,   198,   220,   220, 36000,   416,   399,   375,   417,   628,\n",
       "           628,   198, 50257])]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128, 50258])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(60.2067, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(61.4377, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hvdenv",
   "language": "python",
   "name": "hvdenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
