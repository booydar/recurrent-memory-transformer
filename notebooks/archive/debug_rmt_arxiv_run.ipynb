{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "libgomp: Invalid value for environment variable OMP_NUM_THREADS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/envs/accel_rmt/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n",
      "libgomp: Invalid value for environment variable OMP_NUM_THREADS\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "# from megatron.data.dataset_utils import get_indexed_dataset_\n",
    "\n",
    "# import horovod.torch as hvd\n",
    "# from dotenv import load_dotenv\n",
    "import torch\n",
    "import numpy as np\n",
    "import datasets\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from datasets import Dataset, load_dataset\n",
    "from huggingface_hub import hf_hub_download\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "\n",
    "import transformers  # noqa: E402\n",
    "from transformers import AutoConfig, AutoTokenizer, HfArgumentParser  # noqa: E402\n",
    "\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from lm_experiments_tools.utils import collect_run_configuration, get_cls_by_name, get_optimizer  # noqa: E402\n",
    "import lm_experiments_tools.optimizers as optimizers  # noqa: E402\n",
    "# from lm_experiments_tools import TrainerArgs\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "from megatron.data.dataset_utils import get_indexed_dataset_\n",
    "\n",
    "import horovod.torch as hvd\n",
    "# from dotenv import load_dotenv\n",
    "import torch\n",
    "import numpy as np\n",
    "import datasets\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from datasets import Dataset, load_dataset, load_from_disk\n",
    "from huggingface_hub import hf_hub_download\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "from lm_experiments_tools import TrainerArgs\n",
    "from lm_experiments_tools.trainer import Trainer\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from lm_experiments_tools.lm_datasets import get_lm_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load_dotenv()\n",
    "\n",
    "# logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "#                     level=logging.INFO)\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "# # if CUDA_VISIBLE_DEVICES is not set make all gpus visible\n",
    "# if os.environ.get('CUDA_VISIBLE_DEVICES', None) is None:\n",
    "#     os.environ['CUDA_VISIBLE_DEVICES'] = ','.join([str(i) for i in range(torch.cuda.device_count())])\n",
    "\n",
    "# logger.info(f\"CUDA_VISIBLE_DEVICES: {os.environ['CUDA_VISIBLE_DEVICES']}\")\n",
    "# # first call to torch.cuda.device_count() sets visible gpus, following calls will not change the result\n",
    "# logger.info(f\"CUDA DEVICE COUNT: {torch.cuda.device_count()}\")\n",
    "\n",
    "# hvd.init()\n",
    "\n",
    "\n",
    "# # limit # of CPU threads to be used per pytorch worker, otherwise it might use all cpus and throttle gpus\n",
    "# # > 2 fails cause of https://github.com/pytorch/pytorch/issues/56615\n",
    "# # need to upgrade to torch>1.8.1\n",
    "# torch.set_num_threads(4)\n",
    "# # all gpus set with CUDA_VISIBLE_DEVICES are visible to process, indexing from 0 to ...\n",
    "# torch.cuda.set_device(hvd.local_rank())\n",
    "\n",
    "# parser = HfArgumentParser(TrainerArgs)\n",
    "# parser.add_argument('--task_name', type=str, help=\"Task name, wikitext, ...\")\n",
    "# parser.add_argument('--validate_only', action='store_true', default=False,\n",
    "#                     help='Skip training and run only validation. (default: False)')\n",
    "# parser.add_argument('--working_dir', type=str, default='.',\n",
    "#                     help='working dir, should be a dir with t5-experiments repo (default: .)')\n",
    "# parser.add_argument('--seed', type=int, default=42, help='random seed')\n",
    "# parser.add_argument('--show_valid_examples', type=int, default=0,\n",
    "#                     help='how many valid examples to show during training (default: 0)')\n",
    "# parser.add_argument('--input_seq_len', type=int, default=128, help='input sequnce length (default: 128).')\n",
    "# parser.add_argument('--target_seq_len', type=int, default=16, help='target sequnce length, should be set to '\n",
    "#                                                                    'max(len(target))+1 for EOS (default: 16).')\n",
    "# parser.add_argument('--data_n_workers', type=int, default=2, help='number of dataloader workers (default: 2)')\n",
    "\n",
    "# parser.add_argument('--input_prefix', type=str, default='', help='add task prefix to an input string (default: \"\")')\n",
    "# parser.add_argument('--sliding_window', action='store_true', help='use slinding window attentinon mask, '\n",
    "#                     'eval on last segment only', default=False)\n",
    "\n",
    "# # model args\n",
    "# parser.add_argument('--from_pretrained', type=str, help='model name in HF Model Hub (default: \"\")')\n",
    "# parser.add_argument('--model_cfg', type=str, help='path to model configuration file (default: \"\")')\n",
    "# parser.add_argument('--model_cls', type=str, default='transformers:BertForPreTraining',\n",
    "#                     help='model class name to use (default: transformers:BertForPreTraining)')\n",
    "# parser.add_argument('--memory_cell_cls', type=str, default=None, help='cell class for RMT')\n",
    "# parser.add_argument('--recurrent_wrapper_cls', type=str, default=None, help='recurrent wrapper class for RMT')\n",
    "# parser.add_argument('--model_cpt', type=str, default=None, help='pretrained model checkpoint path')\n",
    "# parser.add_argument('--backbone_cls', type=str, default=None,\n",
    "#                     help='backbone class name to use for RMT')\n",
    "# parser.add_argument('--model_type', type=str, default='encoder-decoder',\n",
    "#                     help='model type, encoder, encoder-decoder, decoder, affects preprocessing '\n",
    "#                          '(default: encoder-decoder)')\n",
    "\n",
    "\n",
    "# # Aydar # RMT args \n",
    "# parser.add_argument('--input_size', type=int, default=None, help='maximal input size of the backbone model')\n",
    "# parser.add_argument('--num_mem_tokens', type=int, default=None, help='number of memory tokens.')\n",
    "# parser.add_argument('--max_n_segments', type=int, default=1, help='maximal segment number')\n",
    "# # parser.add_argument('--sum_loss', action='store_true', default=False,\n",
    "# #                     help='with this flag task loss from all segments is summed')\n",
    "# # parser.add_argument('--bptt_depth', type=int, default=-1, help='max number of previous segments in gradient computation.')\n",
    "# # parser.add_argument('--segment_ordering', type=str, help='segment order', default='regular',\n",
    "# #                     choices=['regular', 'reversed', 'bidirectional', 'repeat_first', 'last_memory_only'])\n",
    "# # parser.add_argument('--memory_forward_func', type=str, help='path to memory forward fun—Åtion script', default=None)\n",
    "# # parser.add_argument('--memory_layers', type=str, help='memory-augmented layer inds or \"all\" for all layers', default=None)\n",
    "# # parser.add_argument('--share_memory_layers', action='store_true', help='share weights of memory layers', default=False)\n",
    "# # parser.add_argument('--reconstruction_loss_coef', type=float, default=None,\n",
    "# #                     help='reconstuction loss ratio in total loss')\n",
    "# # # parser.add_argument('--segment_ordering', type=str,help='????', default='regular',\n",
    "# # #                     choices=['regular', 'reversed', 'bidirectional', 'repeat_first', 'last_memory_only'])\n",
    "# # parser.add_argument('--retain_graph', action='store_true', help='Retain computation graph during backward pass', default=False)\n",
    "# # parser.add_argument('--use_truncated_backward', action='store_true', default=False,\n",
    "# #                     help='whether to use RMT truncated bptt method in backward')\n",
    "# # parser.add_argument('--k1', type=int, default=-1, help='(not implemented) If not -1, gradient update is done each k1 segments')\n",
    "# parser.add_argument('--k2', type=int, default=-1, help='number of last segments used by backward')\n",
    "# parser.add_argument('--freeze_model_weights', action='store_true', default=False,\n",
    "#                     help='Stop training all model weights except memory layers')\n",
    "# parser.add_argument('--backbone_cpt', type=str, default=None, help='backbone model checkpoint path')\n",
    "\n",
    "\n",
    "# # tokenizer\n",
    "# # todo: add wordpiece tokenizers support?\n",
    "# parser.add_argument('--tokenizer', type=str, default=None, help='path or name of pre-trained HF Tokenizer')\n",
    "\n",
    "# # optimizer args\n",
    "# parser.add_argument('--optimizer', type=str, default='AdamW', help='optimizer name: AdamW, Adafactor. (default: AdamW)')\n",
    "# parser.add_argument('--weight_decay', type=float, default=0.0, help='optimizer weight decay (default: 0.0)')\n",
    "# parser.add_argument('--scale_parameter', action='store_true', default=False,\n",
    "#                     help='Adafactor scale_parameter (default: False)')\n",
    "# parser.add_argument('--relative_step', action='store_true', default=False,\n",
    "#                     help='Adafactor relative_step (default: False)')\n",
    "# parser.add_argument('--warmup_init', action='store_true', default=False,\n",
    "#                     help='Adafactor warmup_init (default: False)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Holder:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            return super().__getattr__(name)\n",
    "        except AttributeError:\n",
    "            return None\n",
    "\n",
    "args = Holder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.from_pretrained = 'gpt2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs = (\"--model_path\", \"/cephfs/home/bulatov/bulatov/RMT_light/runs/test/cnli_original\",\n",
    "\"--from_pretrained\", \"gpt2\",\n",
    "\"--task_name\", \"arxiv\",\n",
    "\"--model_type\", \"decoder\",\n",
    "\"--model_cls\", \"modeling_rmt.deprecated:RMTDecoderLMHeadMultiSeg\",\n",
    "# \"--backbone_cls\",  \"transformers:AutoModelForCausalLM\",\n",
    "\"--backbone_cls\",  \"base_models.modeling_gpt2:GPT2LMHeadModel\",\n",
    "\"--model_cpt\", \"/home/jovyan/rmt/runs/arxiv/gpt2/linear_adamw_wd1e-03_868-128-7x128_mem2_bs32_regular_bptt-7_from_cpt_cv2_6-7/run_1\", \n",
    "# \"--model_cpt\", \"/home/jovyan/rmt/runs/arxiv/gpt2/linear_adamw_wd1e-03_248-128-2x128_mem2_bs32_regular_bptt-2_from_cpt_cv2_1-2/run_1\", \n",
    "\"--optimizer\", \"AdamW\",\n",
    "\"--weight_decay\", \"0.001\",\n",
    "\"--lr\", \"1e-03\", \n",
    "\"--lr_scheduler\", \"constant_with_warmup\",\n",
    "\"--clip_grad_value\", \"5.0\")\n",
    "\n",
    "int_attrs = (\"--input_seq_len\", str((128-4)*64),\n",
    "\"--input_size\", \"128\",\n",
    "\"--target_seq_len\", \"128\",\n",
    "\"--num_mem_tokens\", \"2\",\n",
    "\"--max_n_segments\" ,\"64\", \n",
    "\"--batch_size\", \"1\", \n",
    "\"--gradient_accumulation_steps\", \"1\",\n",
    "\"--iters\", \"100\",\n",
    "\"--num_warmup_steps\", \"100\",\n",
    "\"--data_n_workers\", \"2\",\n",
    "\"--log_interval\", \"10\",\n",
    "\"--show_valid_examples\", \"5\",\n",
    "\"--early_stopping_patience\", \"15\",\n",
    "\"--seed\", \"42\",\n",
    "\"--k2\", \"-1\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a, v in zip(int_attrs[::2], int_attrs[1::2]):\n",
    "    setattr(args, a.split('--')[1], int(v))\n",
    "\n",
    "for a, v in zip(attrs[::2], attrs[1::2]):\n",
    "    try:\n",
    "        setattr(args, a.split('--')[1], float(v))\n",
    "    except ValueError:\n",
    "        setattr(args, a.split('--')[1], v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# args = parser.parse_args()\n",
    "# set current working dir\n",
    "# args.working_dir = str(Path(args.working_dir).expanduser().absolute())\n",
    "# os.chdir(args.working_dir)\n",
    "# if hvd.rank() == 0:\n",
    "#     logger.info(f'hvd size: {hvd.size()}')\n",
    "#     logger.info(f'FP16: {args.fp16}')\n",
    "\n",
    "# if hvd.rank() == 0 and args.model_path is None:\n",
    "#     logger.warning('model_path is not set: config, logs and checkpoints will not be saved.')\n",
    "\n",
    "# # create model path and save configuration\n",
    "# if hvd.rank() == 0 and args.model_path is not None:\n",
    "#     model_path = Path(args.model_path)\n",
    "#     if not model_path.exists():\n",
    "#         Path(model_path).mkdir(parents=True)\n",
    "#     args_dict = collect_run_configuration(args)\n",
    "#     # todo: if model path exists and there is config file, write new config file aside\n",
    "#     json.dump(args_dict, open(model_path/'config.json', 'w'), indent=4)\n",
    "\n",
    "if not args.from_pretrained:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.from_pretrained)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "block_size = args.input_size \n",
    "if args.num_mem_tokens is not None:\n",
    "    block_size -= 2 * args.num_mem_tokens\n",
    "if args.xl_cache_size is not None:\n",
    "    block_size -= args.xl_cache_size\n",
    "history_size = args.input_seq_len - block_size\n",
    "\n",
    "class segmentDataLoaderOTF(DataLoader):\n",
    "    def __init__(self, dataset, block_size, history_size, max_samples=None, shuffle=False, *args, **kwargs):\n",
    "        super().__init__(dataset, *args, **kwargs)\n",
    "        self.block_size = block_size\n",
    "        self.history_size = history_size\n",
    "        self.max_samples = max_samples\n",
    "        self.shuffle = shuffle\n",
    "            \n",
    "    def get_samples(self, document):\n",
    "        input_ids, attention_mask = document['input_ids'], document['attention_mask']\n",
    "        samples = [input_ids[max({0, start - self.history_size}): start + self.block_size] for start in range(0, len(input_ids), self.block_size)]\n",
    "        return samples\n",
    "    \n",
    "    def __iter__(self):\n",
    "        inds = list(range(len(self.dataset)))\n",
    "\n",
    "        if self.max_samples is not None:\n",
    "            inds = inds[:self.max_samples]\n",
    "\n",
    "        if self.shuffle: \n",
    "            np.random.shuffle(inds)\n",
    "\n",
    "        doc_ind = 0\n",
    "        samples = []\n",
    "\n",
    "        while True:\n",
    "            if doc_ind >= len(inds):\n",
    "                break\n",
    "            try: \n",
    "                while len(samples) < self.batch_size:\n",
    "                    document = self.dataset[inds[doc_ind]]\n",
    "                    doc_ind += 1\n",
    "                    samples += self.get_samples(document)\n",
    "                    if doc_ind >= len(inds):\n",
    "                        raise(StopIteration)\n",
    "            except(StopIteration):\n",
    "                pass\n",
    "\n",
    "            batch, samples = samples[:self.batch_size], samples[self.batch_size:]\n",
    "            yield self.collate_fn(batch)\n",
    "        \n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "id_pad_value = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "def collate_fn(batch):\n",
    "    input_ids = labels = [torch.tensor(b[::-1]) for b in batch]\n",
    "    attention_mask = [torch.ones_like(b, dtype=int) for b in input_ids]\n",
    "    input_ids = pad_sequence(input_ids, padding_value=id_pad_value).T.flip(1)\n",
    "    labels = pad_sequence(labels, padding_value=-100).T.flip(1)\n",
    "    attention_mask = pad_sequence(attention_mask, padding_value=0).T.flip(1)\n",
    "\n",
    "    collated = {'input_ids': input_ids,\n",
    "                'labels': labels, \n",
    "                'attention_mask': attention_mask}\n",
    "    \n",
    "    if input_ids.shape[1] != block_size:\n",
    "        labels_mask = torch.ones_like(input_ids, dtype=bool)\n",
    "        labels_mask[:, :-block_size] = False\n",
    "        collated['labels_mask'] = labels_mask\n",
    "\n",
    "    if getattr(args, 'vary_n_segments', False):\n",
    "        if getattr(args, 'sampling_prob', False):\n",
    "            random_value = torch.rand((1)).item()\n",
    "            if random_value > args.sampling_prob:\n",
    "                return collated\n",
    "        n_segments = random.randint(0, args.max_n_segments)\n",
    "        n_tokens = n_segments * block_size\n",
    "        for k in collated:\n",
    "            collated[k] = collated[k][:, -n_tokens:]\n",
    "    \n",
    "    return collated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_from_disk('/home/jovyan/rmt/datasets/arxiv/train')\n",
    "valid_dataset = load_from_disk('/home/jovyan/rmt/datasets/arxiv/valid')\n",
    "# test_dataset = load_from_disk('/home/jovyan/rmt/datasets/arxiv/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# shuffle train data each epoch (one loop over train_dataset)\n",
    "# train_sampler = RandomSampler(train_dataset, rank=hvd.rank(), num_replicas=hvd.size(), shuffle=True,\n",
    "                                    # drop_last=False, seed=args.seed)\n",
    "per_worker_batch_size = args.batch_size \n",
    "# global_batch_size = per_worker_batch_size * hvd.size()\n",
    "kwargs = {'pin_memory': True, 'num_workers': args.data_n_workers}\n",
    "if not args.validate_only:\n",
    "    train_dataloader = segmentDataLoaderOTF(train_dataset, batch_size=per_worker_batch_size, \n",
    "                                            # sampler=train_sampler,\n",
    "                                    block_size=block_size, \n",
    "                                    history_size=history_size, \n",
    "                                    shuffle=True,\n",
    "                                    collate_fn=collate_fn, **kwargs)\n",
    "else:\n",
    "    train_dataloader = None\n",
    "\n",
    "# get validation dataset\n",
    "max_samples = None if args.validate_only else 100\n",
    "# max_samples = None\n",
    "valid_dataloader = None\n",
    "# if hvd.rank() == 0:\n",
    "#     logger.info(f'preparing validation data')\n",
    "# valid_sampler = DistributedSampler(valid_dataset, rank=hvd.rank(), num_replicas=hvd.size(), shuffle=False)\n",
    "valid_dataloader = segmentDataLoaderOTF(valid_dataset, batch_size=per_worker_batch_size,\n",
    "                                        #  sampler=valid_sampler,\n",
    "                                block_size=block_size, \n",
    "                                history_size=history_size, \n",
    "                                shuffle=False,\n",
    "                                max_samples=max_samples,\n",
    "                                collate_fn=collate_fn, drop_last=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# batch = next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in train_dataloader:\n",
    "#     for b in batch['input_ids']:\n",
    "#         if 50256 not in b:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = iter(train_dataloader)\n",
    "batches = []\n",
    "\n",
    "for _ in range(10000):\n",
    "    batch = next(gen)\n",
    "    inputs = batch['input_ids'][0]\n",
    "\n",
    "    if len(inputs) >= args.input_seq_len and 50256 not in inputs:\n",
    "        batches.append(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = iter(valid_dataloader)\n",
    "valid_batches = []\n",
    "\n",
    "for _ in range(10000):\n",
    "    batch = next(gen)\n",
    "    inputs = batch['input_ids'][0]\n",
    "\n",
    "    if len(inputs) >= args.input_seq_len and 50256 not in inputs:\n",
    "        valid_batches.append(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5556, 5449)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batches), len(valid_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-20 09:24:41,638] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model_cls = get_cls_by_name(args.backbone_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions\n",
    "\n",
    "from modeling_rmt.deprecated import RMTBaseModel\n",
    "\n",
    "class RMTDecoderLMHeadMultiSegSaveMem(RMTBaseModel):\n",
    "    def set_params(self, num_mem_tokens, tokenizer, **rmt_config):\n",
    "        self.rmt_config = rmt_config\n",
    "        self.extract_special_tokens(tokenizer)\n",
    "        self.create_memory(num_mem_tokens)\n",
    "\n",
    "        self.segment_size = rmt_config['input_size'] - 2 * num_mem_tokens - tokenizer.num_special_tokens_to_add()\n",
    "        if 'sep_token' in tokenizer.special_tokens_map:\n",
    "            self.segment_size -= 1\n",
    "\n",
    "    def create_memory(self, num_mem_tokens):\n",
    "        self.num_mem_tokens = num_mem_tokens\n",
    "        embeddings = self.model.get_input_embeddings()\n",
    "        memory_weights = torch.randn((num_mem_tokens, self.model.config.n_embd)) * embeddings.weight.data.std()\n",
    "        self.register_parameter('memory', torch.nn.Parameter(memory_weights, requires_grad=True))\n",
    "\n",
    "        self.read_memory_position = range(num_mem_tokens)\n",
    "        self.write_memory_position = range(-num_mem_tokens, 0)\n",
    "\n",
    "    def set_memory(self, input_shape):\n",
    "        create_memory = True\n",
    "        if create_memory:\n",
    "            memory = self.memory.repeat(input_shape[0], 1, 1)\n",
    "        else:\n",
    "            memory = self.memory_state[:input_shape[0]]\n",
    "        return memory\n",
    "    \n",
    "    def detach_memory(self, seg_num):\n",
    "        k2, max_n_segments = self.rmt_config.get('k2'), self.rmt_config.get('max_n_segments')\n",
    "        if seg_num == 0 \\\n",
    "            or k2 in {-1, None} \\\n",
    "            or seg_num + k2 > max_n_segments:\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\n",
    "                inputs_embeds=None, labels=None, labels_mask=None, output_attentions=None, output_hidden_states=None, return_dict=None):\n",
    "        kwargs = {'attention_mask': attention_mask, \n",
    "                #   'token_type_ids': token_type_ids,\n",
    "                  'position_ids': position_ids, 'inputs_embeds': inputs_embeds,\n",
    "                  'labels_mask': labels_mask, #'pos_weight': pos_weight,\n",
    "                  'labels': labels, 'output_attentions': output_attentions,\n",
    "                  'output_hidden_states': output_hidden_states, 'return_dict': return_dict,\n",
    "                  }\n",
    "\n",
    "        memory = self.set_memory(input_ids.shape)\n",
    "        segmented = self.pad_and_segment(input_ids, labels)\n",
    "\n",
    "        base_model_outputs = []\n",
    "        memories = []\n",
    "        for seg_num, segment in enumerate(zip(*segmented)):\n",
    "\n",
    "            seg_kwargs, non_empty_mask = self.prepare_kwargs(segment, memory, kwargs)\n",
    "            if sum(non_empty_mask) == 0:\n",
    "                continue\n",
    "            \n",
    "            if self.detach_memory(seg_num):\n",
    "                memory = memory.detach()\n",
    "\n",
    "            seg_kwargs['inputs_embeds'][:, self.read_memory_position] = memory[non_empty_mask]\n",
    "            seg_kwargs['inputs_embeds'][:, self.write_memory_position] = memory[non_empty_mask]\n",
    "            out = self.model(**seg_kwargs)\n",
    "            base_model_outputs.append(out)\n",
    "            \n",
    "            memory[non_empty_mask] = out.hidden_states[-1][:, self.write_memory_position]\n",
    "            memories.append(memory[non_empty_mask])\n",
    "\n",
    "        self.memory_state = memory\n",
    "        out = self.process_outputs(base_model_outputs, kwargs)\n",
    "        out['memories'] = memories\n",
    "        return out\n",
    "    \n",
    "    def pad_and_segment(self, input_ids, labels=None):\n",
    "        segmented_batch = []\n",
    "        segmented_batch_labels = []\n",
    "\n",
    "        if labels is None:\n",
    "            labels = [None] * input_ids.shape[0]\n",
    "        batch_labels = labels\n",
    "        for seq, labels in zip(input_ids, batch_labels):\n",
    "\n",
    "            align = self.rmt_config.get('segment_alignment')\n",
    "            if align in {'right', None}:\n",
    "                split_inds = (list(range(len(seq), 0, -self.segment_size)) + [0])[::-1]\n",
    "            elif align == 'left':\n",
    "                split_inds = list(range(0, len(seq), self.segment_size)) + [len(seq)]\n",
    "            elif align == 'center':\n",
    "                n_seg = math.ceil(len(seq) / self.segment_size)\n",
    "                split_inds = list(range(0, len(seq), math.ceil(len(seq) / n_seg))) + [len(seq)]\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            input_segments = [seq[start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "            # add empty segment markers if needed\n",
    "            n_empty_segments = self.rmt_config['max_n_segments'] - len(input_segments)\n",
    "            input_segments = [None] * n_empty_segments + input_segments\n",
    "            segmented_batch.append(input_segments)\n",
    "\n",
    "            if labels is not None:\n",
    "                labels_segments = [labels[start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "                labels_segments = [None] * n_empty_segments + labels_segments\n",
    "                segmented_batch_labels.append(labels_segments)\n",
    "\n",
    "        segmented_batch = [[sample[seg_num] for sample in segmented_batch]\n",
    "                           for seg_num in range(self.rmt_config['max_n_segments'])]\n",
    "        segmented_batch_labels = [[sample[seg_num] for sample in segmented_batch_labels]\n",
    "                                  for seg_num in range(self.rmt_config['max_n_segments'])]\n",
    "\n",
    "        return segmented_batch, segmented_batch_labels\n",
    "\n",
    "    def prepare_kwargs(self, segment, memory, kwargs):\n",
    "        segment_input_ids, segment_labels = segment\n",
    "        seg_kwargs = dict(**kwargs)\n",
    "        non_empty_mask = [s is not None for s in segment_input_ids]\n",
    "        if sum(non_empty_mask) == 0:\n",
    "            return None, non_empty_mask\n",
    "\n",
    "        input_ids = torch.stack([s for s in segment_input_ids if s is not None])\n",
    "        inputs_embeds = self.model.get_input_embeddings()(input_ids)\n",
    "        inputs_embeds = torch.cat([memory, inputs_embeds, memory], dim=1)\n",
    "\n",
    "        seg_kwargs['input_ids'] = None\n",
    "        seg_kwargs['inputs_embeds'] = inputs_embeds\n",
    "        seg_kwargs['attention_mask'] = self.get_attention_mask(inputs_embeds, input_ids)\n",
    "        seg_kwargs['output_hidden_states'] = True\n",
    "        if seg_kwargs['labels'] is not None:\n",
    "            labels = torch.stack([el for el, m in zip(segment_labels, non_empty_mask) if m])\n",
    "            memory_labels = torch.ones((labels.shape[0], self.num_mem_tokens), dtype=labels.dtype, device=labels.device) * -100\n",
    "            seg_kwargs['labels'] = torch.cat((memory_labels, labels, memory_labels), dim=1)\n",
    "            seg_kwargs['labels'][:, self.num_mem_tokens] = -100\n",
    "        seg_kwargs.pop('labels_mask')\n",
    "\n",
    "        return seg_kwargs, non_empty_mask\n",
    "    \n",
    "    def get_attention_mask(self, inputs_embeds, input_ids):\n",
    "        pad = torch.ones(*inputs_embeds.shape[:2], dtype=torch.int64).to(inputs_embeds.device)\n",
    "        if self.num_mem_tokens not in {0, None}:\n",
    "            mask = input_ids != self.pad_token_id\n",
    "            pad[:, self.num_mem_tokens: -self.num_mem_tokens] = mask\n",
    "        return pad\n",
    "    \n",
    "    def process_outputs(self, model_outputs, kwargs):\n",
    "        if self.num_mem_tokens in {0, None}:\n",
    "            full_logits = torch.cat([o.logits for o in model_outputs], dim=1)\n",
    "            truncated_hs = [[lh for lh in o.hidden_states] for o in model_outputs]\n",
    "        else:    \n",
    "            full_logits = torch.cat([o.logits[:, self.num_mem_tokens:-self.num_mem_tokens] for o in model_outputs], dim=1)\n",
    "            truncated_hs = [[lh[:, self.num_mem_tokens:-self.num_mem_tokens] for lh in o.hidden_states] for o in model_outputs]\n",
    "        full_hidden_states = tuple([torch.cat(layer_hs, dim=1) for layer_hs in zip(*truncated_hs)])\n",
    "\n",
    "        rmt_out = CausalLMOutputWithCrossAttentions()\n",
    "        full_labels = kwargs.get('labels')\n",
    "        if full_labels is not None:\n",
    "            shift_labels = full_labels[..., 1:].contiguous()\n",
    "            shift_logits = full_logits[..., :-1, :].contiguous()\n",
    "            flat_labels = shift_labels.view(-1)\n",
    "            flat_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "            \n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            labels_mask = kwargs.get('labels_mask')\n",
    "            if labels_mask is not None:\n",
    "                shift_mask = labels_mask[..., :-1].contiguous()\n",
    "\n",
    "                flat_labels = flat_labels[shift_mask.view(-1)]\n",
    "                flat_logits = flat_logits[shift_mask.view(-1)]\n",
    "                \n",
    "                \n",
    "            rmt_out['loss'] = loss_fct(flat_logits, flat_labels)\n",
    "\n",
    "        rmt_out['logits'] = full_logits\n",
    "        segment_keys = ['loss', 'logits']\n",
    "        if kwargs.get('output_attentions'):\n",
    "            segment_keys.append('attentions')\n",
    "        if kwargs.get('output_hidden_states'):\n",
    "            segment_keys.append('hidden_states')\n",
    "            rmt_out['hidden_states'] = full_hidden_states\n",
    "\n",
    "        for seg_num, out in enumerate(model_outputs):\n",
    "            for key, value in out.items():\n",
    "                if any([sk in key for sk in segment_keys]):\n",
    "                    rmt_out[f'{key}_{seg_num}'] = value\n",
    "\n",
    "        return rmt_out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not args.from_pretrained:\n",
    "    model_cfg = AutoConfig.from_pretrained(args.model_cfg)\n",
    "    model = model_cls(config=model_cfg)\n",
    "else:\n",
    "    # if hvd.rank() == 0:\n",
    "    #     logger.info(f'Loading pretrained model: {args.from_pretrained}')\n",
    "    model = model_cls.from_pretrained(args.from_pretrained)\n",
    "\n",
    "## load cpt of backbone model\n",
    "if args.backbone_cpt:\n",
    "    backbone_cpt = os.path.join(args.backbone_cpt, \"model_best.pth\")\n",
    "    cpt = torch.load(backbone_cpt, map_location='cpu')\n",
    "    model.load_state_dict(cpt['model_state_dict'])\n",
    "    # if hvd.rank() == 0:\n",
    "    #     logger.info(f'Loaded baseline state dict from: {args.backbone_cpt}')\n",
    "# Aydar # Pass memory settings to pretrained model\n",
    "if args.num_mem_tokens is not None:\n",
    "    if args.memory_forward_func is not None:\n",
    "        args.memory_forward_func = get_cls_by_name(args.memory_forward_func)\n",
    "\n",
    "    rmt_config = {\n",
    "        'num_mem_tokens': args.num_mem_tokens, \n",
    "        'max_n_segments': args.max_n_segments,\n",
    "        'xl_cache_size': args.xl_cache_size,\n",
    "        # 'segment_ordering': args.segment_ordering,\n",
    "        'input_size': args.input_size,\n",
    "        'k1': args.k1, 'k2': args.k2,\n",
    "        'sum_loss': args.sum_loss,\n",
    "        'tokenizer': tokenizer,\n",
    "        'memory_forward_func': args.memory_forward_func,\n",
    "        'memory_layers': args.memory_layers,\n",
    "        'share_memory_layers': args.share_memory_layers,\n",
    "        'reconstruction_loss_coef': args.reconstruction_loss_coef,\n",
    "    }\n",
    "    # rmt_cls = get_cls_by_name(args.model_cls)\n",
    "    rmt_cls = RMTDecoderLMHeadMultiSegSaveMem\n",
    "    # if hvd.rank() == 0:\n",
    "    #     logger.info(f'Wrapping in: {rmt_cls}')\n",
    "    \n",
    "    model = rmt_cls(model, **rmt_config)\n",
    "\n",
    "    ## turn on memory resetting on validation\n",
    "    model.rmt_config['reinit_mem_each_fwd'] = True\n",
    "\n",
    "    ## load cpt of rmt\n",
    "    if args.model_cpt:\n",
    "        model_cpt = os.path.join(args.model_cpt, \"model_best.pth\")\n",
    "        cpt = torch.load(model_cpt, map_location='cpu')\n",
    "        model.load_state_dict(cpt['model_state_dict'])\n",
    "        # if hvd.rank() == 0:\n",
    "        #     logger.info(f'Loaded RMT state dict from: {args.model_cpt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [k for k in model.state_dict().keys() if k not in cpt['model_state_dict'].keys()]\n",
    "# [k for k in cpt['model_state_dict'].keys() if k not in model.state_dict().keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jovyan/rmt/wip/notebooks/debug_rmt_arxiv_run.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B8a100-3/home/jovyan/rmt/wip/notebooks/debug_rmt_arxiv_run.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m1\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m0\u001b[39;49m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is in segments when memory is changed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "case 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Segment with different memory:  the dimensionless form $${\\boldsymbol{N}} = \\left[ \\begin{mat ... _1 &= \\left[ \\begin{matrix} 0            & -1 & \\\n",
      "\n",
      "\n",
      "### Segment n-1:  matrix and ${\\boldsymbol{\\eta}}$ is the Stroh vector, defined in. Note that the vector ${\\boldsymbol{\\eta}}$ is different from its counterpart in the ...  the charge-controlled case, the incremental electric boundary condition is in terms of the electric displacement $\\Delta$. In the present situation the Stroh matrix has\n",
      "\n",
      "\n",
      "### Segment n-2: $, $\\bar{c}=c/\\mu$ and $\\bar{d}=d/\\sqrt{\\mu\\varepsilon}$, and a prime denotes differentiation with respect ... mbol{N\\eta}}, \\label{stroh-form}$$ where ${\\boldsymbol{N}}$ is the Stroh\n",
      "\n",
      "\n",
      "### Segment n-3: \\eta_5'=\\mathrm{i}[(\\bar{c}-\\bar{a})\\eta_2-\\eta_4],\n",
      "\\notag \\\\\n",
      " ... 4],\\end{aligned}$$ where $\\bar{a}=a/\\mu$, $\\bar{b}=b/\\mu\n",
      "\n",
      "\n",
      "### Segment n-4:  \\\\\n",
      "&\\eta_2'=-\\mathrm{i}\\eta_1,\n",
      "\\notag \\\\\n",
      "&\\eta_3'=\\mathrm{i}(2 ... }^2)\\eta_1-\\eta_5+2\\bar{d}\\eta_6],\n",
      "\\notag \\\\\n",
      "&\n",
      "\n",
      "\n",
      "### Segment n-5: ],\\label{strohvector}$$ where ${\\boldsymbol{U}}$ is the ‚Äòdisplacement‚Äô vector and ${\\boldsymbol{S}}$ is ... {c}^{\\,-1}\\eta_3+\\bar{c}^{\\,-1}\\eta_4),\n",
      "\\notag\n",
      "\n",
      "\n",
      "### Segment n-6:  is the wavelength of the wrinkles.\n",
      "\n",
      "We now arrange the variables so that they all have the same dimensions by defining a Stroh vector ${\\boldsymbol{\\eta}}$ as $${\\ ... 21}/\\mu,\\Sigma_{22}/\\mu,\\Delta/\\sqrt{\\mu\\varepsilon}\n",
      "\n",
      "\n",
      "### Segment n-7: left[ U_1, U_2, \\Phi, \\mathrm{i}k\\Sigma_{21},\\mathrm{i}k\\Sigma_{22}, ...  $kx_2$, $k = 2\\pi / \\mathcal L$ is the wave number and $\\mathcal{L}$\n",
      "\n",
      "\n",
      "### Segment n-8: varphi$, $\\dot{T}_{021}$, $\\dot{T}_{022}$, $\\dot{D}_{L02}$ and consider increments that are sinusoidal in ... }_{L02} \\right\\rbrace = \\Re \\lbrace e^{\\mathrm{i}kx_1} \\\n"
     ]
    }
   ],
   "source": [
    "sample_idx = 1000\n",
    "segment_idx = 31\n",
    "\n",
    "\n",
    "batch = batches[sample_idx]\n",
    "segments = model.pad_and_segment(input_ids=batch.reshape(1, -1))\n",
    "\n",
    "n_seg = 8\n",
    "n = tokenizer.decode(segments[0][segment_idx][0][:20]) + ' ... ' + tokenizer.decode(segments[0][segment_idx][0][-30:])\n",
    "expl_str = \"### Segment with different memory: {}\".format(n)\n",
    "\n",
    "for i in range(1, n_seg + 1):\n",
    "    ni = tokenizer.decode(segments[0][segment_idx-i][0][:40]) + ' ... ' + tokenizer.decode(segments[0][segment_idx-i][0][-30:])\n",
    "    expl_str += \"\\n\\n\\n### Segment n-{}: {}\".format(i, ni)\n",
    "\n",
    "print(expl_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "case 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Segment with different memory:  c}\n",
      "qb_1 & qb_2 & b_3 & b_4 \\\\\n",
      "qb_5 & qb_6 & b_7 & b_8\n",
      " ... 7b_8}  = \\dfrac{\\tilde{g}-qb_1}{\\tilde{g}-b\n",
      "\n",
      "\n",
      "### Segment n-1: 6], which introduced the idea of the preservation of a connection matrix¬†[@Birkhoff]. The article was also responsible for introducing a discrete analogue of the sixth Painlev√© equation [@S ...  b_8\n",
      "\\end{array} : f,g \\right\\} \\to \\left\\{\\begin{array}{c c c\n",
      "\n",
      "\n",
      "### Segment n-2: ,\\lambda_1,\\lambda_2}$ being trivial, we also have that the transformation $T_{a_1,a_2,a_3,\\kappa_ ... \n",
      "-------------------------------------------------------------\n",
      "\n",
      "This particular case was originally the subject of the article by Jimbo and Sakai¬†[@Sakai:qP\n",
      "\n",
      "\n",
      "### Segment n-3: end{gathered}$$ under the relations $$\\begin{gathered}\n",
      "a_1  = a_2b_3,\\qquad\n",
      "a_2  = a_ ... \\kappa_2.\\end{gathered}$$ In addition to $T_{\\kappa_1,\\kappa_2\n",
      "\n",
      "\n",
      "### Segment n-4:  = \\dfrac{q \\left(y-a_1\\right) \\left(y-a_2\\right) \\kappa _1}{\\left(y-a ... {a_2}{f},\\qquad\n",
      "z  = -\\dfrac{b_2}{b_0}g,\\\n",
      "\n",
      "\n",
      "### Segment n-5: a_3 \\kappa _2 \\tilde{z}\\right) \\left(\\lambda _1 \\tilde{z}-q a_1 a_2 \\kappa _ ... 1,a_2,\\lambda_1,\\lambda_2} : \\ }{} \\\n",
      "\\tilde{z}z\n",
      "\n",
      "\n",
      "### Segment n-6: {gathered}$$ We may identify the evolution of $q$-$\\mathrm{P}_{\\rm V}$ with the Schlesinger transformation $$\\begin{gathered} ... tilde{z}),\\qquad\n",
      "\\tilde{y}y = \\dfrac{\\left(q \\lambda _1+\n",
      "\n",
      "\n",
      "### Segment n-7:  lattice of connection preserving deformations as $$\\begin{gathered}\n",
      "\\mathbb{Z}^6  \\cong \\langle T_{\\kappa_1,\\lambda ... a_2,\\lambda_1}, T_{a_3,\\lambda_1} \\rangle = G_A.\\end\n",
      "\n",
      "\n",
      "### Segment n-8: \\dfrac{q \\left(\\psi  a_1 \\kappa _1-1\\right) \\left(\\psi  a_2 \\kappa _1-1 ... {y}{a_1 a_2 \\kappa _1-z \\lambda _1},$$ for convenience. We may now identify the\n",
      "\n",
      "\n",
      "### Segment n-9: lambda\n",
      "   _1\\right)}{y z \\kappa _1},\\\\\n",
      "\\phantom{T_{\\kappa_1,\\lambda_1}  : \\ ... psi  \\kappa _1 \\lambda _1 \\tilde{z}},\\qquad\n",
      "\\tilde{z}=\n"
     ]
    }
   ],
   "source": [
    "sample_idx = 1200\n",
    "segment_idx = 31\n",
    "\n",
    "\n",
    "batch = batches[sample_idx]\n",
    "segments = model.pad_and_segment(input_ids=batch.reshape(1, -1))\n",
    "\n",
    "n_seg = 9\n",
    "n = tokenizer.decode(segments[0][segment_idx][0][:40]) + ' ... ' + tokenizer.decode(segments[0][segment_idx][0][-30:])\n",
    "expl_str = \"### Segment with different memory: {}\".format(n)\n",
    "\n",
    "for i in range(1, n_seg + 1):\n",
    "    ni = tokenizer.decode(segments[0][segment_idx-i][0][:40]) + ' ... ' + tokenizer.decode(segments[0][segment_idx-i][0][-30:])\n",
    "    expl_str += \"\\n\\n\\n### Segment n-{}: {}\".format(i, ni)\n",
    "\n",
    "print(expl_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "case 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Segment with different memory: (t,{\\widehat{{\\boldsymbol{x}}}}))\\psi( ... }}))\\psi(t)^{{d}/2}$, $i=1,\\dots, {d}$.\n",
      "\n",
      "\n",
      "\n",
      "### Segment n-1: hat{D}})}\n",
      "+\\dfrac{\\lambda}{2}\\int_0^T|{d_t}{{{\\boldsymbol \\alpha}}}|^2={\\math ... {\\widehat{{\\boldsymbol{x}}}}):={{\\boldsymbol P}}_i({\\mathbf{X}}\n",
      "\n",
      "\n",
      "### Segment n-2:  ${\\mathcal{J}}$ as $$\\label{eq:calJ_D}\n",
      "{\\mathcal{J}}({{{\\boldsymbol \\alpha}}})=\\df ... -{\\widehat{{\\mathrm{f}}}}_i\\|^2_{{\\mathrm{L}}^2({\\wide\n",
      "\n",
      "\n",
      "### Segment n-3: t,{\\mathbf{X}}(t,{\\widehat{{\\boldsymbol{x}}}}))\\psi(t)^{{d}/2}\\right)^ ... t,{\\widehat{{\\boldsymbol{x}}}}))=\\psi(t)^{d}$. Then, we rewrite\n"
     ]
    }
   ],
   "source": [
    "sample_idx = 1400\n",
    "segment_idx = 16\n",
    "\n",
    "\n",
    "batch = batches[sample_idx]\n",
    "segments = model.pad_and_segment(input_ids=batch.reshape(1, -1))\n",
    "\n",
    "n_seg = 3\n",
    "n = tokenizer.decode(segments[0][segment_idx][0][:20]) + ' ... ' + tokenizer.decode(segments[0][segment_idx][0][-30:])\n",
    "expl_str = \"### Segment with different memory: {}\".format(n)\n",
    "\n",
    "for i in range(1, n_seg + 1):\n",
    "    ni = tokenizer.decode(segments[0][segment_idx-i][0][:40]) + ' ... ' + tokenizer.decode(segments[0][segment_idx-i][0][-30:])\n",
    "    expl_str += \"\\n\\n\\n### Segment n-{}: {}\".format(i, ni)\n",
    "\n",
    "print(expl_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "case 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = 1200\n",
    "segment_idx = 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Segment n-2:  $G_E^n (Q^2) = [1.91\\tau/( ...  the matter distribution $G_0(Q)$ of the deuteron gives good agreement with the deuteron charge form factor defined by $\n",
      "\n",
      "\n",
      "### Segment n-1: G_{ch}(Q) = \n",
      "G_0(Q)[G_E^ ...  fm and is smaller than that of the free proton radius. However, an input of the dipole form of $G_E^p\n",
      "\n",
      "\n",
      "### Segment with different memory: $ reproduces the $ed$ data well as can also be found in [@arenhovel ... , $$\\label{potd} \n",
      "V_d(Q) = 4 \\pi e \\, {G_0(Q)\n"
     ]
    }
   ],
   "source": [
    "batch = batches[sample_idx]\n",
    "segments = model.pad_and_segment(input_ids=batch.reshape(1, -1))\n",
    "\n",
    "expl_str = \"### Segment n-2: {}\\n\\n\\n### Segment n-1: {}\\n\\n\\n### Segment with different memory: {}\"\n",
    "n2 = tokenizer.decode(segments[0][segment_idx-2][0][:20]) + ' ... ' + tokenizer.decode(segments[0][segment_idx-2][0][-30:])\n",
    "n1 = tokenizer.decode(segments[0][segment_idx-1][0][:20]) + ' ... ' + tokenizer.decode(segments[0][segment_idx-1][0][-30:])\n",
    "n = tokenizer.decode(segments[0][segment_idx][0][:20]) + ' ... ' + tokenizer.decode(segments[0][segment_idx][0][-30:])\n",
    "\n",
    "print(expl_str.format(n2, n1, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "case 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = 1000\n",
    "segment_idx = 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = batches[sample_idx]\n",
    "segments = model.pad_and_segment(input_ids=batch.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Segment n-2: ... int_{\\Sigma_t} \\left| \\vec{H} + \\frac{(x-y-(t_0-t)y')^\\perp}{2(t_0-t)} \\right|^2 \\Phi_{y,t_0\n",
      "\n",
      "\n",
      "### Segment n-1: ... _0} |y'(\\tau)|^2 {\\mathop{}\\!\\mathrm{d}}\\tau\\right) \\int_{\\Sigma_t} \\Phi_{y(t),t_0}$ is non-increasing for $t<\n",
      "\n",
      "\n",
      "### Segment with different memory: ...  The energy functional for curves in $\\mathbb{R}^n$ is of course minimised by straight lines; in this case the equality case is easy to interpret:\n",
      "\n",
      "Fix $x_0,y_0\\in\\mathbb{R}^n$ and $t_0\\in\\mathbb{R}$ and let $\\{\\Sigma_t\\\n"
     ]
    }
   ],
   "source": [
    "expl_str = \"### Segment n-2: ... {}\\n\\n\\n### Segment n-1: ... {}\\n\\n\\n### Segment with different memory: ... {}\"\n",
    "n2 = tokenizer.decode(segments[0][segment_idx-2][0][-60:])\n",
    "n1 = tokenizer.decode(segments[0][segment_idx-1][0][-60:])\n",
    "n = tokenizer.decode(segments[0][segment_idx][0][-80:])\n",
    "\n",
    "print(expl_str.format(n2, n1, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segment_idx = 31\n",
    "segment_idx = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Segment n-2: ... ^2\\right).$$\n",
      "\n",
      "We will now make use of the divergence theorem (\\[eq:mcfdivthm\\]). In particular, for fixed $t$ we set $$\\label{eq:mcfX} X = -\\frac{1}{2(t_0-t)}(x-y-2(t_0-t)y').\n",
      "\n",
      "\n",
      "### Segment n-1: ...  X)&=& \\Phi_{y,t_0}\\left( {\\operatorname{div}}_{\\Sigma_t} X -  \\frac{\\langle X, (x-y)^T\\rangle}{2(t_0-t)} \\right)\\\\&=&\\label{eq:mcfdiv} \\Phi\n",
      "\n",
      "\n",
      "### Segment with different memory: ... 2 - (t_0-t)^2|(y')^T|^2  }{4(t_0-t)^2}\\right),\\end{aligned}$$ where we have used that $x-y = (x-y-(t_0-t)y') + (t_0-t)y'$.\n",
      "\n",
      "Combining\n"
     ]
    }
   ],
   "source": [
    "expl_str = \"### Segment n-2: ... {}\\n\\n\\n### Segment n-1: ... {}\\n\\n\\n### Segment with different memory: ... {}\"\n",
    "n2 = tokenizer.decode(segments[0][segment_idx-2][0][-80:])\n",
    "n1 = tokenizer.decode(segments[0][segment_idx-1][0][-80:])\n",
    "n = tokenizer.decode(segments[0][segment_idx][0][-80:])\n",
    "\n",
    "print(expl_str.format(n2, n1, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Segment n-2: ... Q^2) Q^2/(8 m_p^2)]$ in [@gilmangross] (see Fig. 1). The proton radius corresponding to the dipole $G_E^p$ is 0.81 fm and is smaller than that of the free proton radius. However, an input of the dipole form of $G_E^p\n",
      "\n",
      "\n",
      "### Segment n-1: ... $ interaction potential in (\\[potqspace\\]) by noting that the deuteron electric potential should be associated with the Coulomb interaction with form factors but cannot depend on the mass of the probe, in this case the muon. Thus, $$\\label{potd} \n",
      "V_d(Q) = 4 \\pi e \\, {G_0(Q)\n",
      "\n",
      "\n",
      "### Segment with different memory: ... ^2\\over 8 m_N^2} \\biggr ) \\,,$$ where $e$ is the positive charge of the deuteron. Denoting, $G_0(Q)(G_E^p(Q^2) + G_E^n(Q^2)) (1 - Q^2/8m_N^2) = G_\n"
     ]
    }
   ],
   "source": [
    "segment_idx = 35\n",
    "\n",
    "expl_str = \"### Segment n-2: ... {}\\n\\n\\n### Segment n-1: ... {}\\n\\n\\n### Segment with different memory: ... {}\"\n",
    "n2 = tokenizer.decode(segments[0][segment_idx-2][0][-80:])\n",
    "n1 = tokenizer.decode(segments[0][segment_idx-1][0][-80:])\n",
    "n = tokenizer.decode(segments[0][segment_idx][0][-80:])\n",
    "\n",
    "print(expl_str.format(n2, n1, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "case 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = 0\n",
    "# segment_idx = 23\n",
    "segment_idx = 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Segment n-2: ... ah\\_omitting\\_2014,goldbring\\_enforceable\\_2017,farah\\_model\\_2017]{\n",
      "\n",
      "\n",
      "### Segment n-1: ...  focuses on the study of *classes* of objects, rather than single objects on their own. The important notion of *language* or *signature\n",
      "\n",
      "\n",
      "### Segment with different memory: ...  $L$ has attached a natural number $n_{f}$, called its *arity*, and a function $%\n",
      "\\varpi ^\n"
     ]
    }
   ],
   "source": [
    "expl_str = \"### Segment n-2: ... {}\\n\\n\\n### Segment n-1: ... {}\\n\\n\\n### Segment with different memory: ... {}\"\n",
    "n2 = tokenizer.decode(segments[0][segment_idx-2][0][-30:])\n",
    "n1 = tokenizer.decode(segments[0][segment_idx-1][0][-30:])\n",
    "n = tokenizer.decode(segments[0][segment_idx][0][-30:])\n",
    "\n",
    "print(expl_str.format(n2, n1, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = batches[sample_idx]\n",
    "# segments = model.pad_and_segment(input_ids=batch.reshape(1, -1))\n",
    "\n",
    "# print(\"segment with different memory\")\n",
    "# print('... ' + tokenizer.decode(segments[0][segment_idx][0][-50:]))\n",
    "\n",
    "# print()\n",
    "# print()\n",
    "\n",
    "# print(\"previous segment\")\n",
    "# print('... ' + tokenizer.decode(segments[0][segment_idx - 1][0][-50:]))\n",
    "\n",
    "# print()\n",
    "# print()\n",
    "\n",
    "# print(\"previous segment\")\n",
    "# print('... ' + tokenizer.decode(segments[0][segment_idx - 2][0][-50:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save train memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict = {}\n",
    "\n",
    "for i in range(0, 2000, 200):\n",
    "    batch = batches[i]\n",
    "    collated = collate_fn([list(batch)])\n",
    "    out = model(**collated)\n",
    "\n",
    "    res_dict[f'sample_{i}_memories'] = out['memories']\n",
    "\n",
    "for k in res_dict:\n",
    "    res_dict[k] = [t.detach().numpy() for t in res_dict[k]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/home/jovyan/rmt/data/memories/7-64seg_memories_train.npy'\n",
    "path = '/home/jovyan/rmt/data/memories/2-64seg_memories_train.npy'\n",
    "np.save(path, res_dict,  allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save valid memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict = {}\n",
    "\n",
    "for i in range(0, 2000, 200):\n",
    "    batch = valid_batches[i]\n",
    "    collated = collate_fn([list(batch)])\n",
    "    out = model(**collated)\n",
    "\n",
    "    res_dict[f'sample_{i}_memories'] = out['memories']\n",
    "\n",
    "for k in res_dict:\n",
    "    res_dict[k] = [t.detach().numpy() for t in res_dict[k]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/home/jovyan/rmt/data/memories/7-64seg_memories_valid.npy'\n",
    "path = '/home/jovyan/rmt/data/memories/2-64seg_memories_valid.npy'\n",
    "np.save(path, res_dict,  allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7-64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save train memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict = {}\n",
    "\n",
    "for i in range(0, 2000, 200):\n",
    "    batch = batches[i]\n",
    "    collated = collate_fn([list(batch)])\n",
    "    out = model(**collated)\n",
    "\n",
    "    res_dict[f'sample_{i}_memories'] = out['memories']\n",
    "\n",
    "for k in res_dict:\n",
    "    res_dict[k] = [t.detach().numpy() for t in res_dict[k]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/jovyan/rmt/data/memories/7-64seg_memories_train.npy'\n",
    "# path = '/home/jovyan/rmt/data/memories/2-64seg_memories_train.npy'\n",
    "np.save(path, res_dict,  allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save valid memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict = {}\n",
    "\n",
    "for i in range(0, 2000, 200):\n",
    "    batch = valid_batches[i]\n",
    "    collated = collate_fn([list(batch)])\n",
    "    out = model(**collated)\n",
    "\n",
    "    res_dict[f'sample_{i}_memories'] = out['memories']\n",
    "\n",
    "for k in res_dict:\n",
    "    res_dict[k] = [t.detach().numpy() for t in res_dict[k]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/jovyan/rmt/data/memories/7-64seg_memories_valid.npy'\n",
    "# path = '/home/jovyan/rmt/data/memories/2-64seg_memories_valid.npy'\n",
    "np.save(path, res_dict,  allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[6329,  198,  397,  ...,    3,  857,   13]]),\n",
       " 'labels': tensor([[6329,  198,  397,  ...,    3,  857,   13]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]]),\n",
       " 'labels_mask': tensor([[False, False, False,  ...,  True,  True,  True]])}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "collate_fn([list(batches[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dicta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9173, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out.memories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.memories[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "620981"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define optimizer\n",
    "optimizer_cls = get_optimizer(args.optimizer)\n",
    "if optimizer_cls is None:\n",
    "    raise RuntimeError(f'{args.optimizer} was not found in optimizers, torch.optim, transformers.optimization')\n",
    "\n",
    "# if hvd.rank() == 0:\n",
    "#     logger.info(f'Using optimizer class: {optimizer_cls}')\n",
    "\n",
    "# todo: group optimizer params\n",
    "if optimizer_cls in [transformers.optimization.Adafactor, optimizers.Adafactor]:\n",
    "    # https://github.com/huggingface/transformers/pull/9751/files -> transformers 4.3.0\n",
    "    optimizer = optimizer_cls(model.parameters(), lr=args.lr,\n",
    "                                scale_parameter=args.scale_parameter,\n",
    "                                relative_step=args.relative_step,\n",
    "                                warmup_init=args.warmup_init,\n",
    "                                weight_decay=args.weight_decay)\n",
    "else:\n",
    "    optimizer = optimizer_cls(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "def keep_for_metrics_fn(batch, output):\n",
    "    data = {}\n",
    "    if 'generation_outputs' in output:\n",
    "            # data['labels'] = batch['answer']\n",
    "        data['generation_outputs'] = output['generation_outputs']\n",
    "        \n",
    "    data['labels'] = batch['labels']\n",
    "    for key in batch.keys():\n",
    "        if 'loss' in key: \n",
    "            data[key] = batch[key]\n",
    "    data['predictions'] = torch.argmax(output['logits'].detach(), dim=-1)\n",
    "    if 'labels_mask' in batch:\n",
    "        data['predictions'] = [data['predictions'][i, mask] for i, mask in enumerate(batch['labels_mask'])]\n",
    "    return data\n",
    "\n",
    "# HF datasets can compute metrics on each gpu process and then aggregate them on process with rank 0\n",
    "# synchronization is done by using temporay files on a shared filesystem\n",
    "# rank and number of workers is set by num_process and process_id params\n",
    "# BUT our Trainer aggregates all prediction from all gpus!\n",
    "#   this will lead to computing metrics for predictions repeated xN_GPUS times\n",
    "# need to try:\n",
    "# - keep_in_memory=True, may lead to OOM for large validation sets, after sync predictions and targets for the full\n",
    "#       validation set would be stored on each GPU -> xN_GPUs RAM\n",
    "#   - implemented currently\n",
    "# - compute metrics on batch lvl\n",
    "# - add support of HF metrics and turn off aggregation in case if metric has .add_batch method\n",
    "# scrolls_metric = datasets.load_metric(scrolls_metric_path, args.task_name, keep_in_memory=True)\n",
    "\n",
    "def metrics_fn(data):\n",
    "    metrics = {}\n",
    "    y, p = None, None\n",
    "    if 'generation_outputs' in data:\n",
    "        # replace -100 with pad token in labels\n",
    "        y = data['labels']\n",
    "        p = tokenizer.batch_decode(data['generation_outputs'], skip_special_tokens=True)\n",
    "        \n",
    "        if hvd.rank() == 0 and args.show_valid_examples > 0:\n",
    "            for i in range(min(args.show_valid_examples, len(y))):\n",
    "                logger.info(f'y: {y[i]}')\n",
    "                logger.info(f'p: {p[i]}')\n",
    "                logger.info(f'p ids: {data[\"generation_outputs\"][i]}')\n",
    "                logger.info('-' * 50)\n",
    "\n",
    "    if y is not None and p is not None:\n",
    "        metrics['exact_match'] = accuracy_score(y, p) * 100\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': \"Receiving Party shall not solicit some of Disclosing Party's representatives. MUTUAL NON-DISCLOSURE AGREEMENT\\nThis Non-Disclosure Agreement (‚ÄúAgreement‚Äù) is made on ___ day of ___ 20__, (‚Äúeffective date‚Äù) by and between _____________________, a _____________ Corporation, (‚ÄúXXXXX‚Äù) and Data Boiler Technologies, LLC., a Massachusetts corporation (‚ÄúDBT‚Äù).\\nWHEREAS, DBT and XXX (the ‚ÄúParties‚Äù) desire to engage in business related discussions and negotiations regarding a potential business relationship (‚ÄúTransaction‚Äù).\\nWHEREAS, the Parties may provide to each other certain confidential and proprietary information in connection with the Transaction and each desires that any such information provided shall be kept confidential by the other party; and\\nWHEREAS, in consideration of the disclosure of such information, each party is willing to keep the other party‚Äôs information confidential in accordance with the terms and conditions set forth in this Agreement;\\nNOW, THEREFORE, DBT and XXX hereby agree as follows:\\n1. Confidential Information ‚ÄúConfidential Information‚Äù means nonpublic information that disclosing party (‚ÄúDisclosing Party‚Äù) designates as being confidential or which, under the circumstances surrounding disclosure the receiving party (‚ÄúReceiving Party‚Äù) should know is treated as confidential by the Disclosing Party. Confidential Information includes, without limitation, non-public information relating to released or unreleased Disclosing Party software products, the marketing or promotion of any Disclosing Party product, Disclosing Party‚Äôs business policies or practices, financial information, technical information, computer systems, infrastructure designs, data, analysis, compilations, studies or other documentation and information received from others that Disclosing Party is obligated to treat as confidential. Confidential Information disclosed to Receiving Party by any Disclosing Party, its related entities and/or agents is covered by this Agreement. Confidential Information shall not include any information that: (i) is or subsequently becomes publicly available without Receiving Party‚Äôs breach of any obligation owed to Disclosing Party; (ii) became known to Receiving Party prior to Disclosing Party‚Äôs disclosure of such information to Receiving Party; (iii) became known to Receiving Party from a source other than Disclosing Party other than by the breach of an obligation of confidentiality owed to Disclosing Party; (iv) is independently developed by Receiving Party without access to the Disclosing Party‚Äôs information, or (v) the Confidential Information is required to be disclosed pursuant to a requirement of a governmental agency or law so long as the other party is provided notice of such requirement prior to any such disclosure.\\n2. Obligations Each party agrees that it shall not make use of, disseminate, or in any way disclose any Confidential Information of the Disclosing Party to any person, firm, or business, except to the extent necessary for the Transaction. The existence of any business negotiations, discussions, consultations, or agreements in progress between the parties shall not be released to any form of public media, unless agreed between the parties in writing. Both parties acknowledge that the Receiving Party shall protect the secrecy of all Confidential Information, that said Confidential Information is of critical importance to the Disclosing Party, that any violation of this Agreement would seriously and irreparably impair and damage the Disclosing Party's business, and that the Recipient shall keep all Confidential Information in a fiduciary capacity for the sole benefit of the Disclosing Party. The Receiving Party agrees that it shall treat all Confidential Information of the Disclosing Party with the same degree of care as it accords to its own Confidential Information, and the Receiving Party represents that it exercises reasonable care to protect its own Confidential Information. The Receiving Party agrees disclose Confidential Information only to those employees who need to know such information and certifies that such employees have previously agreed, either as a condition to employment or in order to obtain the Confidential Information, to be bound by terms and conditions substantially similar to those of this Agreement. The Receiving Party will immediately and unconditionally give written notice to the Disclosing Party of any unauthorized use or disclosure of the Confidential Information. The Receiving Party agrees to assist the Disclosing Party in remedying any such unauthorized use or disclosure of the Confidential Information.\\nMUTUAL NON-DISCLOSURE AGREEMENT\\n3. Return of Information Upon the request of the Disclosing Party, Receiving Party shall return all originals, copies, reproductions and summaries of Confidential Information at Disclosing Party‚Äôs request, or at Disclosing Party‚Äôs option, certify destruction of the same.\\n4. Injunctive Relief Receiving Party acknowledges that monetary damages may not be a sufficient remedy for unauthorized disclosure of Confidential Information and that Disclosing Party shall be entitled, without waiving any other rights or remedies, to such injunctive and other equitable relief (without bond and without the necessity of showing actual monetary damages) as may be deemed proper by a court.\\n5. No Further Rights All Confidential Information is and shall remain the property of Disclosing Party. Nothing contained in this Agreement shall be construed to as granting or conferring any rights in the Confidential Information except as provided herein.\\n6. No Commitment. The parties expressly agree that the provision of Information hereunder and discussions held in connection with the Transaction shall not prevent either party from pursuing similar discussions with third parties or obligate either party to continue discussions with the other or to take, continue or forego any action relating to the Transaction. Any estimates or forecasts provided by either party to the other shall not constitute commitments.\\n7. Miscellaneous\\nI. This Agreement constitutes the entire agreement between the parties with respect to the subject matter hereof. It shall not be modified except by a written agreement dated subsequent to the date of this Agreement and signed by both parties. None of the provisions of this Agreement shall be deemed to have been waived by any act or acquiescence on the part of Disclosing Party, its agents, or employees, but only by an instrument in writing signed by an authorized officer of Disclosing Party. No waiver of any provision of this Agreement shall constitute a waiver of any other provision(s) or of the same provision on another occasion.\\nII. This Agreement shall be construed and controlled by the laws of the State of Massachusetts and both parties further consent to jurisdiction by the state and federal courts sitting in Boston, Massachusetts.\\nIII. Subject to the limitations set forth in this Agreement, this Agreement will inure to the benefit of and be binding upon the parties, their successors and assigns. Neither party may assign, delegate or otherwise transfer this Agreement or any of its rights or obligations hereunder without the other party's prior approval.\\nData Boiler Technologies, LLC. XXXXX\\nSigned: _________________________ Signed: _________________________\\nName: _________________________ Name: _________________________\\nTitle: __________________________ Title: __________________________\\nDate: __________________________ Date: __________________________\\n\",\n",
       " 'text_id': 88,\n",
       " 'answer': 'NotMentioned',\n",
       " 'hyp_id': 18,\n",
       " 'supp_spans': [[0, 78]]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen = iter(train_dataloader)\n",
    "# batch = next(gen)\n",
    "\n",
    "batch_train = collate_train([train_dataset[i] for i in range(4)])\n",
    "batch_valid = collate_valid([valid_dataset[i] for i in range(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen = iter(valid_dataloader)\n",
    "# batch = next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 3041,   344,  1412,  ...,  1463,   276, 50256],\n",
       "         [18546, 35599,  6188,  ...,  6335,  2867, 50256],\n",
       "         [ 3041,   344,  1412,  ...,   603,   434, 50256],\n",
       "         [ 3041,   344,  1412,  ...,   603,   434, 50256]]),\n",
       " 'labels': tensor([[ 3041,   344,  1412,  ...,  1463,   276, 50256],\n",
       "         [18546, 35599,  6188,  ...,  6335,  2867, 50256],\n",
       "         [ 3041,   344,  1412,  ...,   603,   434, 50256],\n",
       "         [ 3041,   344,  1412,  ...,   603,   434, 50256]]),\n",
       " 'labels_mask': tensor([[False, False, False,  ...,  True,  True, False],\n",
       "         [False, False, False,  ...,  True,  True, False],\n",
       "         [False, False, False,  ...,  True,  True, False],\n",
       "         [False, False, False,  ...,  True,  True, False]]),\n",
       " 'attention_mask': tensor([[ True,  True,  True,  ...,  True,  True, False],\n",
       "         [ True,  True,  True,  ...,  True,  True, False],\n",
       "         [ True,  True,  True,  ...,  True,  True, False],\n",
       "         [ True,  True,  True,  ...,  True,  True, False]])}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Receiving Party shall not solicit some of Disclosing Party's representatives. MUTUAL NON-DISCLOSURE AGREEMENT\\nThis Non-Disclosure Agreement (‚ÄúAgreement‚Äù) is made on ___ day of ___ 20__, (‚Äúeffective date‚Äù) by and between _____________________, a _____________ Corporation, (‚ÄúXXXXX‚Äù) and Data Boiler Technologies, LLC., a Massachusetts corporation (‚ÄúDBT‚Äù).\\nWHEREAS, DBT and XXX (the ‚ÄúParties‚Äù) desire to engage in business related discussions and negotiations regarding a potential business relationship (‚ÄúTransaction‚Äù).\\nWHEREAS, the Parties may provide to each other certain confidential and proprietary information in connection with the Transaction and each desires that any such information provided shall be kept confidential by the other party; and\\nWHEREAS, in consideration of the disclosure of such information, each party is willing to keep the other party‚Äôs information confidential in accordance with the terms and conditions set forth in this Agreement;\\nNOW, THEREFORE, DBT and XXX hereby agree as follows:\\n1. Confidential Information ‚ÄúConfidential Information‚Äù means nonpublic information that disclosing party (‚ÄúDisclosing Party‚Äù) designates as being confidential or which, under the circumstances surrounding disclosure the receiving party (‚ÄúReceiving Party‚Äù) should know is treated as confidential by the Disclosing Party. Confidential Information includes, without limitation, non-public information relating to released or unreleased Disclosing Party software products, the marketing or promotion of any Disclosing Party product, Disclosing Party‚Äôs business policies or practices, financial information, technical information, computer systems, infrastructure designs, data, analysis, compilations, studies or other documentation and information received from others that Disclosing Party is obligated to treat as confidential. Confidential Information disclosed to Receiving Party by any Disclosing Party, its related entities and/or agents is covered by this Agreement. Confidential Information shall not include any information that: (i) is or subsequently becomes publicly available without Receiving Party‚Äôs breach of any obligation owed to Disclosing Party; (ii) became known to Receiving Party prior to Disclosing Party‚Äôs disclosure of such information to Receiving Party; (iii) became known to Receiving Party from a source other than Disclosing Party other than by the[GEN]NotMentioned<|endoftext|>\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(batch_train['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[GEN]NotMentioned'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(batch_train['input_ids'][0][batch_train['labels_mask'][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 3041,   344,  1412,  ...,   287,   393, 50257],\n",
       "         [18546, 35599,  6188,  ...,   286, 50257, 50256],\n",
       "         [ 4366, 13675,   286,  ..., 33147,  1626, 50257],\n",
       "         [ 3041,   344,  1412,  ...,   290, 50257, 50256]]),\n",
       " 'labels': tensor([[14539,   603,   434, 50256, 50256],\n",
       "         [ 3673,    44,  1463,   276, 50256],\n",
       "         [14539,   603,   434, 50256, 50256],\n",
       "         [ 3673,    44,  1463,   276, 50256]]),\n",
       " 'attention_mask': tensor([[ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True,  True, False],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True,  True, False]])}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Receiving Party may share some Confidential Information with some of Receiving Party's employees. Attachment 8\\nNon-Disclosure Agreement for Request for Proposal ‚Äì California Courts Protective Order Registry Development\\nTHIS CONFIDENTIALITY AGREEMENT (‚ÄúAgreement‚Äù), effective as of _________________[date of company‚Äôs signed proposal] (‚ÄúEffective Date‚Äù), is entered into by and between the Administrative Office of the Courts (the ‚ÄúAOC‚Äù) and [company name], a [ ] corporation, with its principal address at [complete address], (hereinafter the ‚ÄúCompany‚Äù).\\nWHEREAS, the State of California, acting through the Judicial Council of California, Administrative Office of the Courts is planning to disclose certain confidential information to potential vendors who wish to bid on the Request for Proposal for Judicial Council of California Administrative Office of the Courts (AOC) for California Courts Protective Order Registry Development, including all Statements of Work, dated on or about April, 2009 (collectively, the ‚ÄúRFP‚Äù);\\nWHEREAS, each potential vendor who wishes to submit a response to the RFP must have access to that confidential information in order to construct a response to the RFP, including a bid; and\\nWHEREAS, the AOC requires each potential vendor identified to receive the RFP to sign this Confidentiality Agreement and return it to the AOC as a condition of receiving the RFP;\\nNOW THEREFORE, in consideration of the promises and of the mutual promises and agreements herein contained, it is agreed by and between the parties hereto as follows:\\n1. Company agrees that all information, documents, data, materials and the AOC proprietary software systems disclosed to, or accessed by, Company in connection with, or related to, the RFP or the RFP process, in any form whether oral or written, or in any medium, including but not limited to any of the following is ‚ÄúConfidential Information‚Äù under the terms of this Agreement:\\na. The subject matter described in, and referred to, in the RFP, its associated Statements of Work (‚ÄúSOWs‚Äù) or any other attachments, or during the RFP process (collectively ‚ÄúAOC Requirements‚Äù);\\nb. The RFP document (including all attachments), its content, and all supporting data, materials and all other information provided to Company in any form or medium in or[GEN]\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(batch_valid['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Entailment<|endoftext|><|endoftext|>'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(batch_valid['labels'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = batch.pop('labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "generated = model.memory_cell.model.generate(**batch, max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Receiving Party may share some Confidential Information with some of Receiving Party's employees. Attachment 8\\nNon-Disclosure Agreement for Request for Proposal ‚Äì California Courts Protective Order Registry Development\\nTHIS CONFIDENTIALITY AGREEMENT (‚ÄúAgreement‚Äù), effective as of _________________[date of company‚Äôs signed proposal] (‚ÄúEffective Date‚Äù), is entered into by and between the Administrative Office of the Courts (the ‚ÄúAOC‚Äù) and [company name], a [ ] corporation, with its principal address at [complete address], (hereinafter the ‚ÄúCompany‚Äù).\\nWHEREAS, the State of California, acting through the Judicial Council of California, Administrative Office of the Courts is planning to disclose certain confidential information to potential vendors who wish to bid on the Request for Proposal for Judicial Council of California Administrative Office of the Courts (AOC) for California Courts Protective Order Registry Development, including all Statements of Work, dated on or about April, 2009 (collectively, the ‚ÄúRFP‚Äù);\\nWHEREAS, each potential vendor who wishes to submit a response to the RFP must have access to that confidential information in order to construct a response to the RFP, including a bid; and\\nWHEREAS, the AOC requires each potential vendor identified to receive the RFP to sign this Confidentiality Agreement and return it to the AOC as a condition of receiving the RFP;\\nNOW THEREFORE, in consideration of the promises and of the mutual promises and agreements herein contained, it is agreed by and between the parties hereto as follows:\\n1. Company agrees that all information, documents, data, materials and the AOC proprietary software systems disclosed to, or accessed by, Company in connection with, or related to, the RFP or the RFP process, in any form whether oral or written, or in any medium, including but not limited to any of the following is ‚ÄúConfidential Information‚Äù under the terms of this Agreement:\\na. The subject matter described in, and referred to, in the RFP, its associated Statements of Work (‚ÄúSOWs‚Äù) or any other attachments, or during the RFP process (collectively ‚ÄúAOC Requirements‚Äù);\\nb. The RFP document (including all attachments), its content, and all supporting data, materials and all other information provided to Company in any form or medium in or[GEN]NotMentioned<|endoftext|><|endoftext|>\",\n",
       " 'Confidential Information may include verbally conveyed information. CONFIDENTIALITY (NON-DISCLOSURE) AGREEMENT\\nThis CONFIDENTIALITY (NON-DISCLOSURE) AGREEMENT (‚ÄúAgreement‚Äù) is made and entered into this ______ day of _____________, 201__, by and between BROOKS‚Äô BOTTLING COMPANY, LLC, a New York Limited Liability Company (‚ÄúBROOKS‚Äù) and [ ] (‚ÄúCLIENT‚Äù) (BROOKS and CLIENT are sometimes collectively referred to as the ‚ÄúParties‚Äù)\\nRECITALS\\nA. BROOKS is engaged in the business of bottling products for various CLIENTS and has entered into discussion with relating to establishing a business relationship in connection with preparing and bottling certain of CLIENT‚Äôs product(s). As such, BROOKS will need access to certain personal, financial and product information relating to CLIENT and its products.\\nB. CLIENT desires to maintain the confidentiality of all personal, financial and product information disclosed to BROOKS. BROOKS is willing to receive all such personal, financial and product information in confidence, and the Parties deem it to be in their mutual best interest to protect such personal, financial and product information as provided in this Agreement.\\nNOW, THEREFORE, in consideration of the mutual promises, agreements, covenants, conditions and undertakings herein contained, and for other good and valuable consideration, the receipt and sufficiency of which are hereby acknowledged, the Parties agree as follows:\\n1. Confidential Information\\n1.1 With the understanding that CLIENT‚Äôs Formulations and Processing Procedures are proprietary to CLIENT, to the extent that CLIENT shall disclose to BROOKS personal, financial and product information, including their Formulations and Processing Procedures (‚ÄúConfidential Information‚Äù), then all the Confidential Information disclosed to BROOKS shall be received by BROOKS in confidence for purposes of this Agreement except as otherwise provided under Section 3 below.\\n1.2 BROOKS, its directors, employees, agents and representatives shall not disclose, disseminate, publish, communicate or divulge any Confidential Information to anyone outside BROOKS, or to any employee of BROOKS not having reasonable need for access to such information, unless CLIENT expressly consents to such disclosure in writing.\\n2. Representations and Warranties\\n2.1 Each of[GEN]<|endoftext|>[GEN]NotMentioned<|endoftext|>']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Entailment<|endoftext|><|endoftext|>', 'NotMentioned<|endoftext|>']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = model\n",
    "input_ids = batch['input_ids']\n",
    "attention_mask = batch['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleAttributeError",
     "evalue": "'MemoryCell' object has no attribute 'generate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb#Y241sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m# self.manage_gradients(memory_state, seg_num)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb#Y241sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m final_segment \u001b[39m=\u001b[39m segmented[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb#Y241sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmemory_cell\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfinal_segment, memory_state\u001b[39m=\u001b[39mmemory_state)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/nn/modules/module.py:778\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m    777\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m--> 778\u001b[0m \u001b[39mraise\u001b[39;00m ModuleAttributeError(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    779\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mModuleAttributeError\u001b[0m: 'MemoryCell' object has no attribute 'generate'"
     ]
    }
   ],
   "source": [
    "memory_state = None\n",
    "segmented = self.segment(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "cell_outputs = []\n",
    "for seg_num, segment in enumerate(segmented[:-1]):\n",
    "    cell_out, memory_state = self.memory_cell(**segment, memory_state=memory_state, output_hidden_states=True)\n",
    "    cell_outputs.append(cell_out)\n",
    "    # self.manage_gradients(memory_state, seg_num)\n",
    "\n",
    "final_segment = segmented[-1]\n",
    "out = self.memory_cell.generate(**final_segment, memory_state=memory_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = dict(**final_segment)\n",
    "input_ids = kwargs.pop('input_ids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "If inputs_embeds is passed as model-specific keyword input then model has to be an encoder-decoder and not a GPT2LMHeadModel.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb#Y246sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m seg_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory_cell\u001b[39m.\u001b[39mprocess_input(input_ids, memory_state, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb#Y246sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmemory_cell\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mseg_kwargs)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:26\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     25\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m():\n\u001b[0;32m---> 26\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/transformers/generation_utils.py:1083\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1074\u001b[0m return_dict_in_generate \u001b[39m=\u001b[39m (\n\u001b[1;32m   1075\u001b[0m     return_dict_in_generate \u001b[39mif\u001b[39;00m return_dict_in_generate \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mreturn_dict_in_generate\n\u001b[1;32m   1076\u001b[0m )\n\u001b[1;32m   1078\u001b[0m \u001b[39m# 2. Define model inputs\u001b[39;00m\n\u001b[1;32m   1079\u001b[0m \u001b[39m# inputs_tensor has to be defined\u001b[39;00m\n\u001b[1;32m   1080\u001b[0m \u001b[39m# model_input_name is defined if model-specific keyword input is passed\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m \u001b[39m# otherwise model_input_name is None\u001b[39;00m\n\u001b[1;32m   1082\u001b[0m \u001b[39m# all model-specific keyword inputs are removed from `model_kwargs`\u001b[39;00m\n\u001b[0;32m-> 1083\u001b[0m inputs_tensor, model_input_name, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_model_inputs(inputs, bos_token_id, model_kwargs)\n\u001b[1;32m   1084\u001b[0m batch_size \u001b[39m=\u001b[39m inputs_tensor\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1086\u001b[0m \u001b[39m# 3. Define other model kwargs\u001b[39;00m\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/transformers/generation_utils.py:423\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_model_inputs\u001b[0;34m(self, inputs, bos_token_id, model_kwargs)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[39m# 4. Only encoder-decoder models can have non `input_ids` input format\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder \u001b[39mand\u001b[39;00m input_name \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 423\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    424\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIf \u001b[39m\u001b[39m{\u001b[39;00minput_name\u001b[39m}\u001b[39;00m\u001b[39m is passed as model-specific keyword \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    425\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minput then model has to be an encoder-decoder and not a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    426\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    427\u001b[0m     )\n\u001b[1;32m    429\u001b[0m \u001b[39m# 5. if `inputs` is still None, try to create `input_ids` from BOS token\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[39mif\u001b[39;00m inputs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: If inputs_embeds is passed as model-specific keyword input then model has to be an encoder-decoder and not a GPT2LMHeadModel."
     ]
    }
   ],
   "source": [
    "seg_kwargs = self.memory_cell.process_input(input_ids, memory_state, **kwargs)\n",
    "out = self.memory_cell.model.generate(**seg_kwargs)\n",
    "# out, new_memory_state = self.memory_cell.process_output(out, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 768])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_kwargs['inputs_embeds'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 768]' is invalid for input of size 32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb Cell 27\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb#Y245sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mmemory_cell\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(seg_kwargs[\u001b[39m'\u001b[39;49m\u001b[39minputs_embeds\u001b[39;49m\u001b[39m'\u001b[39;49m], max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:26\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     25\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m():\n\u001b[0;32m---> 26\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/transformers/generation_utils.py:1190\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1185\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1186\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnum_return_sequences has to be 1, but is \u001b[39m\u001b[39m{\u001b[39;00mnum_return_sequences\u001b[39m}\u001b[39;00m\u001b[39m when doing greedy search.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1187\u001b[0m         )\n\u001b[1;32m   1189\u001b[0m     \u001b[39m# 10. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgreedy_search(\n\u001b[1;32m   1191\u001b[0m         input_ids,\n\u001b[1;32m   1192\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1193\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1194\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mpad_token_id,\n\u001b[1;32m   1195\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49meos_token_id,\n\u001b[1;32m   1196\u001b[0m         output_scores\u001b[39m=\u001b[39;49moutput_scores,\n\u001b[1;32m   1197\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1198\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1199\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1200\u001b[0m     )\n\u001b[1;32m   1202\u001b[0m \u001b[39melif\u001b[39;00m is_sample_gen_mode:\n\u001b[1;32m   1203\u001b[0m     \u001b[39m# 10. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1204\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(\n\u001b[1;32m   1205\u001b[0m         top_k\u001b[39m=\u001b[39mtop_k, top_p\u001b[39m=\u001b[39mtop_p, typical_p\u001b[39m=\u001b[39mtypical_p, temperature\u001b[39m=\u001b[39mtemperature, num_beams\u001b[39m=\u001b[39mnum_beams\n\u001b[1;32m   1206\u001b[0m     )\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/transformers/generation_utils.py:1530\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1527\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 1530\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   1531\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   1532\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1533\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1534\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1535\u001b[0m )\n\u001b[1;32m   1537\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   1538\u001b[0m     cur_len \u001b[39m=\u001b[39m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:1047\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1044\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1045\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1047\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m   1048\u001b[0m     input_ids,\n\u001b[1;32m   1049\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1050\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1051\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1052\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1053\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1054\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1055\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1056\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   1057\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1058\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1059\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1060\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1061\u001b[0m )\n\u001b[1;32m   1062\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1064\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:784\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    782\u001b[0m     token_type_ids \u001b[39m=\u001b[39m token_type_ids\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, input_shape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m    783\u001b[0m \u001b[39mif\u001b[39;00m position_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 784\u001b[0m     position_ids \u001b[39m=\u001b[39m position_ids\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, input_shape[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m])\n\u001b[1;32m    786\u001b[0m \u001b[39mif\u001b[39;00m past_key_values \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    787\u001b[0m     past_length \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 768]' is invalid for input of size 32"
     ]
    }
   ],
   "source": [
    "model.memory_cell.model.generate(seg_kwargs['inputs_embeds'], max_new_tokens=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 5)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cell_outputs), len(segmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 508])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Receiving Party may share some Confidential Information with some of Receiving Party's employees. Attachment 8\\nNon-Disclosure Agreement for Request for Proposal ‚Äì California Courts Protective Order Registry Development\\nTHIS CONFIDENTIALITY AGREEMENT (‚ÄúAgreement‚Äù), effective as of _________________[date of company‚Äôs signed proposal] (‚ÄúEffective Date‚Äù), is entered into by and between the Administrative Office of the Courts (the ‚ÄúAOC‚Äù) and [company name], a [ ] corporation, with its principal address at [complete address], (hereinafter the ‚ÄúCompany‚Äù).\\nWHEREAS, the State of California, acting through the Judicial Council of California, Administrative Office of the Courts is planning to disclose certain confidential information to potential vendors who wish to bid on the Request for Proposal for Judicial Council of California Administrative Office of the Courts (AOC) for California Courts Protective Order Registry Development, including all Statements of Work, dated on or about April, 2009 (collectively, the ‚ÄúRFP‚Äù);\\nWHEREAS, each potential vendor who wishes to submit a response to the RFP must have access to that confidential information in order to construct a response to the RFP, including a bid; and\\nWHEREAS, the AOC requires each potential vendor identified to receive the RFP to sign this Confidentiality Agreement and return it to the AOC as a condition of receiving the RFP;\\nNOW THEREFORE, in consideration of the promises and of the mutual promises and agreements herein contained, it is agreed by and between the parties hereto as follows:\\n1. Company agrees that all information, documents, data, materials and the AOC proprietary software systems disclosed to, or accessed by, Company in connection with, or related to, the RFP or the RFP process, in any form whether oral or written, or in any medium, including but not limited to any of the following is ‚ÄúConfidential Information‚Äù under the terms of this Agreement:\\na. The subject matter described in, and referred to, in the RFP, its associated Statements of Work (‚ÄúSOWs‚Äù) or any other attachments, or during the RFP process (collectively ‚ÄúAOC Requirements‚Äù);\\nb. The RFP document (including all attachments), its content, and all supporting data, materials and all other information provided to Company in any form or medium in or[GEN]\",\n",
       " 'Confidential Information may include verbally conveyed information. CONFIDENTIALITY (NON-DISCLOSURE) AGREEMENT\\nThis CONFIDENTIALITY (NON-DISCLOSURE) AGREEMENT (‚ÄúAgreement‚Äù) is made and entered into this ______ day of _____________, 201__, by and between BROOKS‚Äô BOTTLING COMPANY, LLC, a New York Limited Liability Company (‚ÄúBROOKS‚Äù) and [ ] (‚ÄúCLIENT‚Äù) (BROOKS and CLIENT are sometimes collectively referred to as the ‚ÄúParties‚Äù)\\nRECITALS\\nA. BROOKS is engaged in the business of bottling products for various CLIENTS and has entered into discussion with relating to establishing a business relationship in connection with preparing and bottling certain of CLIENT‚Äôs product(s). As such, BROOKS will need access to certain personal, financial and product information relating to CLIENT and its products.\\nB. CLIENT desires to maintain the confidentiality of all personal, financial and product information disclosed to BROOKS. BROOKS is willing to receive all such personal, financial and product information in confidence, and the Parties deem it to be in their mutual best interest to protect such personal, financial and product information as provided in this Agreement.\\nNOW, THEREFORE, in consideration of the mutual promises, agreements, covenants, conditions and undertakings herein contained, and for other good and valuable consideration, the receipt and sufficiency of which are hereby acknowledged, the Parties agree as follows:\\n1. Confidential Information\\n1.1 With the understanding that CLIENT‚Äôs Formulations and Processing Procedures are proprietary to CLIENT, to the extent that CLIENT shall disclose to BROOKS personal, financial and product information, including their Formulations and Processing Procedures (‚ÄúConfidential Information‚Äù), then all the Confidential Information disclosed to BROOKS shall be received by BROOKS in confidence for purposes of this Agreement except as otherwise provided under Section 3 below.\\n1.2 BROOKS, its directors, employees, agents and representatives shall not disclose, disseminate, publish, communicate or divulge any Confidential Information to anyone outside BROOKS, or to any employee of BROOKS not having reasonable need for access to such information, unless CLIENT expressly consents to such disclosure in writing.\\n2. Representations and Warranties\\n2.1 Each of[GEN]<|endoftext|>']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(batch['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  ...,  True,  True,  True],\n",
       "        [ True,  True,  True,  ...,  True,  True, False]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Entailment<|endoftext|><|endoftext|>', 'NotMentioned<|endoftext|>']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(batch['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpt = torch.load(\"../../runs/cnli_original/gpt2/lr5e-05_linear_adamw_wd1e-03_124-128-1x128_mem2_bs32_iters6000_regular_bptt-1_refactor/run_1/model_best.pth\", map_location='cpu')\n",
    "model.load_state_dict(cpt['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "#     out = model(**batch)\n",
    "#     out.loss.backward()\n",
    "#     optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([50257,  3673,    44,  1463,   276])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'][0][batch['labels_mask'][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3551, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = out.logits.argmax(dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Notailment<|endoftext|>', 'Notailment<|endoftext|>']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode([p[m] for p, m in zip(pred, batch['labels_mask'])], add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[GEN]Entailment', '[GEN]Entailment']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode([p[m] for p, m in zip(batch['labels'], batch['labels_mask'])], add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NotMentioned<|endoftext|>', 'Notailment<|endoftext|>']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer.batch_decode([p[m] for p, m in zip(pred, batch['labels_mask'])], add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NotMentioned<|endoftext|>', 'NotMentionedNot']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer.batch_decode([p[m] for p, m in zip(pred, batch['labels_mask'])], add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2207, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_inputs = [b[~m] for b, m in zip(batch['input_ids'], batch['labels_mask'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Receiving Party shall not disclose the fact that Agreement was agreed or negotiated. Johns Hopkins University\\nNON-DISCLOSURE AGREEMENT For Bilateral Disclosure\\nThis Agreement is effective this of in the year ______ is by and between JHU and COMPANY, each defined below.\\nJHU: The Johns Hopkins University\\nAddress: 100 N. Charles St., 5th Floor\\nBaltimore, Maryland 21201\\nJHU Contact:\\nCOMPANY:\\nAddress:\\nCOMPANY Contact:\\nWHEREAS, each party has certain technical information described below which shall hereinafter be referred to as \"CONFIDENTIAL INFORMATION\";\\nCONFIDENTIAL INFORMATION:\\nWHEREAS, JHU and COMPANY are each interested in examining the CONFIDENTIAL INFORMATION of the other solely for the PURPOSE, defined below;\\nPURPOSE: To explore licensing, collaborative or sponsored research agreement opportunities related to the CONFIDENTIAL INFORMATION.\\nNOW, THEREFORE, in consideration of the premises and mutual covenants contained herein, the parties hereto agree as follows:\\n‚ÄùPROVIDER‚Äù shall mean the party hereto disclosing CONFIDENTIAL INFORMATION to the RECIPIENT party.\\n‚ÄúRECIPIENT‚Äù shall mean the party receiving CONFIDENTIAL INFORMATION from the PROVIDER party.\\n 1. PROVIDER, through its employee, the PROVIDER Contact, shall disclose CONFIDENTIAL INFORMATION to RECIPIENT, through its employee, the RECIPIENT Contact, to enable RECIPIENT to fully evaluate such disclosure solely for the PURPOSE. CONFIDENTIAL INFORMATION shall be indicated as confidential at the time of disclosure.\\n2. RECIPIENT agrees to accept the CONFIDENTIAL INFORMATION and to employ all reasonable efforts to maintain the CONFIDENTIAL INFORMATION as secret and confidential, such efforts to be no less than the degree of care employed by RECIPIENT to preserve and safeguard RECIPIENT\\'s own confidential information. The CONFIDENTIAL INFORMATION shall not be disclosed or revealed to anyone except employees of RECIPIENT who have a need to know the CONFIDENTIAL INFORMATION for the PURPOSE and who agree to be bound by the terms of this Agreement.\\n3. It is hereby acknowledged by PROVIDER that RECIPIENT shall incur no liability merely for examining and considering the CONFIDENTIAL INFORMATION. However, RECIPIENT agrees that it will not use the CONFIDENTIAL INFORMATION for any purpose other<|endoftext|>'"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(gen_inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = tokenizer.batch_decode([l[m] for l, m in zip(batch['labels'], batch['labels_mask'])], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Entailment', 'Entailment']"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(pred, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NotMentioned', 'Contradiction']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer.batch_decode([l[m] for l, m in zip(batch['labels'], batch['labels_mask'])], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(62.5257, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(15):\n",
    "    out = model(**batch)\n",
    "    out.loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.8596, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = out.logits.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_text = tokenizer.batch_decode(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "generated = model.memory_cell.model.generate(batch['labels'][:, -5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([50257,  3673,    44,  1463,   276, 50256])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'][0][batch['labels_mask'][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([50257,  3673,    44,  1463,   276, 50256])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['labels'][0][batch['labels_mask'][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_inputs = [torch.cat((b[~m], torch.tensor([gen_token]))) for b, m in zip(batch['input_ids'], batch['labels_mask'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb Cell 34\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb#Y310sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m gen \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mmemory_cell\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(gen_inputs)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:26\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     25\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m():\n\u001b[0;32m---> 26\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/transformers/generation_utils.py:1084\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[39m# 2. Define model inputs\u001b[39;00m\n\u001b[1;32m   1079\u001b[0m \u001b[39m# inputs_tensor has to be defined\u001b[39;00m\n\u001b[1;32m   1080\u001b[0m \u001b[39m# model_input_name is defined if model-specific keyword input is passed\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m \u001b[39m# otherwise model_input_name is None\u001b[39;00m\n\u001b[1;32m   1082\u001b[0m \u001b[39m# all model-specific keyword inputs are removed from `model_kwargs`\u001b[39;00m\n\u001b[1;32m   1083\u001b[0m inputs_tensor, model_input_name, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_model_inputs(inputs, bos_token_id, model_kwargs)\n\u001b[0;32m-> 1084\u001b[0m batch_size \u001b[39m=\u001b[39m inputs_tensor\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1086\u001b[0m \u001b[39m# 3. Define other model kwargs\u001b[39;00m\n\u001b[1;32m   1087\u001b[0m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39moutput_attentions\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m output_attentions\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "gen = model.memory_cell.model.generate(gen_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def keep_for_metrics_fn(batch, output):\n",
    "    data = {}\n",
    "    if isinstance(output, torch.Tensor):\n",
    "        data['labels'] = batch['labels']\n",
    "        \n",
    "    for key in batch.keys():\n",
    "        if 'loss' in key: \n",
    "            data[key] = batch[key]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[GEN]NotMentioned[GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN]',\n",
       " ' in[GEN]Contradiction[GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN]']"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(generated, add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  def keep_for_metrics_fn(batch, output):\n",
    "data = {}\n",
    "# if 'generation_outputs' in output:\n",
    "if isinstance(output, dict):\n",
    "    data['predictions'] = torch.argmax(output['logits'].detach(), dim=-1)\n",
    "    if 'labels_mask' in batch:\n",
    "        data['predictions'] = [data['predictions'][i, mask] for i, mask in enumerate(batch['labels_mask'])]\n",
    "else:\n",
    "    data['generation_outputs'] = output\n",
    "    \n",
    "data['labels'] = batch['labels']\n",
    "for key in batch.keys():\n",
    "    if 'loss' in key: \n",
    "        data[key] = batch[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = CausalLMOutputWithCrossAttentions()\n",
    "# full_logits = torch.cat([o.logits for o in cell_outputs], dim=1)\n",
    "# full_hidden_states = tuple([torch.cat(layer_hs, dim=1) for layer_hs in zip(*[o.hidden_states for o in cell_outputs])])\n",
    "\n",
    "full_logits = out.logits\n",
    "labels = kwargs.get('labels')\n",
    "if labels is not None:\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "    shift_logits = full_logits[..., :-1, :].contiguous()\n",
    "    flat_labels = shift_labels.view(-1)\n",
    "    flat_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "    \n",
    "    loss_fct = CrossEntropyLoss()\n",
    "    labels_mask = kwargs.get('labels_mask')\n",
    "    if labels_mask is not None:\n",
    "        shift_mask = labels_mask[..., :-1].contiguous()\n",
    "\n",
    "        flat_labels = flat_labels[shift_mask.view(-1)]\n",
    "        flat_logits = flat_logits[shift_mask.view(-1)]\n",
    "        \n",
    "    out['loss'] = loss_fct(flat_logits, flat_labels)\n",
    "\n",
    "out['logits'] = full_logits\n",
    "segment_keys = ['loss', 'logits']\n",
    "if kwargs.get('output_attentions'):\n",
    "    segment_keys.append('attentions')\n",
    "if kwargs.get('output_hidden_states'):\n",
    "    segment_keys.append('hidden_states')\n",
    "    out['hidden_states'] = full_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_sampler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb#Y221sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m### booydar\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb#Y221sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m batch_metrics_fn \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m _, y: {key: y[key] \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m y\u001b[39m.\u001b[39mkeys() \u001b[39mif\u001b[39;00m ((\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m key) \u001b[39mor\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39m!log\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m key))}\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb#Y221sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(args, model, optimizer, train_dataloader, valid_dataloader, train_sampler,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb#Y221sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m                     keep_for_metrics_fn\u001b[39m=\u001b[39mkeep_for_metrics_fn, metrics_fn\u001b[39m=\u001b[39mmetrics_fn,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb#Y221sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m                     \u001b[39m###booydar\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb#Y221sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m                     batch_metrics_fn\u001b[39m=\u001b[39mbatch_metrics_fn,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb#Y221sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m                     generate_kwargs\u001b[39m=\u001b[39m{})\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb#Y221sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m args\u001b[39m.\u001b[39mvalidate_only:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb#Y221sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# train loop\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb#Y221sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     trainer\u001b[39m.\u001b[39mtrain()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_sampler' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "### booydar\n",
    "batch_metrics_fn = lambda _, y: {key: y[key] for key in y.keys() if (('loss' in key) or ('!log' in key))}\n",
    "trainer = Trainer(args, model, optimizer, train_dataloader, valid_dataloader, train_sampler,\n",
    "                    keep_for_metrics_fn=keep_for_metrics_fn, metrics_fn=metrics_fn,\n",
    "                    ###booydar\n",
    "                    batch_metrics_fn=batch_metrics_fn,\n",
    "                    generate_kwargs={})\n",
    "\n",
    "if not args.validate_only:\n",
    "    # train loop\n",
    "    trainer.train()\n",
    "    # make sure all workers are done\n",
    "    hvd.barrier()\n",
    "    # run validation after training\n",
    "    if args.save_best:\n",
    "        best_model_path = str(Path(args.model_path) / 'model_best.pth')\n",
    "        if hvd.rank() == 0:\n",
    "            logger.info(f'Loading best saved model from {best_model_path}')\n",
    "        trainer.load(best_model_path)\n",
    "    if valid_dataloader is not None:\n",
    "        if hvd.rank() == 0:\n",
    "            logger.info('Runnning validation on valid data:')\n",
    "        trainer.validate(valid_dataloader, write_tb=False)\n",
    "else:\n",
    "    # run validation, do not write to tensorboard\n",
    "    if hvd.rank() == 0:\n",
    "        logger.info('Running validation on train set:')\n",
    "    trainer.validate(train_dataloader, split='train', write_tb=True)\n",
    "    if valid_dataloader is not None:\n",
    "        if hvd.rank() == 0:\n",
    "            logger.info('Running validation on valid data:')\n",
    "        trainer.validate(valid_dataloader, write_tb=True)\n",
    "    # if test_dataloader is not None:\n",
    "    #     if hvd.rank() == 0:\n",
    "    #         logger.info('Runnning validation on test data:')\n",
    "    #     trainer.validate(test_dataloader, write_tb=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Holder:\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq_len = 512\n",
    "target_seq_len = 512\n",
    "\n",
    "num_mem_tokens = 2\n",
    "input_size = 128\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "args = Holder\n",
    "args.target_seq_len = target_seq_len\n",
    "args.input_seq_len = input_seq_len\n",
    "args.num_mem_tokens = num_mem_tokens\n",
    "args.input_size = input_size\n",
    "args.input_prefix = ''\n",
    "args.block_size = None\n",
    "args.task_name = 'wikitext-2-v1'\n",
    "\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/cryptography/hazmat/backends/openssl/x509.py:14: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = 'gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/home/bulatov/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1360fde036a34b9292dfbe062f075a5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/bulatov/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-389b922bfc5fe729.arrow\n",
      "Loading cached processed dataset at /home/bulatov/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-6067a66e735cfbb1.arrow\n",
      "Loading cached processed dataset at /home/bulatov/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-941845a5470f2db7.arrow\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "raw_datasets = datasets.load_dataset('wikitext', args.task_name)\n",
    "column_names = raw_datasets[\"train\"].column_names\n",
    "text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[text_column_name])\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=column_names,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "block_size = args.input_size \n",
    "if args.num_mem_tokens is not None:\n",
    "    block_size -= 2 * args.num_mem_tokens\n",
    "history_size = args.input_seq_len - block_size\n",
    "\n",
    "def group_texts(examples, block_size, history_size=None):\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "\n",
    "    if history_size is None:\n",
    "        result = {\n",
    "            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "    else:\n",
    "        result = {\n",
    "            k: [t[max({0, i - history_size}) : i + block_size] for i in range(0, total_length, block_size)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_pad_value = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124, 388)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size, history_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/bulatov/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-1504f9373e317eca.arrow\n",
      "Loading cached processed dataset at /home/bulatov/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-c6da793e710ea6d8.arrow\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     input_ids = [torch.tensor(b['input_ids'][::-1]) for b in batch]\n",
    "#     labels = [torch.tensor(b['labels'][::-1]) for b in batch]\n",
    "#     attention_mask = [torch.tensor(b['attention_mask'][::-1]) for b in batch]\n",
    "#     input_ids = pad_sequence(input_ids, padding_value=id_pad_value).T.flip(1)\n",
    "#     labels = pad_sequence(labels, padding_value=-100).T.flip(1)\n",
    "#     attention_mask = pad_sequence(attention_mask, padding_value=0).T.flip(1)\n",
    "\n",
    "#     collated = {'input_ids': input_ids,\n",
    "#                 'labels': labels, \n",
    "#                 'attention_mask': attention_mask}\n",
    "\n",
    "#     if input_ids.shape[1] != block_size:\n",
    "#         labels_mask = torch.ones_like(input_ids, dtype=bool)\n",
    "#         labels_mask[:, :-block_size] = False\n",
    "#         collated['labels_mask'] = labels_mask\n",
    "\n",
    "#     return collated\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [torch.tensor(b['input_ids']) for b in batch]\n",
    "    labels = [torch.tensor(b['labels']) for b in batch]\n",
    "    labels_mask = [torch.ones_like(l, dtype=bool) for l in labels]\n",
    "    attention_mask = [torch.tensor(b['attention_mask']) for b in batch]\n",
    "\n",
    "    input_ids = pad_sequence(input_ids, padding_value=id_pad_value).T\n",
    "    labels = pad_sequence(labels, padding_value=-100).T\n",
    "    labels_mask = pad_sequence(labels_mask, padding_value=False).T\n",
    "    attention_mask = pad_sequence(attention_mask, padding_value=0).T\n",
    "\n",
    "    collated = {'input_ids': input_ids,\n",
    "                'labels': labels, \n",
    "                'labels_mask': labels_mask,\n",
    "                'attention_mask': attention_mask}\n",
    "\n",
    "    # if args.vary_n_segments:\n",
    "    #     n_segments = np.random.randint(1, args.max_n_segments + 1)\n",
    "    #     n_tokens = n_segments * block_size\n",
    "    #     for k in collated:\n",
    "    #         collated[k] = collated[k][:, -n_tokens:]\n",
    "\n",
    "    return collated\n",
    "\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"].map(lambda x: group_texts(x, block_size, history_size), \n",
    "                                        batched=True, desc=f\"Grouping train in chunks of {block_size} and history {history_size}\")\n",
    "valid_dataset = tokenized_datasets[\"validation\"].map(lambda x: group_texts(x, block_size), \n",
    "                                        batched=True, desc=f\"Grouping valid in chunks of {block_size}\")\n",
    "\n",
    "\n",
    "# shuffle train data each epoch (one loop over train_dataset)\n",
    "# train_sampler = DistributedStrain_dataset[i] for i in range(4)ampler(train_dataset, rank=hvd.rank(), num_replicas=hvd.size(), shuffle=True,\n",
    "#                                     drop_last=False, seed=args.seed)\n",
    "# per_worker_batch_size = args.batch_size * args.gradient_accumulation_steps\n",
    "# global_batch_size = per_worker_batch_size * hvd.size()\n",
    "\n",
    "# train_sampler = RandomSampler(train_dataset)\n",
    "kwargs = {'pin_memory': True}#, 'num_workers': args.data_n_workers}\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [train_dataset[i] for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in b[0]:\n",
    "    b[0][k] = b[0][k][:124]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = collate_fn(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = iter(train_dataloader)\n",
    "batch = next(gen)\n",
    "batch = next(gen)\n",
    "batch = next(gen)\n",
    "batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_datasets['train'][1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions\n",
    "\n",
    "class MemoryCell(torch.nn.Module):\n",
    "    def __init__(self, base_model, num_mem_tokens):\n",
    "        super().__init__()\n",
    "        self.model = base_model\n",
    "        self.create_memory(num_mem_tokens)\n",
    "\n",
    "    def create_memory(self, num_mem_tokens):\n",
    "        self.num_mem_tokens = num_mem_tokens\n",
    "        embeddings = self.model.get_input_embeddings()\n",
    "        memory_weights = torch.randn((num_mem_tokens, self.model.config.n_embd)) * embeddings.weight.data.std()\n",
    "        self.register_parameter('memory', torch.nn.Parameter(memory_weights, requires_grad=True))\n",
    "\n",
    "        self.read_memory_position = range(num_mem_tokens)\n",
    "        self.write_memory_position = range(-num_mem_tokens, 0)\n",
    "\n",
    "    def set_memory(self, input_shape):\n",
    "        memory = self.memory.repeat(input_shape[0], 1, 1)\n",
    "        return memory\n",
    "\n",
    "    def forward(self, input_ids, memory_state=None, **kwargs):\n",
    "        if memory_state is None:\n",
    "            memory_state = self.set_memory(input_ids.shape)\n",
    "\n",
    "        seg_kwargs = self.process_input(input_ids, memory_state, **kwargs)\n",
    "        out = self.model(**seg_kwargs)\n",
    "        out, new_memory_state = self.process_output(out, **kwargs)\n",
    "\n",
    "        return out, new_memory_state\n",
    "    \n",
    "    def process_input(self, input_ids, memory_state, **kwargs):\n",
    "        seg_kwargs = dict(**kwargs)\n",
    "\n",
    "        inputs_embeds = kwargs.get('inputs_embeds')\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.model.get_input_embeddings()(input_ids)\n",
    "        inputs_embeds = torch.cat([memory_state, inputs_embeds, memory_state], dim=1)\n",
    "\n",
    "        seg_kwargs['input_ids'] = None\n",
    "        seg_kwargs['inputs_embeds'] = inputs_embeds\n",
    "        if kwargs.get('attention_mask') is not None:\n",
    "            seg_kwargs['attention_mask'] = self.pad_attention_mask(kwargs['attention_mask'], inputs_embeds.shape)\n",
    "        seg_kwargs['output_hidden_states'] = True\n",
    "        return seg_kwargs\n",
    "    \n",
    "    def pad_attention_mask(self, attention_mask, shape):\n",
    "        if self.num_mem_tokens in {0, None}:\n",
    "            return attention_mask\n",
    "        else:\n",
    "            mask = torch.ones(*shape[:2], dtype=torch.int64).to(attention_mask.device)\n",
    "            mask[:, self.num_mem_tokens:-self.num_mem_tokens] = attention_mask\n",
    "            return mask\n",
    "    \n",
    "    def process_output(self, model_outputs, **kwargs):\n",
    "        if self.num_mem_tokens not in {0, None}:\n",
    "            out = CausalLMOutputWithCrossAttentions()\n",
    "            memory_state = model_outputs.hidden_states[-1][:, -self.num_mem_tokens:]\n",
    "            out['logits'] = model_outputs.logits[:, self.num_mem_tokens:-self.num_mem_tokens]\n",
    "            \n",
    "            if kwargs.get('output_hidden_states'):\n",
    "                out['hidden_states'] = [lh[:, self.num_mem_tokens:-self.num_mem_tokens] for lh in model_outputs.hidden_states]\n",
    "            if kwargs.get('output_attentions'):\n",
    "                out['attentions'] = model_outputs['attentions']\n",
    "        else:\n",
    "            memory_state = None\n",
    "            out = model_outputs\n",
    "            \n",
    "        return out, memory_state \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentWrapper(torch.nn.Module):\n",
    "    def __init__(self, memory_cell, **rmt_kwargs):\n",
    "        super().__init__()\n",
    "        self.memory_cell = memory_cell\n",
    "        self.rmt_config = rmt_kwargs\n",
    "\n",
    "    def forward(self, input_ids, labels=None, labels_mask=None, inputs_embeds=None, attention_mask=None, output_attentions=None, output_hidden_states=None):\n",
    "        memory_state = None\n",
    "        segmented = self.segment(input_ids=input_ids, inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
    "\n",
    "        cell_outputs = []\n",
    "        for seg_num, segment in enumerate(segmented):\n",
    "            cell_out, memory_state = self.memory_cell(**segment, memory_state=memory_state, output_hidden_states=True)\n",
    "            cell_outputs.append(cell_out)\n",
    "            self.manage_gradients(memory_state, seg_num)\n",
    "\n",
    "        out = self.process_outputs(cell_outputs, labels=labels, \n",
    "                                   labels_mask=labels_mask,\n",
    "                                   output_attentions=output_attentions, \n",
    "                                   output_hidden_states=output_hidden_states)\n",
    "        return out\n",
    "\n",
    "    def segment(self, **kwargs):\n",
    "        segments = []\n",
    "        for k, tensor in kwargs.items():\n",
    "            if tensor is not None:\n",
    "                k_segments = self.split_tensor(tensor)\n",
    "                for s, k_seg in enumerate(k_segments):\n",
    "                    if s < len(segments):\n",
    "                        segments[s][k] = k_seg\n",
    "                    else:\n",
    "                        segments.append({k: k_seg})\n",
    "\n",
    "        return segments\n",
    "    \n",
    "    def split_tensor(self, tensor):\n",
    "        align = self.rmt_config.get('segment_alignment')\n",
    "        segment_size = self.rmt_config.get('segment_size')\n",
    "        if align in {'left', None}:\n",
    "            split_inds = list(range(0, tensor.shape[1], segment_size)) + [tensor.shape[1]]\n",
    "            segments = [tensor[:, start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "        elif align in {'right', None}:\n",
    "            split_inds = (list(range(tensor.shape[1], 0, -segment_size)) + [0])[::-1]\n",
    "            segments = [tensor[:, start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "        elif align == 'center':\n",
    "            n_seg = math.ceil(tensor.shape[1] / segment_size)\n",
    "            segments = torch.chunk(tensor, n_seg, dim=1)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return segments\n",
    "\n",
    "    def process_outputs(self, cell_outputs, **kwargs):\n",
    "        out = CausalLMOutputWithCrossAttentions()\n",
    "        full_logits = torch.cat([o.logits for o in cell_outputs], dim=1)\n",
    "        full_hidden_states = tuple([torch.cat(layer_hs, dim=1) for layer_hs in zip(*[o.hidden_states for o in cell_outputs])])\n",
    "\n",
    "        labels = kwargs.get('labels')\n",
    "        if labels is not None:\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            shift_logits = full_logits[..., :-1, :].contiguous()\n",
    "            flat_labels = shift_labels.view(-1)\n",
    "            flat_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "            \n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            labels_mask = kwargs.get('labels_mask')\n",
    "            if labels_mask is not None:\n",
    "                shift_mask = labels_mask[..., :-1].contiguous()\n",
    "\n",
    "                flat_labels = flat_labels[shift_mask.view(-1)]\n",
    "                flat_logits = flat_logits[shift_mask.view(-1)]\n",
    "                \n",
    "            out['loss'] = loss_fct(flat_logits, flat_labels)\n",
    "\n",
    "        out['logits'] = full_logits\n",
    "        segment_keys = ['loss', 'logits']\n",
    "        if kwargs.get('output_attentions'):\n",
    "            segment_keys.append('attentions')\n",
    "        if kwargs.get('output_hidden_states'):\n",
    "            segment_keys.append('hidden_states')\n",
    "            out['hidden_states'] = full_hidden_states\n",
    "\n",
    "        # for seg_num, o in enumerate(cell_outputs):\n",
    "        #     for key, value in o.items():\n",
    "        #         if any([sk in key for sk in segment_keys]):\n",
    "        #             out[f'{key}_{seg_num}'] = value\n",
    "\n",
    "        return out \n",
    "        \n",
    "    def manage_gradients(self, memory_state, seg_num):\n",
    "        k2, max_n_segments = self.rmt_config.get('k2'), self.rmt_config.get('max_n_segments')\n",
    "        if seg_num == 0 \\\n",
    "            or k2 in {-1, None} \\\n",
    "            or seg_num + k2 > max_n_segments:\n",
    "                return True\n",
    "        \n",
    "        memory_state = memory_state.detach()\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/cryptography/hazmat/backends/openssl/x509.py:14: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_mem_tokens = 10\n",
    "# device = torch.device(3)\n",
    "device = 'cpu'\n",
    "\n",
    "rmt_config = {'num_mem_tokens': 10, \n",
    "               #  'max_n_segments': 1,\n",
    "               #  'segment_alignment': 'right',\n",
    "               #  'tokenizer': tokenizer,\n",
    "               #  'memory_layers': 'all', \n",
    "               #  'share_memory_layers': True,\n",
    "               #  'reconstruction_loss_coef': 0.1,\n",
    "               #  'k1': -1, 'k2': 3,\n",
    "               #  'segment_ordering': 'regular',\n",
    "               #  'input_size': 1024, \n",
    "               #  'bptt_depth': -1, \n",
    "               #  'sum_loss': False,\n",
    "             }\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "cell = MemoryCell(base_model, num_mem_tokens=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmt = RecurrentWrapper(cell, max_n_segments=5, segment_size=124, segment_alignment='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = iter(train_dataloader)\n",
    "batch = next(gen)\n",
    "batch = next(gen)\n",
    "batch = next(gen)\n",
    "batch.pop('labels_mask')\n",
    "# batch.pop('labels')\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmt_out = rmt(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['loss', 'logits'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmt_out.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contract-NLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "class CNLIDataset(Dataset):\n",
    "    def __init__(self, json_path):\n",
    "        with open(json_path, 'r') as f:\n",
    "            self.samples = json.load(f)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.samples[item]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done   1 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Batch computation too fast (0.0032s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=12)]: Done   8 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Batch computation too fast (0.0160s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=12)]: Done  28 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done  50 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 123 out of 123 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "dataset = CNLIDataset('/cephfs/home/bulatov/bulatov/datasets/contract_nli/contract-nli/test_processed.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.input_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_tokens('[GEN]', special_tokens=True)\n",
    "gen_token = tokenizer.encode('[GEN]')[0]\n",
    "id_pad_value = tokenizer.eos_token_id\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs = [b['context'][:args.input_seq_len * 10] for b in batch]\n",
    "    labels = [b['answer'][:args.input_seq_len * 10] for b in batch]\n",
    "\n",
    "    collated = {}\n",
    "    inputs = tokenizer.batch_encode_plus(list(inputs), padding=False)\n",
    "    labels = tokenizer.batch_encode_plus(list(labels), padding=False)\n",
    "\n",
    "    full_inputs, labels_mask = [], []\n",
    "    for inp, lab in zip(inputs['input_ids'], labels['input_ids']):\n",
    "        inp = inp[:args.input_seq_len - len(lab) - 1]\n",
    "        full_inputs.append(torch.tensor(inp + [gen_token] + lab))\n",
    "        labels_mask.append(torch.tensor([False] * len(inp) + [True] * (len(lab) + 1)))\n",
    "\n",
    "    full_inputs = pad_sequence(full_inputs, padding_value=id_pad_value).T\n",
    "    labels_mask = pad_sequence(labels_mask, padding_value=False).T\n",
    "\n",
    "    collated['input_ids'] = collated['labels'] = full_inputs\n",
    "    collated['labels_mask'] = labels_mask\n",
    "    collated['attention_mask'] = collated['input_ids'] != id_pad_value\n",
    "    return collated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = collate_fn([dataset[i] for i in range(4)])\n",
    "# batch = [dataset[i] for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented = rmt.segment(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([4, 103]),\n",
       " torch.Size([4, 103]),\n",
       " torch.Size([4, 103]),\n",
       " torch.Size([4, 103]),\n",
       " torch.Size([4, 100])]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s['input_ids'].shape for s in segmented]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = batch.pop('labels')\n",
    "rmt_out = rmt(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512, 50258])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmt_out.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 50258])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmt_out.logits[batch['labels_mask']].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[batch['labels_mask']].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NotMentioned<|endoftext|>'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([ 3673,    44,  1463,   276, 50256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Entailment'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([14539,   603,   434])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmt_out = rmt(**batch)\n",
    "\n",
    "self = rmt\n",
    "input_ids = batch['input_ids']\n",
    "\n",
    "memory_state = None\n",
    "segmented = self.segment(input_ids=input_ids, inputs_embeds=None, attention_mask=None)\n",
    "\n",
    "cell_outputs = []\n",
    "for seg_num, segment in enumerate(segmented):\n",
    "    cell_out, memory_state = self.memory_cell(**segment, memory_state=memory_state, output_hidden_states=True)#**batch)\n",
    "    cell_outputs.append(cell_out)\n",
    "    self.manage_gradients(memory_state, seg_num)\n",
    "\n",
    "# out = self.process_outputs(cell_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 248])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1.]), tensor([1., 1., 1., 1.]), tensor([1., 1.]))"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.chunk(torch.ones(10), 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50258, 768)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "# scrolls_metric_path = hf_hub_download(repo_id=\"datasets/tau/scrolls\", filename=\"metrics/scrolls.py\")tokenizer.add_tokens('[GEN]', special_tokens=True)\n",
    "gen_token = tokenizer.encode('[GEN]')[0]\n",
    "\n",
    "rmt.memory_cell.model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# gen_token = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<|endoftext|>',\n",
       " 'eos_token': '<|endoftext|>',\n",
       " 'unk_token': '<|endoftext|>'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_attention_first_token = False  # should be True for LED\n",
    "encode_plus_kwargs = {'truncation': True, 'padding': 'longest', 'pad_to_multiple_of': 1}\n",
    "# generate_kwargs = {'max_length': args.targettokenizer.add_tokens('[GEN]', special_tokens=True)\n",
    "# gen_token = tokenizer.encode('[GEN]')[0]\n",
    "\n",
    "# rmt.memory_cell.model.resize_token_embeddings(len(tokenizer))_seq_len, 'min_length': args.target_seq_len}\n",
    "generate_kwargs = {}\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs = [b['input'][:args.input_seq_len * 10] for b in batch]\n",
    "    labels = [b['output'][:args.input_seq_len * 10] for b in batch]\n",
    "\n",
    "    collated = {}\n",
    "    inputs = tokenizer.batch_encode_plus(list(inputs), padding=False)\n",
    "    labels = tokenizer.batch_encode_plus(list(labels), padding=False)\n",
    "\n",
    "    full_inputs = [torch.tensor(i[:input_size - len(l) - 1] + [gen_token] + l) for i, l in zip(inputs['input_ids'], labels['input_ids'])]\n",
    "    full_inputs = pad_sequence(full_inputs, padding_value=tokenizer.pad_token_id).T\n",
    "    \n",
    "    labels_mask = torch.zeros_like(full_inputs).bool()\n",
    "    for i, l in enumerate(labels['input_ids']):\n",
    "        labels_mask[i, -len(l) -1:] = True\n",
    "    collated['input_ids'] = collated['labels'] = full_inputs\n",
    "    collated['labels_mask'] = labels_mask\n",
    "    collated['attention_mask'] = collated['input_ids'] != tokenizer.pad_token_id\n",
    "\n",
    "    return collated\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset scrolls (/home/bulatov/.cache/huggingface/datasets/tau___scrolls/quality/1.0.0/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bb2ffcdd0604c9bb263db2f85f18d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq2seq_task_name = 'quality'\n",
    "dataset = datasets.load_dataset('tau/scrolls', seq2seq_task_name)\n",
    "train_dataset = dataset['train']\n",
    "# shuffle train data each epoch (one loop over train_dataset)\n",
    "# train_sampler = DistributedSampler(train_dataset, shuffle=True, drop_last=False, seed=args.seed)\n",
    "kwargs = {'pin_memory': True}\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2,\n",
    "                                collate_fn=collate_fn, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = iter(train_dataloader)\n",
    "batch = next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'labels', 'labels_mask', 'attention_mask'])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['labels'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Why is Si retirement so significant to the Space Exploration Team? \\n\\n (A) There aren‚Äôt enough working people in the world. They won‚Äôt be able to find a replacement.\\n (B) As one of two remaining spacemen, it would likely mean the defunding and shut down of the Space Exploration Team.\\n (C) Training new spacemen is costly and time consuming. They won‚Äôt have anyone else ready after him.\\n (D) His retirement may inspire others to[GEN]Training new spacemen is costly and time consuming. They won‚Äôt have anyone else ready after him.',\n",
       " 'What makes Gubelin an outlier in the present day?\\n\\n (A) He is much older than the rest of the population.\\n (B) He refuses new operations that could improve his health.\\n (C) His mind is still active, and he values hard work.\\n (D) He still wears glasses and value objects like the gold watch given to Si.\\n\\n\\nSPACEMAN ON A SPREE\\n\\n\\n\\n\\n   BY MACK REYNOLDS\\n\\n\\n\\n\\n   Illustrated by Nodel\\n\\n\\n\\n\\n[GEN]He still wears glasses and value objects like the gold watch given to Si.']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(batch['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[GEN]Training new spacemen is costly and time consuming. They won‚Äôt have anyone else ready after him.',\n",
       " '[GEN]He still wears glasses and value objects like the gold watch given to Si.']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode([c[i] for c, i in zip(batch['input_ids'], batch['labels_mask'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Why is Si retirement so significant to the Space Exploration Team? \\n\\n (A) There aren‚Äôt enough working people in the world. They won‚Äôt be able to find a replacement.\\n (B) As one of two remaining spacemen, it would likely mean the defunding and shut down of the Space Exploration Team.\\n (C) Training new spacemen is costly and time consuming. They won‚Äôt have anyone else ready after him.\\n (D) His retirement may inspire others toTraining new spacemen is costly and time consuming. They won‚Äôt have anyone else ready after him.',\n",
       " 'What makes Gubelin an outlier in the present day?\\n\\n (A) He is much older than the rest of the population.\\n (B) He refuses new operations that could improve his health.\\n (C) His mind is still active, and he values hard work.\\n (D) He still wears glasses and value objects like the gold watch given to Si.\\n\\n\\nSPACEMAN ON A SPREE\\n\\n\\n\\n\\n   BY MACK REYNOLDS\\n\\n\\n\\n\\n   Illustrated by Nodel\\n\\n\\n\\n\\nHe still wears glasses and value objects like the gold watch given to Si.']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode([c[i] for c, i in zip(batch['input_ids'], batch['attention_mask'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = rmt(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(60.2067, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(61.4377, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hvdenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
