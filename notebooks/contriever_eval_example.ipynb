{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/griver/anaconda3/envs/rmt/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "sys.path.append(\"..\")\n",
    "from babilong_utils import TaskDataset, SentenceSampler, NoiseInjectionDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### extract dataset archive\n",
    "# !unzip ../data/tasks_1-20_v1-2.zip -d ../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qa10_indefinite-knowledge_test.txt   qa1_single-supporting-fact_test.txt\n",
      "qa10_indefinite-knowledge_train.txt  qa1_single-supporting-fact_train.txt\n",
      "qa11_basic-coreference_test.txt      qa20_agents-motivations_test.txt\n",
      "qa11_basic-coreference_train.txt     qa20_agents-motivations_train.txt\n",
      "qa12_conjunction_test.txt\t     qa2_two-supporting-facts_test.txt\n",
      "qa12_conjunction_train.txt\t     qa2_two-supporting-facts_train.txt\n",
      "qa13_compound-coreference_test.txt   qa3_three-supporting-facts_test.txt\n",
      "qa13_compound-coreference_train.txt  qa3_three-supporting-facts_train.txt\n",
      "qa14_time-reasoning_test.txt\t     qa4_two-arg-relations_test.txt\n",
      "qa14_time-reasoning_train.txt\t     qa4_two-arg-relations_train.txt\n",
      "qa15_basic-deduction_test.txt\t     qa5_three-arg-relations_test.txt\n",
      "qa15_basic-deduction_train.txt\t     qa5_three-arg-relations_train.txt\n",
      "qa16_basic-induction_test.txt\t     qa6_yes-no-questions_test.txt\n",
      "qa16_basic-induction_train.txt\t     qa6_yes-no-questions_train.txt\n",
      "qa17_positional-reasoning_test.txt   qa7_counting_test.txt\n",
      "qa17_positional-reasoning_train.txt  qa7_counting_train.txt\n",
      "qa18_size-reasoning_test.txt\t     qa8_lists-sets_test.txt\n",
      "qa18_size-reasoning_train.txt\t     qa8_lists-sets_train.txt\n",
      "qa19_path-finding_test.txt\t     qa9_simple-negation_test.txt\n",
      "qa19_path-finding_train.txt\t     qa9_simple-negation_train.txt\n"
     ]
    }
   ],
   "source": [
    "!ls ../data/tasks_1-20_v1-2/en-10k/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"qa2_two-supporting-facts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/griver/anaconda3/envs/rmt/lib/python3.9/site-packages/datasets/load.py:1429: FutureWarning: The repository for pg19 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/pg19\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_path =f\"../data/tasks_1-20_v1-2/en-10k/{task}_train.txt\"\n",
    "test_path = f\"../data/tasks_1-20_v1-2/en-10k/{task}_test.txt\"\n",
    "noise_dataset_name = \"pg19\"\n",
    "noise_dataset = datasets.load_dataset(noise_dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load task datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task \n",
    "task_dataset_train = TaskDataset(train_path)\n",
    "task_dataset_test = TaskDataset(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# background text\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "noise_sampler_train = SentenceSampler(noise_dataset['train'], tokenizer=tokenizer)\n",
    "noise_sampler_test = SentenceSampler(noise_dataset['test'], tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 500               # max number of tokens in sample\n",
    "dataset_train = NoiseInjectionDataset(task_dataset=task_dataset_train,\n",
    "                                        noise_sampler=noise_sampler_train,\n",
    "                                        tokenizer=tokenizer,\n",
    "                                        sample_size=sample_size)\n",
    "\n",
    "dataset_test = NoiseInjectionDataset(task_dataset=task_dataset_test,\n",
    "                                        noise_sampler=noise_sampler_test,\n",
    "                                        tokenizer=tokenizer,\n",
    "                                        sample_size=sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['facts', 'question', 'answer', 'references', 'background_tokens', 'background_text', 'fact_positions', 'input_tokens', 'question_tokens', 'target_tokens'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = dataset_train[0]\n",
    "sample.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary moved to the bathroom.\n",
      "Sandra journeyed to the bedroom.\n",
      "Mary got the football there.\n",
      "John went to the kitchen.\n",
      "Mary went back to the kitchen.\n",
      "Mary went back to the garden.\n",
      "fact position: [ 1  6  6  9 10 12]\n",
      "question: Where is the football? \n",
      "\n",
      "BACKGROUND:\n",
      "'\"From what I have already observed,\" said Mr.',\n",
      "'Ellison, \"you will\n",
      "understand that I reject the idea, here expressed, of 'recalling the\n",
      "original beauty of the country.'',\n",
      "'The original beauty is never so great\n",
      "as that which may be introduced.',\n",
      "'Of course, much depends upon the\n",
      "selection of a spot with capabilities.',\n",
      "'What is said in respect to the\n",
      "'detecting and bringing into practice those nice relations of size,\n",
      "proportion and color,' is a mere vagueness of speech, which may mean\n",
      "much, or little, or nothing, and which guides in no degree.',\n",
      "'That the\n",
      "true 'result of the natural style of gardening is seen rather in the\n",
      "absence of all defects and incongruities, than in the creation of any\n",
      "special wonders or miracles,' is a proposition better suited to the\n",
      "grovelling apprehension of the herd, than to the fervid dreams of the\n",
      "man of genius.',\n",
      "'The merit suggested is, at best, negative, and appertains\n",
      "to that hobbling criticism which, in letters, would elevate Addison\n",
      "into apotheosis.',\n",
      "'In truth, while that merit which consists in the mere\n",
      "avoiding demerit, appeals directly to the understanding, and can thus\n",
      "be foreshadowed in Rule, the loftier merit, which breathes and flames\n",
      "in invention or creation, can be apprehended solely in its results.',\n",
      "'Rule\n",
      "applies but to the excellences of avoidance--to the virtues which deny\n",
      "or refrain.',\n",
      "'We may be\n",
      "instructed to build an Odyssey, but it is in vain that we are told\n",
      "how to conceive a 'Tempest,' an 'Inferno,' a 'Prometheus Bound,' a\n",
      "'Nightingale,' such as that of Keats, or the 'Sensitive Plant' of\n",
      "Shelley.',\n",
      "'But, the thing done, the wonder accomplished, and the capacity\n",
      "for apprehension becomes universal.',\n",
      "'The sophists of the negative school,\n",
      "who, through inability to create, have scoffed at creation, are now\n",
      "found the loudest in applause.',\n",
      "'What, in its chrysalis condition of\n",
      "principle, affronted their demure reason, never fails, in its maturity\n",
      "',\n"
     ]
    }
   ],
   "source": [
    "for f in sample['facts']:\n",
    "    print(f)\n",
    "print(\"fact position:\", sample['fact_positions'])\n",
    "print(\"question:\", sample['question'])\n",
    "print(\"\\nBACKGROUND:\")\n",
    "\n",
    "#background_text = tokenizer.batch_decode()\n",
    "for s in sample['background_text'][:20]:\n",
    "    print(f'\\'{s}\\',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize one sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Facts: Mary moved to the bathroom. Sandra journeyed to the bedroom. Mary got the football there. John went to the kitchen. Mary went back to the kitchen. Mary went back to the garden.\n",
      "Question: Where is the football? \n",
      "Answer: garden\n",
      "References: Mary got the football there. Mary went back to the garden.\n",
      "\n",
      "Background text:  \"From what I have already observed,\" said Mr. Ellison, \"you will\n",
      "understand that I reject the idea, here expressed, of 'recalling the\n",
      "original beauty of the country.' The original beauty is never so great\n",
      "as that which may be introduced. Of course, much depends upon the\n",
      "selection of a spot with capabilities. What is said in respect to the\n",
      "'detecting and bringing into practice those nice relations of size,\n",
      "proportion and color,' is a mere vagueness of speech, which may mean\n",
      "much, or little, or nothing, and which guides in no degree. That the\n",
      "true 'result of the natural style of gardening is seen rather in the\n",
      "absence of all defects and incongruities, than in the creation of any\n",
      "special wonders or miracles,' is a proposition better suited to the\n",
      "grovelling apprehension of the herd, than to the fervid dreams of the\n",
      "man of genius. The merit suggested is, at best, negative, and appertains\n",
      "to that hobbling criticism which, in letters, would elevate Addison\n",
      "into apotheosis. In truth, while that merit which consists in the mere\n",
      "avoiding demerit, appeals directly to the understanding, and can thus\n",
      "be foreshadowed in Rule, the loftier merit, which breathes and flames\n",
      "in invention or creation, can be apprehended solely in its results. Rule\n",
      "applies but to the excellences of avoidance--to the virtues which deny\n",
      "or refrain. We may be\n",
      "instructed to build an Odyssey, but it is in vain that we are told\n",
      "how to conceive a 'Tempest,' an 'Inferno,' a 'Prometheus Bound,' a\n",
      "'Nightingale,' such as that of Keats, or the 'Sensitive Plant' of\n",
      "Shelley. But, the thing done, the wonder accomplished, and the capacity\n",
      "for apprehension becomes universal. The sophists of the negative school,\n",
      "who, through inability to create, have scoffed at creation, are now\n",
      "found the loudest in applause. What, in its chrysalis condition of\n",
      "principle, affronted their demure reason, never fails, in its maturity\n",
      "\n",
      "Fact positions:  [ 1  6  6  9 10 12]\n",
      "Combined input:  \"From what I have already observed,\" said Mr.Mary moved to the bathroom.Ellison, \"you will\n",
      "understand that I reject the idea, here expressed, of'recalling the\n",
      "original beauty of the country.'The original beauty is never so great\n",
      "as that which may be introduced.Of course, much depends upon the\n",
      "selection of a spot with capabilities.What is said in respect to the\n",
      "'detecting and bringing into practice those nice relations of size,\n",
      "proportion and color,' is a mere vagueness of speech, which may mean\n",
      "much, or little, or nothing, and which guides in no degree.That the\n",
      "true'result of the natural style of gardening is seen rather in the\n",
      "absence of all defects and incongruities, than in the creation of any\n",
      "special wonders or miracles,' is a proposition better suited to the\n",
      "grovelling apprehension of the herd, than to the fervid dreams of the\n",
      "man of genius.Sandra journeyed to the bedroom.Mary got the football there.The merit suggested is, at best, negative, and appertains\n",
      "to that hobbling criticism which, in letters, would elevate Addison\n",
      "into apotheosis.In truth, while that merit which consists in the mere\n",
      "avoiding demerit, appeals directly to the understanding, and can thus\n",
      "be foreshadowed in Rule, the loftier merit, which breathes and flames\n",
      "in invention or creation, can be apprehended solely in its results.Rule\n",
      "applies but to the excellences of avoidance--to the virtues which deny\n",
      "or refrain.John went to the kitchen.We may be\n",
      "instructed to build an Odyssey, but it is in vain that we are told\n",
      "how to conceive a 'Tempest,' an 'Inferno,' a 'Prometheus Bound,' a\n",
      "'Nightingale,' such as that of Keats, or the 'Sensitive Plant' of\n",
      "Shelley.Mary went back to the kitchen.But, the thing done, the wonder accomplished, and the capacity\n",
      "for apprehension becomes universal.The sophists of the negative school,\n",
      "who, through inability to create, have scoffed at creation, are now\n",
      "found the loudest in applause.Mary went back to the garden.What, in its chrysalis condition of\n",
      "principle, affronted their demure reason, never fails, in its maturity\n",
      "\n",
      "Target: garden\n"
     ]
    }
   ],
   "source": [
    "facts = sample['facts']\n",
    "question = sample['question']\n",
    "answer = tokenizer.decode(sample['target_tokens'])\n",
    "\n",
    "background_text = sample['background_text']\n",
    "\n",
    "input_tokens = tokenizer.decode(sample['input_tokens'])\n",
    "\n",
    "print(f\"Facts: {' '.join(facts)}\")\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")\n",
    "print(f\"References: {' '.join(sample['references'])}\")\n",
    "print()\n",
    "print('Background text: ', ' '.join(background_text))\n",
    "print('Fact positions: ', sample['fact_positions'])\n",
    "print('Combined input: ', input_tokens)\n",
    "\n",
    "print(f\"Target: {answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "id_pad_value = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "gen_token = tokenizer.encode('GEN')[0]\n",
    "eos_token = tokenizer.eos_token_id\n",
    "\n",
    "def collate_fn(batch):\n",
    "    targets = [torch.tensor(b['target_tokens']) for b in batch]\n",
    "    input_ids = [torch.tensor(b['input_tokens'] + [gen_token] + b['target_tokens'] + [eos_token]) for b in batch]\n",
    "    gen_inputs = [torch.tensor(b['input_tokens'] + [gen_token]) for b in batch]\n",
    "\n",
    "    attention_mask = [torch.ones_like(b, dtype=int) for b in input_ids]\n",
    "    labels_mask = [torch.zeros_like(b, dtype=bool) for b in input_ids]\n",
    "    for m, t in zip(labels_mask, targets):\n",
    "        m[-len(t) - 2:] = True\n",
    "\n",
    "    input_ids = pad_sequence(input_ids, padding_value=id_pad_value, batch_first=True)\n",
    "    gen_inputs = pad_sequence(gen_inputs, padding_value=id_pad_value, batch_first=True)\n",
    "    # labels = pad_sequence(input_ids, padding_value=-100, batch_first=True)\n",
    "    attention_mask = pad_sequence(attention_mask, padding_value=0, batch_first=True)\n",
    "    labels_mask = pad_sequence(labels_mask, padding_value=0, batch_first=True)\n",
    "\n",
    "    collated = {}\n",
    "    collated['input_ids'] = collated['labels'] = input_ids\n",
    "    collated['input_ids_generate'] = gen_inputs\n",
    "    collated['labels_mask'] = labels_mask\n",
    "    collated['attention_mask'] = attention_mask.bool()\n",
    "    collated['attention_mask_generate'] = (gen_inputs != id_pad_value).bool()\n",
    "\n",
    "    collated['target_text'] = [b['answer'] for b in batch]\n",
    "    \n",
    "    collated['background_text'] = [b['background_text'] for b in batch]\n",
    "    collated['facts'] = [b['facts'] for b in batch]\n",
    "    collated['question'] = [b['question'] for b in batch]\n",
    "    \n",
    "    return collated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'labels', 'input_ids_generate', 'labels_mask', 'attention_mask', 'attention_mask_generate', 'target_text', 'background_text', 'facts', 'question'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = [dataset_train[i] for i in range(10)]\n",
    "collated = collate_fn(batch)\n",
    "collated.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels are marked with labels_mask\n",
    "#tokenizer.batch_decode([c[m] for c, m in zip(collated['input_ids'], collated['labels_mask'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different input_ids for .forward() and .generate()\n",
    "#tokenizer.batch_decode([c[m] for c, m in zip(collated['input_ids'], collated['attention_mask'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.batch_decode([c[m] for c, m in zip(collated['input_ids_generate'], collated['attention_mask_generate'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if Contriever can find relevant facts from the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "contriever_path = \"../../contriever/\" \n",
    "if contriever_path not in sys.path:\n",
    "    sys.path.append(contriever_path)\n",
    "\n",
    "#for p in sys.path:    \n",
    "#    print(p)\n",
    "\n",
    "from src.contriever import Contriever\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/contriever were not used when initializing Contriever: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing Contriever from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Contriever from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "contriever = Contriever.from_pretrained(\"facebook/contriever\") \n",
    "c_tokenizer = AutoTokenizer.from_pretrained(\"facebook/contriever\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "@torch.no_grad()\n",
    "def compite_statistics(res, num_retr=5, verbose=False):\n",
    "    N = len(res['query'])\n",
    "    num_retrieved_facts = 0\n",
    "    num_facts = 0\n",
    "    num_retrieves = 0\n",
    "    for i in range(N):\n",
    "        scores = torch.inner(res['query'][i], res['sentences'][i])\n",
    "        sorted_scores = torch.argsort(scores)\n",
    "        \n",
    "        fact_ids = res['facts_ids'][i]\n",
    "        k = num_retr if num_retr > 0 else len(fact_ids) \n",
    "        top_k = sorted_scores[-k:]\n",
    "        \n",
    "        num_retrieved_facts += sum(id < len(fact_ids) for id in top_k)\n",
    "        num_facts += len(fact_ids)\n",
    "        num_retrieves += k\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"retrieved sentences: {top_k}, all_facts: {fact_ids}\")    \n",
    "\n",
    "    stats = dict()\n",
    "    stats['precision'] = num_retrieved_facts/ num_retrieves\n",
    "    stats['recall'] = num_retrieved_facts / num_facts\n",
    "    \n",
    "    print(f\"precision: {stats['precision']:.2f}, recall: {stats['recall']:.2f}\")\n",
    "    return stats\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_contriever_embeds(collated):\n",
    "    sent_embeds = []\n",
    "    facts_ids = []\n",
    "    query_embeds = []\n",
    "    N = len(collated['facts'])\n",
    "    for i in range(N):\n",
    "        sentences = []\n",
    "        sentences.extend(collated['facts'][i])\n",
    "        facts_ids.append(np.arange(len(sentences)))\n",
    "        sentences.extend(collated['background_text'][i])\n",
    "        sentences.append(collated['question'][i]) # append as this is a single str\n",
    "        \n",
    "        # print(\"fact_ids:\", facts_ids[i])\n",
    "        # for i, s in enumerate(sentences):\n",
    "        #     print(f\"{i}: type={type(s).__name__}, {s}\")\n",
    "            \n",
    "        inputs = c_tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        embeds = contriever(**inputs)\n",
    "        sent_embeds.append(embeds[:-1])\n",
    "        query_embeds.append(embeds[-1])\n",
    "        \n",
    "        #print('===================')\n",
    "    #print(\"DONE\")\n",
    "    return dict(query=query_embeds, sentences=sent_embeds, facts_ids=facts_ids) \n",
    "\n",
    "#type(collated[\"question\"][0])\n",
    "res = get_contriever_embeds(collated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 1.00, recall: 0.07\n",
      "precision: 0.98, recall: 0.33\n",
      "precision: 0.90, recall: 0.61\n"
     ]
    }
   ],
   "source": [
    "#sum([s.shape[0] for s in res['sentences']])\n",
    "stats = compite_statistics(res, num_retr=1)\n",
    "stats = compite_statistics(res, num_retr=5)\n",
    "stats = compite_statistics(res, num_retr=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Contriever Similiarity Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[48;2;191;128;191mWhat a nice red background!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def monocolor(v, text):\n",
    "    r=127 + int(v*128) \n",
    "    b=127 + int(128*(1-v))\n",
    "    g=128\n",
    "    #print(f\"{r},{g},{b}\")\n",
    "    return colored_background(r, g, b, text)\n",
    "    \n",
    "def colored_background(r, g, b, text):\n",
    "    return f'\\033[48;2;{r};{g};{b}m{text}\\033[0m'\n",
    "\n",
    "text = \"What a nice red background!\"\n",
    "colored_text = colored_background(255, 0, 0, text)\n",
    "colored_text = monocolor(0.5, text)\n",
    "print(colored_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contriever-based similarity score between question and Babilong context\n",
      "---------------------------------\n",
      "QUESTION: Where is the football? \n",
      "---------------------------------\n",
      "TOP 25 sentences sorted from highest to lowest similarity score:\n",
      "0.961, \u001b[48;2;150;255;150m[F]\u001b[0m: \u001b[48;2;127;128;255mMary got the football there.\u001b[0m\n",
      "0.682, \u001b[48;2;150;255;150m[F]\u001b[0m: \u001b[48;2;193;128;188mMary dropped the football.\u001b[0m\n",
      "0.578, \u001b[48;2;150;255;150m[F]\u001b[0m: \u001b[48;2;218;128;163mJohn got the milk there.\u001b[0m\n",
      "0.572, \u001b[48;2;150;255;150m[F]\u001b[0m: \u001b[48;2;219;128;162mMary went back to the kitchen.\u001b[0m\n",
      "0.562, \u001b[48;2;150;255;150m[F]\u001b[0m: \u001b[48;2;221;128;160mSandra went back to the office.\u001b[0m\n",
      "0.561, \u001b[48;2;150;255;150m[F]\u001b[0m: \u001b[48;2;221;128;160mMary moved to the bathroom.\u001b[0m\n",
      "0.555, \u001b[48;2;150;255;150m[F]\u001b[0m: \u001b[48;2;223;128;158mMary went back to the garden.\u001b[0m\n",
      "0.547, \u001b[48;2;150;255;150m[F]\u001b[0m: \u001b[48;2;225;128;156mJohn went to the kitchen.\u001b[0m\n",
      "0.544, \u001b[48;2;200;200;200m[N]\u001b[0m: \u001b[48;2;226;128;155mIs it not too much for the strongest constitution to endure?\u001b[0m\n",
      "0.544, \u001b[48;2;150;255;150m[F]\u001b[0m: \u001b[48;2;226;128;155mDaniel went back to the kitchen.\u001b[0m\n",
      "0.541, \u001b[48;2;150;255;150m[F]\u001b[0m: \u001b[48;2;226;128;155mSandra journeyed to the hallway.\u001b[0m\n",
      "0.538, \u001b[48;2;200;200;200m[N]\u001b[0m: \u001b[48;2;227;128;154ma service which the society has no right to demand from any of its members?\u001b[0m\n",
      "0.512, \u001b[48;2;150;255;150m[F]\u001b[0m: \u001b[48;2;233;128;148mSandra journeyed to the bedroom.\u001b[0m\n",
      "0.496, \u001b[48;2;200;200;200m[N]\u001b[0m: \u001b[48;2;237;128;144mSuppose, for example--and it is surely not wholly an imaginary danger I foresee--suppose that some day some event should...\u001b[0m\n",
      "0.485, \u001b[48;2;200;200;200m[N]\u001b[0m: \u001b[48;2;240;128;141mHave they left themselves any epithet in reserve capable of expressing their sensations at all adequately?\u001b[0m\n",
      "0.481, \u001b[48;2;200;200;200m[N]\u001b[0m: \u001b[48;2;240;128;141mAnd so, night after night, the authors of these headlines harrow themselves by announcing such items as \"Blank protests ...\u001b[0m\n",
      "0.470, \u001b[48;2;200;200;200m[N]\u001b[0m: \u001b[48;2;243;128;138mNow this loss of the sense of proportion in human affairs, Sir, is a very bad sign, and a well-nigh infallible indicator...\u001b[0m\n",
      "0.468, \u001b[48;2;200;200;200m[N]\u001b[0m: \u001b[48;2;244;128;137m\"Distressing Scene on the Scaffold.\"\u001b[0m\n",
      "0.465, \u001b[48;2;200;200;200m[N]\u001b[0m: \u001b[48;2;244;128;137mThere is clearly a profound conviction that the jury who heard the evidence, the judge who pronounced their verdict of g...\u001b[0m\n",
      "0.460, \u001b[48;2;200;200;200m[N]\u001b[0m: \u001b[48;2;246;128;135mBut I find a yet more unmistakable evidence in support of my contention in the extraordinary emotional sensibility revea...\u001b[0m\n",
      "0.454, \u001b[48;2;200;200;200m[N]\u001b[0m: \u001b[48;2;247;128;134mYes, Sir, whether these devoted servants of the public know it or not, they are running a most frightful risk; the word ...\u001b[0m\n",
      "0.441, \u001b[48;2;150;255;150m[F]\u001b[0m: \u001b[48;2;250;128;131mJohn moved to the office.\u001b[0m\n",
      "0.437, \u001b[48;2;200;200;200m[N]\u001b[0m: \u001b[48;2;251;128;130mThe convict has said to himself, and that seems to be considered sufficient.\u001b[0m\n",
      "0.430, \u001b[48;2;200;200;200m[N]\u001b[0m: \u001b[48;2;253;128;128mThey have not; they have squandered participles and adjectives in such reckless profusion that they will discover they a...\u001b[0m\n",
      "0.423, \u001b[48;2;200;200;200m[N]\u001b[0m: \u001b[48;2;255;128;127mConsider the strain of all these alterations of hope and despair, repeated time after time, and almost invariably withou...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def print_sorted_sentences_scores(collated, idx):\n",
    "    sentences = []\n",
    "    sentences.extend(collated['facts'][idx])\n",
    "    facts_ids = np.arange(len(sentences))\n",
    "    sentences.extend(collated['background_text'][idx])\n",
    "    sentences.append(collated['question'][idx]) # append as this is a single str\n",
    "    \n",
    "    inputs = c_tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    embeds = contriever(**inputs)\n",
    "    sent_embeds = embeds[:-1]\n",
    "    query_embeds = embeds[-1]\n",
    "    \n",
    "    scores = torch.inner(query_embeds, sent_embeds)        \n",
    "    print(\"Contriever-based similarity score between question and Babilong context\")\n",
    "    #print(scores)\n",
    "    \n",
    "    normalized_scores = (scores-scores.min())/(scores.max()-scores.min())\n",
    "    #normalized_scores = normalized_scores.tolist()\n",
    "    print(\"---------------------------------\")\n",
    "    print(\"QUESTION:\", sentences[-1])\n",
    "    print(\"---------------------------------\")\n",
    "    print(f\"TOP {min(len(sentences)-1, 25)} sentences sorted from highest to lowest similarity score:\")\n",
    "    for i in reversed(torch.argsort(normalized_scores)[-25:].tolist()):\n",
    "        norm_score = normalized_scores[i]\n",
    "        texttype = colored_background(150, 255, 150, \"[F]\") if i in facts_ids else colored_background(200, 200, 200, \"[N]\")\n",
    "        sent = sentences[i].replace(\"\\n\", \" \")\n",
    "        if len(sent) > 120:\n",
    "            sent = sent[:120] + \"...\"\n",
    "        print(f\"{scores[i]:.3f}, {texttype:>8s}: {monocolor(1-norm_score, sent)}\")\n",
    "\n",
    "\n",
    "print_sorted_sentences_scores(collated, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'labels', 'input_ids_generate', 'labels_mask', 'attention_mask', 'attention_mask_generate', 'target_text'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dl = DataLoader(batch_size=2, dataset=dataset_train, collate_fn=collate_fn)\n",
    "gen = iter(dl)\n",
    "batch = next(gen)\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'contriever'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcontriever\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Contriever\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'contriever'"
     ]
    }
   ],
   "source": [
    "from contriever import Contriever\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
