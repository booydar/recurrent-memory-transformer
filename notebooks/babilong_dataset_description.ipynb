{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "libgomp: Invalid value for environment variable OMP_NUM_THREADS\n",
      "\n",
      "libgomp: Invalid value for environment variable OMP_NUM_THREADS\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets\n",
    "import os\n",
    "import re\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import nltk.data\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess selected babi text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_df(dataset_path):\n",
    "    with open(dataset_path, 'r') as f:\n",
    "        texts = f.read().strip()\n",
    "        texts = texts.split('\\n')\n",
    "        df = pd.DataFrame(texts, columns=['text'])\n",
    "\n",
    "    # parse samples\n",
    "    df['phrase_num'] = df.text.apply(lambda x: int(x.split(' ')[0]))\n",
    "    df.text = df.text.apply(lambda x: x[x.index(' ') + 1:])\n",
    "    df['answer'] = df.text.apply(lambda x: x[x.index('\\t') + 1:] if '\\t' in x else None)\n",
    "    # df['reference_num'] = df.answer.apply(lambda x: x if x is None else x.split('\\t| ')[1:])\n",
    "    df['reference_num'] = df.answer.apply(lambda x: x if x is None else [int(n) for n in re.split('\\t| ', x)[1:]])\n",
    "    df.answer = df.answer.apply(lambda x: x if x is None else x.split('\\t')[0])\n",
    "    df.text = df.text.apply(lambda x: x.split('\\t')[0] if '\\t' in x else x)\n",
    "\n",
    "    # mark each sample\n",
    "    sample_start_inds = list(np.where(df.phrase_num == 1)[0]) + [df.shape[0]]\n",
    "    for i, (start, end) in enumerate(zip(sample_start_inds, sample_start_inds[1:])):\n",
    "        df.loc[start:end, 'initial_sample_num'] = i\n",
    "\n",
    "    df.initial_sample_num = df.initial_sample_num.astype(int)\n",
    "\n",
    "    # multiple questions in sample -> samples with single question\n",
    "    initial_samples = [df[df.initial_sample_num == sn] for sn in df.initial_sample_num.unique()]\n",
    "\n",
    "    single_question_slices = []\n",
    "    for sample in initial_samples:\n",
    "        answer_positions = sample[~sample.answer.isna()].index\n",
    "        slices = [sample[:ans_pos+1] for ans_pos in answer_positions]\n",
    "        for i, slc in enumerate(slices):\n",
    "            slices[i] = slc[(slc.answer.isna()) | (slc.index == slc.index[-1])]\n",
    "        single_question_slices += slices\n",
    "    \n",
    "    df = pd.concat(single_question_slices).reset_index(drop=True)\n",
    "\n",
    "    # mark each sample again\n",
    "    sample_start_inds = list(np.where(df.phrase_num == 1)[0]) + [df.shape[0]]\n",
    "    for i, (start, end) in enumerate(zip(sample_start_inds, sample_start_inds[1:])):\n",
    "        df.loc[start:end, 'sample_num'] = i\n",
    "\n",
    "    df.sample_num = df.sample_num.astype(int)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = datasets.load_dataset(\"facebook/babi_qa\", \"en-10k-qa1\") # fails for me\n",
    "train_path = \"/home/bulatov/datasets/babi/tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_train.txt\"\n",
    "test_path = \"/home/bulatov/datasets/babi/tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_test.txt\"\n",
    "noise_dataset = datasets.load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '/home/bulatov/datasets/babi/tasks_1-20_v1-2/en-10k'\n",
    "fns = next(os.walk(data_folder))[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test that all 20 babi datasets can be opened\n",
    "i = 0\n",
    "for i in range(len(fns)):\n",
    "    fn = fns[i]\n",
    "    dataset_path = os.path.join(data_folder, fn)\n",
    "\n",
    "    df = get_dataset_df(dataset_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task dataset\n",
    "Each sample is one task that contains facts, question, answer and reference facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskDataset(Dataset):\n",
    "    def __init__(self, dataset_path):\n",
    "        self.fact_dataset = get_dataset_df(dataset_path)\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        slc = self.fact_dataset[self.fact_dataset.sample_num == ind]\n",
    "        references = slc[slc.phrase_num.isin(slc.reference_num.values[-1])].text.values\n",
    "        sample = {'facts': slc.text.values[:-1],\n",
    "                  'question': slc.text.values[-1],\n",
    "                  'answer': slc.answer.values[-1],\n",
    "                  'references': references}\n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.fact_dataset.sample_num.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/home/jovyan/rmt/datasets/babi/data/tasks_1-20_v1-2/en-10k/qa3_three-supporting-facts_test.txt\"\n",
    "task_dataset = TaskDataset(dataset_path)\n",
    "sample = task_dataset[10]\n",
    "# sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise dataset\n",
    "Noise dataset samples background text by sentences.\n",
    "\n",
    "Sample_size is defined in tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokenizer = nltk.PunktSentenceTokenizer()\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_lengths(sentences):\n",
    "    return sum([len(s) for s in sentences])\n",
    "\n",
    "\n",
    "class SentenceSampler:\n",
    "    def __init__(self, dataset, tokenizer):\n",
    "        self.sample_ind = 0\n",
    "        self.dataset = dataset\n",
    "        self.sentences = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sentence_tokenizer = nltk.PunktSentenceTokenizer()\n",
    "\n",
    "    def get_sample(self, sample_size):\n",
    "        total_tokens = sum_lengths(self.sentences)\n",
    "        while total_tokens < sample_size: # add a new dataset item\n",
    "            text = self.dataset[self.sample_ind]['text']\n",
    "            self.sample_ind += 1\n",
    "            sentences = self.sentence_tokenizer.tokenize(text)\n",
    "            tokenized = [self.tokenizer.encode(s, add_special_tokens=False) for s in sentences]\n",
    "            self.sentences += tokenized\n",
    "            total_tokens += sum_lengths(tokenized)\n",
    "\n",
    "        sample = []\n",
    "        sample_tokens = 0\n",
    "        for sent in self.sentences: # add new sentence until sample_size is reached\n",
    "            sample_tokens += len(sent)\n",
    "            if sample_tokens >= sample_size:\n",
    "                break\n",
    "            sample.append(sent)\n",
    "            self.sentences = self.sentences[1:]\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_sampler = SentenceSampler(noise_dataset['train'], tokenizer=tokenizer)\n",
    "\n",
    "class NoiseInjectionDataset(Dataset):\n",
    "    def __init__(self, task_dataset, noise_sampler, tokenizer, sample_size=100):\n",
    "        self.task_dataset = task_dataset\n",
    "        self.noise_sampler = noise_sampler\n",
    "        self.sample_size = sample_size\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        sample = self.task_dataset[ind]\n",
    "        facts_tok = self.tokenizer(list(sample['facts']))['input_ids']\n",
    "        question_tok = self.tokenizer(sample['question'])['input_ids']\n",
    "        answer_tok = self.tokenizer(sample['answer'])['input_ids']\n",
    "        \n",
    "        task_len = sum_lengths(facts_tok) + len(question_tok) + len(answer_tok)\n",
    "        background_text_len = self.sample_size - task_len\n",
    "        background_text = self.noise_sampler.get_sample(background_text_len)\n",
    "        sample['background_text'] = background_text\n",
    "\n",
    "        possible_positions = range(len(background_text) + 1) \n",
    "        fact_positions = np.random.choice(possible_positions, len(facts_tok))\n",
    "        fact_positions.sort()\n",
    "\n",
    "        updated_sample = [[] for _ in range(len(background_text) + 1)] \n",
    "        for fact, pos in zip(facts_tok, fact_positions):\n",
    "            updated_sample[pos].append(fact)\n",
    "\n",
    "        updated_sample[-1].append(question_tok)\n",
    "\n",
    "        for i, s in enumerate(background_text):\n",
    "            updated_sample[i].append(s)\n",
    "\n",
    "        flat = [i for s in updated_sample for i in s]\n",
    "        tokens = [i for s in flat for i in s]\n",
    "\n",
    "        sample['input_tokens'] = tokens\n",
    "        sample['target_tokens'] = answer_tok\n",
    "\n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.task_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 512\n",
    "noise_injection_dataset = NoiseInjectionDataset(task_dataset=task_dataset,\n",
    "                                                noise_sampler=noise_sampler,\n",
    "                                                tokenizer=tokenizer,\n",
    "                                                sample_size=sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize one sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['facts', 'question', 'answer', 'references', 'background_text', 'input_tokens', 'target_tokens'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = noise_injection_dataset[10]\n",
    "sample.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Facts: Sandra moved to the bathroom. John picked up the football. John dropped the football. Daniel went to the kitchen. Daniel got the apple. John went to the kitchen. Mary went back to the hallway. Daniel travelled to the office. John travelled to the garden. John journeyed to the kitchen. Mary moved to the kitchen. Daniel moved to the garden. Mary journeyed to the bathroom. Daniel grabbed the milk. Mary went to the hallway. Mary got the football there. Mary dropped the football. Sandra moved to the bedroom. Mary went back to the office. John travelled to the office. Mary went back to the bathroom. John moved to the hallway. Sandra went to the office. Mary journeyed to the kitchen. Sandra travelled to the garden. John went back to the garden. Sandra went back to the office. Mary went to the hallway. Daniel went to the bedroom. Mary picked up the football there. John travelled to the kitchen. Mary moved to the bedroom. Sandra went to the bathroom. Daniel put down the milk. Daniel discarded the apple. Mary took the milk. Daniel went to the office. Mary dropped the milk. Mary discarded the football. John went back to the garden. Sandra went to the bedroom. Daniel travelled to the bathroom. Sandra picked up the milk. Sandra moved to the hallway. Sandra dropped the milk. Sandra took the milk. Sandra discarded the milk. Sandra got the milk. Mary got the football. Sandra journeyed to the garden. Daniel went back to the hallway. John moved to the kitchen. Sandra went back to the bathroom. Mary got the apple. Daniel moved to the kitchen. Mary put down the football. John journeyed to the garden. Daniel went to the bedroom. Mary left the apple. Daniel took the apple. John moved to the hallway. Sandra put down the milk. Daniel dropped the apple there. Sandra moved to the kitchen. Mary journeyed to the office. John travelled to the office.\n",
      "Question: Where was the milk before the bathroom? \n",
      "Answer: garden\n",
      "References: Sandra journeyed to the garden. Sandra went back to the bathroom. Sandra put down the milk.\n",
      "\n",
      "Background text:   = Valkyria Chronicles III =  Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3, lit. Valkyria of the Battlefield 3 ), commonly referred to as Valkyria Chronicles III outside Japan, is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable.\n",
      "Combined input:  Sandra moved to the bathroom.John picked up the football.John dropped the football.Daniel went to the kitchen.Daniel got the apple.John went to the kitchen.Mary went back to the hallway.Daniel travelled to the office.John travelled to the garden.John journeyed to the kitchen.Mary moved to the kitchen.Daniel moved to the garden.Mary journeyed to the bathroom.Daniel grabbed the milk.Mary went to the hallway.Mary got the football there. = Valkyria Chronicles III =Mary dropped the football.Sandra moved to the bedroom.Mary went back to the office.John travelled to the office.Mary went back to the bathroom.John moved to the hallway.Sandra went to the office.Mary journeyed to the kitchen.Sandra travelled to the garden.John went back to the garden.Sandra went back to the office.Mary went to the hallway.Daniel went to the bedroom.Mary picked up the football there.John travelled to the kitchen.Mary moved to the bedroom. Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3, lit.Sandra went to the bathroom.Daniel put down the milk.Daniel discarded the apple.Mary took the milk.Daniel went to the office.Mary dropped the milk.Mary discarded the football.John went back to the garden.Sandra went to the bedroom.Daniel travelled to the bathroom.Sandra picked up the milk.Sandra moved to the hallway.Sandra dropped the milk.Sandra took the milk.Sandra discarded the milk.Sandra got the milk.Valkyria of the Battlefield 3 ), commonly referred to as Valkyria Chronicles III outside Japan, is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable.Mary got the football.Sandra journeyed to the garden.Daniel went back to the hallway.John moved to the kitchen.Sandra went back to the bathroom.Mary got the apple.Daniel moved to the kitchen.Mary put down the football.John journeyed to the garden.Daniel went to the bedroom.Mary left the apple.Daniel took the apple.John moved to the hallway.Sandra put down the milk.Daniel dropped the apple there.Sandra moved to the kitchen.Mary journeyed to the office.John travelled to the office.Where was the milk before the bathroom? \n",
      "Target: garden\n"
     ]
    }
   ],
   "source": [
    "facts = sample['facts']\n",
    "question = sample['question']\n",
    "answer = tokenizer.decode(sample['target_tokens'])\n",
    "\n",
    "background_text = tokenizer.batch_decode(sample['background_text'])\n",
    "\n",
    "input_tokens = tokenizer.decode(sample['input_tokens'])\n",
    "\n",
    "print(f\"Facts: {' '.join(facts)}\")\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")\n",
    "print(f\"References: {' '.join(sample['references'])}\")\n",
    "print()\n",
    "print('Background text: ', ' '.join(background_text))\n",
    "print('Combined input: ', input_tokens)\n",
    "print(f\"Target: {answer}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "accel_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
