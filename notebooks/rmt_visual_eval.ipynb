{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/griver/anaconda3/envs/rmt/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "sys.path.append(\"..\")\n",
    "from babilong_utils import TaskDataset, SentenceSampler, NoiseInjectionDataset\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### extract dataset archive\n",
    "# !unzip ../data/tasks_1-20_v1-2.zip -d ../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qa10_indefinite-knowledge_test.txt   qa1_single-supporting-fact_test.txt\n",
      "qa10_indefinite-knowledge_train.txt  qa1_single-supporting-fact_train.txt\n",
      "qa11_basic-coreference_test.txt      qa20_agents-motivations_test.txt\n",
      "qa11_basic-coreference_train.txt     qa20_agents-motivations_train.txt\n",
      "qa12_conjunction_test.txt\t     qa2_two-supporting-facts_test.txt\n",
      "qa12_conjunction_train.txt\t     qa2_two-supporting-facts_train.txt\n",
      "qa13_compound-coreference_test.txt   qa3_three-supporting-facts_test.txt\n",
      "qa13_compound-coreference_train.txt  qa3_three-supporting-facts_train.txt\n",
      "qa14_time-reasoning_test.txt\t     qa4_two-arg-relations_test.txt\n",
      "qa14_time-reasoning_train.txt\t     qa4_two-arg-relations_train.txt\n",
      "qa15_basic-deduction_test.txt\t     qa5_three-arg-relations_test.txt\n",
      "qa15_basic-deduction_train.txt\t     qa5_three-arg-relations_train.txt\n",
      "qa16_basic-induction_test.txt\t     qa6_yes-no-questions_test.txt\n",
      "qa16_basic-induction_train.txt\t     qa6_yes-no-questions_train.txt\n",
      "qa17_positional-reasoning_test.txt   qa7_counting_test.txt\n",
      "qa17_positional-reasoning_train.txt  qa7_counting_train.txt\n",
      "qa18_size-reasoning_test.txt\t     qa8_lists-sets_test.txt\n",
      "qa18_size-reasoning_train.txt\t     qa8_lists-sets_train.txt\n",
      "qa19_path-finding_test.txt\t     qa9_simple-negation_test.txt\n",
      "qa19_path-finding_train.txt\t     qa9_simple-negation_train.txt\n"
     ]
    }
   ],
   "source": [
    "!ls ../data/tasks_1-20_v1-2/en-10k/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"qa2_two-supporting-facts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/griver/anaconda3/envs/rmt/lib/python3.9/site-packages/datasets/load.py:1429: FutureWarning: The repository for pg19 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/pg19\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_path =f\"../data/tasks_1-20_v1-2/en-10k/{task}_train.txt\"\n",
    "test_path = f\"../data/tasks_1-20_v1-2/en-10k/{task}_test.txt\"\n",
    "noise_dataset_name = \"pg19\"\n",
    "noise_dataset = datasets.load_dataset(noise_dataset_name)\n",
    "noise_dataset_train = noise_dataset['train']\n",
    "noise_dataset_test = noise_dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load task datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task \n",
    "task_dataset_train = TaskDataset(train_path)\n",
    "task_dataset_test = TaskDataset(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# background text\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "noise_sampler_train = SentenceSampler(noise_dataset['train'], tokenizer=tokenizer, shuffle=True, random_seed=None)\n",
    "noise_sampler_test = SentenceSampler(noise_dataset['test'], tokenizer=tokenizer, shuffle=True, random_seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = [460] # taken from config file where sample size is 480, 460 = 480 - 20\n",
    "train_dataset = NoiseInjectionDataset(task_dataset=task_dataset_train,\n",
    "                                        noise_sampler=noise_sampler_train,\n",
    "                                        tokenizer=tokenizer,\n",
    "                                        sample_size=sample_size\n",
    "                                     )\n",
    "\n",
    "test_dataset = NoiseInjectionDataset(task_dataset=task_dataset_test,\n",
    "                                        noise_sampler=noise_sampler_test,\n",
    "                                        tokenizer=tokenizer,\n",
    "                                        sample_size=sample_size[0],\n",
    "                                        mixed_length_ratio=0.0,\n",
    "                                        task_start_pct=None,\n",
    "                                        task_end_pct=None\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['facts', 'question', 'answer', 'references', 'background_text', 'fact_positions', 'input_tokens', 'question_tokens', 'target_tokens'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = train_dataset[0]\n",
    "sample.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary moved to the bathroom.\n",
      "Sandra journeyed to the bedroom.\n",
      "Mary got the football there.\n",
      "John went to the kitchen.\n",
      "Mary went back to the kitchen.\n",
      "Mary went back to the garden.\n",
      "fact position: [3 5]\n",
      "question: Where is the football? \n",
      "\n",
      "BACKGROUND:\n",
      "'Dear Sir,\n",
      "\n",
      "    I am much obliged to you for the compliment you make me in\n",
      "    thinking my approbation of any value, to tell you the truth the\n",
      "    reason of my setting so little value on it myself, proceeds not so\n",
      "    much from modesty, or an opinion that I cannot feel the powers of\n",
      "    Poetry, or distinguish beauties from defects, but from a\n",
      "    consciousness that I am unable to determine (as all excellence in\n",
      "    comparative) what rank it ought to hold in the scale of Art; and\n",
      "    this judgement can be possess'd I think by those only who are\n",
      "    acquainted with what the world has produced of that kind.',\n",
      "'I have lately had the pleasure of reading your Poem to several\n",
      "    friends, who have spoken much in its commendation, and Mr.',\n",
      "'Johnson\n",
      "    who is as severe a Critic as old Dennis approves of it very much,\n",
      "    he thinks it superior to any Poem of the kind that has been\n",
      "    publish'd these many years and will venture to lay a wager that\n",
      "    there is not a better publish'd this year or the next.',\n",
      "'The Characters of the several Masters mention'd in the Poem are\n",
      "    truly drawn; and the descriptions of the several kinds of History\n",
      "    Painting shew great imagination and a thorough knowledge of the\n",
      "    Theory of the Art, and that this is deliver'd in Poetry much above\n",
      "    the common standard I have Mr.',\n",
      "'Johnson's word who concluded his\n",
      "    commendation with Imprimatur meo periculo which order if you have\n",
      "    no objection we will immediately put in execution.',\n",
      "'I have scarce left room to subscribe myself\n",
      "\n",
      "      Yours,\n",
      "\n",
      "      J. Reynolds.',\n",
      "'There is no record of any copy of the poem, either printed or\n",
      "manuscript, having been at Yeo Vale; but that the order had indeed been\n",
      "put in execution became apparent lately when Professor Hill',\n"
     ]
    }
   ],
   "source": [
    "for f in sample['facts']:\n",
    "    print(f)\n",
    "print(\"fact position:\", sample['fact_positions'])\n",
    "print(\"question:\", sample['question'])\n",
    "print(\"\\nBACKGROUND:\")\n",
    "\n",
    "background_text = tokenizer.batch_decode(sample['background_text'][:20])\n",
    "for s in background_text:\n",
    "    print(f'\\'{s}\\',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize one sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['facts', 'question', 'answer', 'references', 'background_text', 'fact_positions', 'input_tokens', 'question_tokens', 'target_tokens'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Facts: Mary moved to the bathroom. Sandra journeyed to the bedroom. Mary got the football there. John went to the kitchen. Mary went back to the kitchen. Mary went back to the garden.\n",
      "Question: Where is the football? \n",
      "Answer: garden\n",
      "References: Mary got the football there. Mary went back to the garden.\n",
      "\n",
      "Fact positions:  [3 5]\n",
      "Combined input:  Dear Sir,\n",
      "\n",
      "    I am much obliged to you for the compliment you make me in\n",
      "    thinking my approbation of any value, to tell you the truth the\n",
      "    reason of my setting so little value on it myself, proceeds not so\n",
      "    much from modesty, or an opinion that I cannot feel the powers of\n",
      "    Poetry, or distinguish beauties from defects, but from a\n",
      "    consciousness that I am unable to determine (as all excellence in\n",
      "    comparative) what rank it ought to hold in the scale of Art; and\n",
      "    this judgement can be possess'd I think by those only who are\n",
      "    acquainted with what the world has produced of that kind.I have lately had the pleasure of reading your Poem to several\n",
      "    friends, who have spoken much in its commendation, and Mr.Johnson\n",
      "    who is as severe a Critic as old Dennis approves of it very much,\n",
      "    he thinks it superior to any Poem of the kind that has been\n",
      "    publish'd these many years and will venture to lay a wager that\n",
      "    there is not a better publish'd this year or the next.Mary got the football there.The Characters of the several Masters mention'd in the Poem are\n",
      "    truly drawn; and the descriptions of the several kinds of History\n",
      "    Painting shew great imagination and a thorough knowledge of the\n",
      "    Theory of the Art, and that this is deliver'd in Poetry much above\n",
      "    the common standard I have Mr.Johnson's word who concluded his\n",
      "    commendation with Imprimatur meo periculo which order if you have\n",
      "    no objection we will immediately put in execution.Mary went back to the garden.I have scarce left room to subscribe myself\n",
      "\n",
      "      Yours,\n",
      "\n",
      "      J. Reynolds.There is no record of any copy of the poem, either printed or\n",
      "manuscript, having been at Yeo Vale; but that the order had indeed been\n",
      "put in execution became apparent lately when Professor Hill\n",
      "Target: garden\n"
     ]
    }
   ],
   "source": [
    "facts = sample['facts']\n",
    "question = sample['question']\n",
    "answer = tokenizer.decode(sample['target_tokens'])\n",
    "\n",
    "#background_text = sample['background_text']\n",
    "\n",
    "input_tokens = tokenizer.decode(sample['input_tokens'])\n",
    "\n",
    "print(f\"Facts: {' '.join(facts)}\")\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")\n",
    "print(f\"References: {' '.join(sample['references'])}\")\n",
    "print()\n",
    "#print('Background text: ', ' '.join(background_text))\n",
    "print('Fact positions: ', sample['fact_positions'])\n",
    "print('Combined input: ', input_tokens)\n",
    "\n",
    "print(f\"Target: {answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eos token: 50256 gen token: 35353 id_pad_value: 50256\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "id_pad_value = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "gen_token = tokenizer.encode('GEN')[0]\n",
    "eos_token = tokenizer.eos_token_id\n",
    "print(\"eos token:\", eos_token, \"gen token:\", gen_token, \"id_pad_value:\", id_pad_value)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    targets = [torch.tensor(b['target_tokens']) for b in batch]\n",
    "    input_ids = [torch.tensor(b['input_tokens'] + [gen_token] + b['target_tokens'] + [eos_token]) for b in batch]\n",
    "    gen_inputs = [torch.tensor(b['input_tokens'] + [gen_token]) for b in batch]\n",
    "\n",
    "    attention_mask = [torch.ones_like(b, dtype=int) for b in input_ids]\n",
    "    labels_mask = [torch.zeros_like(b, dtype=bool) for b in input_ids]\n",
    "    for m, t in zip(labels_mask, targets):\n",
    "        m[-len(t) - 2:] = True\n",
    "\n",
    "    input_ids = pad_sequence(input_ids, padding_value=id_pad_value, batch_first=True)\n",
    "    gen_inputs = pad_sequence(gen_inputs, padding_value=id_pad_value, batch_first=True)\n",
    "    # labels = pad_sequence(input_ids, padding_value=-100, batch_first=True)\n",
    "    attention_mask = pad_sequence(attention_mask, padding_value=0, batch_first=True)\n",
    "    labels_mask = pad_sequence(labels_mask, padding_value=0, batch_first=True)\n",
    "\n",
    "    collated = {}\n",
    "    collated['input_ids'] = collated['labels'] = input_ids\n",
    "    collated['input_ids_generate'] = gen_inputs\n",
    "    collated['labels_mask'] = labels_mask\n",
    "    collated['attention_mask'] = attention_mask.bool()\n",
    "    collated['attention_mask_generate'] = (gen_inputs != id_pad_value).bool()\n",
    "\n",
    "    collated['target_text'] = [b['answer'] for b in batch]\n",
    "    \n",
    "    collated['background_text'] = [b['background_text'] for b in batch]\n",
    "    collated['facts'] = [b['facts'] for b in batch]\n",
    "    collated['question'] = [b['question'] for b in batch]\n",
    "    \n",
    "    return collated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[24119, 1392, 262, 7545, 612, 13], [7554, 3888, 284, 262, 14043, 13], [50, 15918, 1816, 736, 284, 262, 9592, 13], [24119, 21650, 284, 262, 23959, 13]]\n",
      "['Mary got the milk there.' 'John moved to the bedroom.'\n",
      " 'Sandra went back to the kitchen.' 'Mary travelled to the hallway.']\n"
     ]
    }
   ],
   "source": [
    "def example():\n",
    "    batch = [test_dataset[0]]\n",
    "    facts_tokens = [tokenizer(list(b['facts']))['input_ids'] for b in batch]\n",
    "    print(facts_tokens[0])\n",
    "    print(batch[0]['facts'])\n",
    "\n",
    "example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = [dataset_test[i] for i in range(10)]\n",
    "# collated = collate_fn(batch)\n",
    "# collated.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #labels are marked with labels_mask\n",
    "# tokenizer.batch_decode([c[m][1:-1] for c, m in zip(collated['input_ids'], collated['labels_mask'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different input_ids for .forward() and .generate()\n",
    "#tokenizer.batch_decode([c[m] for c, m in zip(collated['input_ids'], collated['attention_mask'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.batch_decode([c[m] for c, m in zip(collated['input_ids_generate'], collated['attention_mask_generate'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset wrappers from the `run_finetuning_babilong_rmt.py` script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "kwargs = {'pin_memory': True, 'num_workers': 1, 'collate_fn': collate_fn}\n",
    "per_worker_batch_size = 1\n",
    "seed=43\n",
    "\n",
    "train_sampler = DistributedSampler(train_dataset, rank=0, num_replicas=1, shuffle=True, drop_last=True, seed=43)\n",
    "test_sampler = DistributedSampler(test_dataset, rank=0, num_replicas=1, drop_last=False, shuffle=False)\n",
    "train_dataloader = DataLoader(batch_size=per_worker_batch_size, dataset=train_dataset, sampler=train_sampler, **kwargs)\n",
    "test_dataloader = DataLoader(batch_size=per_worker_batch_size, dataset=test_dataset, sampler=test_sampler, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Pretrained RMT on QA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling_rmt.language_modeling import MemoryCell\n",
    "from modeling_rmt.language_modeling import RecurrentWrapper\n",
    "from transformers import AutoModelForCausalLM\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "memory_cell_cls = \"modeling_rmt.language_modeling:MemoryCell\"\n",
    "recurrent_wrapper_cls = \"modeling_rmt.language_modeling:RecurrentWrapper\"\n",
    "model_cls = \"transformers:AutoModelForCausalLM\"\n",
    "pretrained_rmt_path = \"../../runs/server/babilong_checkpoints/qa2_test/run_4/model_best\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def load_pretrained_rmt(pretrained_rmt_path):\n",
    "    num_memory_tokens = 16\n",
    "    from_pretrained_base = \"gpt2\"\n",
    "    segment_size = 512\n",
    "    max_n_segments = 1\n",
    "    model = AutoModelForCausalLM.from_pretrained(from_pretrained_base, use_safetensors=False)\n",
    "    cell = MemoryCell(model, num_memory_tokens)\n",
    "    \n",
    "    model = RecurrentWrapper(cell, segment_size=segment_size, max_n_segments=max_n_segments, k2=-1)\n",
    "    model_cpt = os.path.join(pretrained_rmt_path, \"pytorch_model.bin\")\n",
    "    cpt = torch.load(model_cpt, map_location='cpu')\n",
    "    model.load_state_dict(cpt, strict=False)\n",
    "    return model\n",
    "\n",
    "rmt = load_pretrained_rmt(pretrained_rmt_path).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def keep_for_metrics_fn(batch, output):\n",
    "    # select data from batch and model output that would be used to compute metrics\n",
    "    data = {}\n",
    "    data['labels'] = batch['labels']\n",
    "    data['loss'] = output['loss']\n",
    "    data['target_text'] = batch['target_text']\n",
    "    if 'logits' in output:\n",
    "        data['predictions'] = torch.argmax(output['logits'].detach(), dim=-1)\n",
    "        data['predicted_labels'] = [p[m] for p, m in zip(data['predictions'], batch['labels_mask'])]\n",
    "    if 'generation_outputs' in output:\n",
    "        data['generation_outputs'] = output['generation_outputs']\n",
    "    return data\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def metrics_fn(data, add_generation=True):\n",
    "    # compute metrics based on stored labels, predictions, ...\n",
    "    metrics = {}\n",
    "    if 'generation_outputs' in data:\n",
    "        generation_outputs = tokenizer.batch_decode([d for d in data['generation_outputs']], add_special_tokens=False)\n",
    "        for i, o in enumerate(generation_outputs):\n",
    "            if '<|endoftext|>' in o:\n",
    "                # print(f\"gt: {data['target_text'][i]}, generated {o}\")\n",
    "                generation_outputs[i] = o.split('<|endoftext|>')[1].strip()\n",
    "\n",
    "        metrics['exact_match'] = np.mean([text == pred for text, pred in zip (data['target_text'], generation_outputs)])\n",
    "\n",
    "    elif 'predictions' in data:\n",
    "        y, p = data['labels'], data['predictions']\n",
    "        predicted_labels = tokenizer.batch_decode(data['predicted_labels'], add_special_tokens=False)\n",
    "        for i, l in enumerate(predicted_labels):\n",
    "            if '<|endoftext|>' in l:\n",
    "                eos_ind = predicted_labels[i].index('<|endoftext|>')\n",
    "                predicted_labels[i] = predicted_labels[i][:eos_ind]\n",
    "                \n",
    "        metrics['exact_match'] = np.mean([text == pred for text, pred in zip (data['target_text'], predicted_labels)])\n",
    "        if args.show_valid_examples > 0:\n",
    "            for i in range(min(args.show_valid_examples, len(y))):\n",
    "                logger.info(f'y: {y[i][-50:]}')\n",
    "                logger.info(f'p: {p[i][-50:]}')\n",
    "\n",
    "                logger.info(f\"y_text: {data['target_text'][i]}\")\n",
    "                logger.info(f\"p_text: {predicted_labels[i]}\")\n",
    "\n",
    "                logger.info('-' * 50)\n",
    "    try:\n",
    "        perplexity = math.exp(data[\"loss\"].mean())\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "    metrics[\"perplexity\"] = perplexity\n",
    "    \n",
    "    if add_generation:\n",
    "        metrics['generation_outputs'] = generation_outputs\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def prepare_generate_args(default_kwargs, batch, device):\n",
    "    generate_kwargs = deepcopy(default_kwargs)\n",
    "    if 'max_length' not in generate_kwargs and 'labels' in batch:\n",
    "        # if max_length is not set and labels are in subbatch, generate to the length of labels+1\n",
    "        # +1 as special tokens could be generated by the model\n",
    "        generate_kwargs['max_length'] = batch['labels'].shape[-1] + 1\n",
    "    if 'attention_mask_generate' in batch:\n",
    "        generate_kwargs['attention_mask'] = batch['attention_mask_generate'].to(device)\n",
    "    elif 'attention_mask' in batch:\n",
    "        generate_kwargs['attention_mask'] = batch['attention_mask']\n",
    "    if 'global_attention_mask' in batch:\n",
    "        generate_kwargs['global_attention_mask'] = batch['global_attention_mask']\n",
    "    \n",
    "    return generate_kwargs\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_model_on_sample(model, batch):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    default_generate_kwargs = {'max_new_tokens': 10, 'pad_token_id': 50256, }\n",
    "    model_forward_args = {'attention_mask', 'input_ids', 'inputs_embeds', 'labels', 'labels_mask', 'output_attentions', 'output_hidden_states'}\n",
    "    for k in batch:\n",
    "    # filter keys in batch to pass to model only supported arguments\n",
    "        if k in model_forward_args:\n",
    "            batch[k] = batch[k].to(device)\n",
    "\n",
    "    outputs = model(**{k: batch[k] for k in batch if k in model_forward_args})\n",
    "    loss = outputs['loss']\n",
    "\n",
    "    generate_kwargs = prepare_generate_args(default_generate_kwargs, batch, device)\n",
    "    generation_outputs = model.generate(batch['input_ids_generate'].to(device), **generate_kwargs)\n",
    "    \n",
    "    outputs['generation_outputs'] = generation_outputs\n",
    "    data = keep_for_metrics_fn(batch, outputs)\n",
    "    metric = metrics_fn(data)\n",
    "    metric['loss'] = loss\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======SAMPLE #0==========\n",
      "Print ONLY FACTS:\n",
      "Mary got the milk there.\n",
      "John moved to the bedroom.\n",
      "Sandra went back to the kitchen.\n",
      "Mary travelled to the hallway.\n",
      "QUESTION: Where is the milk? \n",
      "LABEL: hallway\n",
      "PRED: hallway\n",
      "=======SAMPLE #1==========\n",
      "Print ONLY FACTS:\n",
      "Mary got the milk there.\n",
      "John moved to the bedroom.\n",
      "Sandra went back to the kitchen.\n",
      "Mary travelled to the hallway.\n",
      "John got the football there.\n",
      "John went to the hallway.\n",
      "QUESTION: Where is the football? \n",
      "LABEL: hallway\n",
      "PRED: hallway\n",
      "=======SAMPLE #2==========\n",
      "Print ONLY FACTS:\n",
      "Mary got the milk there.\n",
      "John moved to the bedroom.\n",
      "Sandra went back to the kitchen.\n",
      "Mary travelled to the hallway.\n",
      "John got the football there.\n",
      "John went to the hallway.\n",
      "John put down the football.\n",
      "Mary went to the garden.\n",
      "QUESTION: Where is the football? \n",
      "LABEL: hallway\n",
      "PRED: hallway\n",
      "=======SAMPLE #3==========\n",
      "Print ONLY FACTS:\n",
      "Mary got the milk there.\n",
      "John moved to the bedroom.\n",
      "Sandra went back to the kitchen.\n",
      "Mary travelled to the hallway.\n",
      "John got the football there.\n",
      "John went to the hallway.\n",
      "John put down the football.\n",
      "Mary went to the garden.\n",
      "John went to the kitchen.\n",
      "Sandra travelled to the hallway.\n",
      "QUESTION: Where is the football? \n",
      "LABEL: hallway\n",
      "PRED: hallway\n",
      "=======SAMPLE #4==========\n",
      "Print ONLY FACTS:\n",
      "Mary got the milk there.\n",
      "John moved to the bedroom.\n",
      "Sandra went back to the kitchen.\n",
      "Mary travelled to the hallway.\n",
      "John got the football there.\n",
      "John went to the hallway.\n",
      "John put down the football.\n",
      "Mary went to the garden.\n",
      "John went to the kitchen.\n",
      "Sandra travelled to the hallway.\n",
      "Daniel went to the hallway.\n",
      "Mary discarded the milk.\n",
      "QUESTION: Where is the milk? \n",
      "LABEL: garden\n",
      "PRED: garden\n",
      "=======SAMPLE #5==========\n",
      "Print ONLY FACTS:\n",
      "Mary journeyed to the bathroom.\n",
      "Sandra went to the garden.\n",
      "Daniel went back to the garden.\n",
      "Daniel went to the office.\n",
      "Sandra grabbed the milk there.\n",
      "Sandra put down the milk there.\n",
      "QUESTION: Where is the milk? \n",
      "LABEL: garden\n",
      "PRED: garden\n",
      "=======SAMPLE #6==========\n",
      "Print ONLY FACTS:\n",
      "Mary journeyed to the bathroom.\n",
      "Sandra went to the garden.\n",
      "Daniel went back to the garden.\n",
      "Daniel went to the office.\n",
      "Sandra grabbed the milk there.\n",
      "Sandra put down the milk there.\n",
      "Daniel went to the hallway.\n",
      "Sandra got the milk there.\n",
      "Daniel went to the garden.\n",
      "Daniel journeyed to the kitchen.\n",
      "Daniel journeyed to the bedroom.\n",
      "Mary journeyed to the garden.\n",
      "Daniel took the football there.\n",
      "Mary moved to the office.\n",
      "Sandra travelled to the bedroom.\n",
      "Daniel dropped the football.\n",
      "QUESTION: Where is the football? \n",
      "LABEL: bedroom\n",
      "PRED: office\n",
      "=======SAMPLE #7==========\n",
      "Print ONLY FACTS:\n",
      "Mary journeyed to the bathroom.\n",
      "Sandra went to the garden.\n",
      "Daniel went back to the garden.\n",
      "Daniel went to the office.\n",
      "Sandra grabbed the milk there.\n",
      "Sandra put down the milk there.\n",
      "Daniel went to the hallway.\n",
      "Sandra got the milk there.\n",
      "Daniel went to the garden.\n",
      "Daniel journeyed to the kitchen.\n",
      "Daniel journeyed to the bedroom.\n",
      "Mary journeyed to the garden.\n",
      "Daniel took the football there.\n",
      "Mary moved to the office.\n",
      "Sandra travelled to the bedroom.\n",
      "Daniel dropped the football.\n",
      "Sandra left the milk there.\n",
      "Daniel grabbed the football there.\n",
      "QUESTION: Where is the milk? \n",
      "LABEL: bedroom\n",
      "PRED: garden\n",
      "=======SAMPLE #8==========\n",
      "Print ONLY FACTS:\n",
      "Mary journeyed to the bathroom.\n",
      "Sandra went to the garden.\n",
      "Daniel went back to the garden.\n",
      "Daniel went to the office.\n",
      "Sandra grabbed the milk there.\n",
      "Sandra put down the milk there.\n",
      "Daniel went to the hallway.\n",
      "Sandra got the milk there.\n",
      "Daniel went to the garden.\n",
      "Daniel journeyed to the kitchen.\n",
      "Daniel journeyed to the bedroom.\n",
      "Mary journeyed to the garden.\n",
      "Daniel took the football there.\n",
      "Mary moved to the office.\n",
      "Sandra travelled to the bedroom.\n",
      "Daniel dropped the football.\n",
      "Sandra left the milk there.\n",
      "Daniel grabbed the football there.\n",
      "Sandra grabbed the milk there.\n",
      "Daniel went to the kitchen.\n",
      "QUESTION: Where is the football? \n",
      "LABEL: kitchen\n",
      "PRED: garden\n",
      "=======SAMPLE #9==========\n",
      "Print ONLY FACTS:\n",
      "Mary journeyed to the bathroom.\n",
      "Sandra went to the garden.\n",
      "Daniel went back to the garden.\n",
      "Daniel went to the office.\n",
      "Sandra grabbed the milk there.\n",
      "Sandra put down the milk there.\n",
      "Daniel went to the hallway.\n",
      "Sandra got the milk there.\n",
      "Daniel went to the garden.\n",
      "Daniel journeyed to the kitchen.\n",
      "Daniel journeyed to the bedroom.\n",
      "Mary journeyed to the garden.\n",
      "Daniel took the football there.\n",
      "Mary moved to the office.\n",
      "Sandra travelled to the bedroom.\n",
      "Daniel dropped the football.\n",
      "Sandra left the milk there.\n",
      "Daniel grabbed the football there.\n",
      "Sandra grabbed the milk there.\n",
      "Daniel went to the kitchen.\n",
      "John travelled to the kitchen.\n",
      "Mary moved to the hallway.\n",
      "QUESTION: Where is the football? \n",
      "LABEL: kitchen\n",
      "PRED: kitchen\n",
      "accuracy: 0.7\n"
     ]
    }
   ],
   "source": [
    "samples_limit = 10\n",
    "num_samples = 0\n",
    "exact_matches = 0.\n",
    "for batch in test_dataloader:\n",
    "    batch_len = batch['input_ids'].shape[0]\n",
    "    metric = test_model_on_sample(rmt, batch)\n",
    "    for i in range(batch_len):\n",
    "        print(f'=======SAMPLE #{i+num_samples}==========')\n",
    "        print(\"Print ONLY FACTS:\")\n",
    "        for f in batch['facts'][i]:\n",
    "            print(f)\n",
    "        print(\"QUESTION:\", batch['question'][i])\n",
    "        print(\"LABEL:\", batch['target_text'][i])\n",
    "        print(\"PRED:\", metric['generation_outputs'][i])\n",
    "        \n",
    "    exact_matches += metric['exact_match']*batch_len\n",
    "    num_samples += batch_len\n",
    "    if num_samples >= samples_limit: break\n",
    "\n",
    "print(\"accuracy:\",  exact_matches/num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_index=999/999, exact_match=0.622, E[loss]=0.596\n",
      "exact_match: 0.6216 E[loss]=0.5959\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def total_test(dataloader):\n",
    "    L = len(dataloader)\n",
    "    exact_matches = 0.\n",
    "    total_samples = 0\n",
    "    losses = []\n",
    "    for i, batch in enumerate(dataloader):    \n",
    "        bs = batch['input_ids'].shape[0]\n",
    "        metric = test_model_on_sample(rmt, batch)\n",
    "\n",
    "        total_samples += bs\n",
    "        exact_matches += metric['exact_match']*bs\n",
    "        losses.append(metric['loss'].item())\n",
    "        print(f'\\rcurr_index={i+1}/{L}, exact_match={exact_matches/total_samples:.3f}, E[loss]={np.mean(losses):.3f}', end=\"\")\n",
    "    \n",
    "    print(f\"\\nexact_match: {exact_matches/total_samples:.4f} E[loss]={np.mean(losses):.4f}\")\n",
    "\n",
    "total_test(test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss is too high and accuracy is to low somehow..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try to create trainer like in `run_finetuning_babilong_rmt.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import accelerate\n",
    "from transformers import HfArgumentParser\n",
    "from lm_experiments_tools import Trainer, TrainerArgs, get_optimizer\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "accelerator = accelerate.Accelerator(gradient_accumulation_steps=1)\n",
    "batch_metrics_fn = lambda _, y: {key: y[key] for key in y.keys() if (('loss' in key) or ('!log' in key))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = HfArgumentParser(TrainerArgs)\n",
    "parser.add_argument('--task_dataset', type=str, help=\"Task name\", default=\"qa1_single-supporting-fact\")\n",
    "parser.add_argument('--noise_dataset', type=str, help=\"Task name\", default='wikitext')\n",
    "parser.add_argument('--noise_dataset_split', type=str, help=\"Task name\", default=None)\n",
    "parser.add_argument('--babi_path', type=str, help=\"path to babi folder\", default=\"data/tasks_1-20_v1-2/en-10k\")\n",
    "\n",
    "\n",
    "parser.add_argument('--validate_only', action='store_true', default=False,\n",
    "                    help='Skip training and run only validation. (default: False)')\n",
    "parser.add_argument('--working_dir', type=str, default='.',\n",
    "                    help='working dir, should be a dir with t5-experiments repo (default: .)')\n",
    "parser.add_argument('--seed', type=int, default=42, help='random seed')\n",
    "parser.add_argument('--show_valid_examples', type=int, default=0,\n",
    "                    help='how many valid examples to show during training (default: 0)')\n",
    "parser.add_argument('--block_size', type=int, default=128, help='max size of language modeling block')\n",
    "parser.add_argument('--history_size', type=int, default=0, help='max number of past tokens for each block')\n",
    "parser.add_argument('--data_n_workers', type=int, default=2, help='number of dataloader workers (default: 2)')\n",
    "\n",
    "parser.add_argument('--input_prefix', type=str, default='', help='add task prefix to an input string (default: \"\")')\n",
    "\n",
    "# model args\n",
    "parser.add_argument('--from_pretrained', type=str, help='model name in HF Model Hub (default: \"\")')\n",
    "parser.add_argument('--model_cfg', type=str, help='path to model configuration file (default: \"\")')\n",
    "parser.add_argument('--model_cls', type=str, default='transformers:BertForPreTraining',\n",
    "                    help='model class name to use (default: transformers:BertForPreTraining)')\n",
    "parser.add_argument('--memory_cell_cls', type=str, default=None, help='cell class for RMT')\n",
    "parser.add_argument('--recurrent_wrapper_cls', type=str, default=None, help='recurrent wrapper class for RMT')\n",
    "parser.add_argument('--model_cpt', type=str, default=None, help='pretrained model checkpoint path')\n",
    "parser.add_argument('--model_type', type=str, default='encoder-decoder',\n",
    "                    help='model type, encoder, encoder-decoder, decoder, affects preprocessing '\n",
    "                         '(default: encoder-decoder)')\n",
    "\n",
    "# Babilong parameters\n",
    "parser.add_argument('--sample_size', type=int, default=None, help='max number of tokens in sample')\n",
    "parser.add_argument('--max_n_facts', type=int, default=None, help='drop samples with higher number of facts')\n",
    "parser.add_argument('--task_start_pct', type=float, default=None, help='left border of facts in sample, between 0 and 1')\n",
    "parser.add_argument('--task_end_pct', type=float, default=None, help='right border of facts in sample, between task_start_pct and 1')\n",
    "\n",
    "\n",
    "# RMT args \n",
    "parser.add_argument('--segment_size', type=int, default=None, help='maximal input size of the backbone model')\n",
    "parser.add_argument('--num_mem_tokens', type=int, default=None, help='number of memory tokens.')\n",
    "parser.add_argument('--max_n_segments', type=int, default=1, help='maximal segment number')\n",
    "parser.add_argument('--vary_n_segments', action='store_true', default=False, help='randomly sample input size for each batch')\n",
    "parser.add_argument('--mixed_length_ratio', type=float, default=0.0, help='used for mixed length curriculum. '\n",
    "                    'r > 0.0 means that we will start to sample batches with lengths <= max_n_segments')\n",
    "parser.add_argument('--bptt_depth', type=int, default=-1, help='max number of previous segments in gradient computation.')\n",
    "parser.add_argument('--segment_alignment', type=str, help='way of aligning segments, one of right, left, center', default=None)\n",
    "parser.add_argument('--k2', type=int, default=-1, help='number of last segments used by backward')\n",
    "parser.add_argument('--freeze_model_weights', action='store_true', default=False,\n",
    "                    help='Stop training all model weights except memory layers')\n",
    "parser.add_argument('--backbone_cpt', type=str, default=None, help='backbone model checkpoint path')\n",
    "\n",
    "# tokenizer\n",
    "# todo: add wordpiece tokenizers support?\n",
    "parser.add_argument('--tokenizer', type=str, default=None, help='path or name of pre-trained HF Tokenizer')\n",
    "\n",
    "# optimizer args\n",
    "parser.add_argument('--optimizer', type=str, default='AdamW', help='optimizer name: AdamW, Adafactor. (default: AdamW)')\n",
    "parser.add_argument('--weight_decay', type=float, default=0.0, help='optimizer weight decay (default: 0.0)')\n",
    "parser.add_argument('--scale_parameter', action='store_true', default=False,\n",
    "                    help='Adafactor scale_parameter (default: False)')\n",
    "parser.add_argument('--relative_step', action='store_true', default=False,\n",
    "                    help='Adafactor relative_step (default: False)')\n",
    "parser.add_argument('--warmup_init', action='store_true', default=False,\n",
    "                    help='Adafactor warmup_init (default: False)')\n",
    "\n",
    "# LoRA args\n",
    "parser.add_argument('--use_lora', action='store_true', default=False, help='')\n",
    "parser.add_argument('--lora_attn_dim', type=int, default=8, help='')\n",
    "parser.add_argument('--lora_attn_alpha', type=int, default=32, help='')\n",
    "parser.add_argument('--lora_dropout', type=float, default=0.1, help='')\n",
    "parser.add_argument('--layers_pattern', type=str, default=None, help='')\n",
    "\n",
    "# Parallel Adapter args\n",
    "parser.add_argument('--use_adapter', action='store_true', default=False, help='')\n",
    "parser.add_argument('--adapter_bottleneck_dim', type=int, default=512, help='')\n",
    "parser.add_argument('--adapter_dropout', type=float, default=0.1, help='')\n",
    "parser.add_argument('--adapter_scale', type=float, default=4.0, help='')\n",
    "\n",
    "# Dataset args\n",
    "parser.add_argument('--pile_subset_names', type=str, default=None, help='use only these subsets of The PILE, separated by ;')\n",
    "parser.add_argument('--min_tokens_in_document', type=int, default=None, help='do not use documents shorter than this value')\n",
    "parser.add_argument('--max_tokens_in_document', type=int, default=None, help='do not use documents longer than this value')\n",
    "\n",
    "arg_string = \"--validate_only --task_dataset qa2_two-supporting-facts --noise_dataset pg19 --babi_path data/tasks_1-20_v1-2/en-10k --model_path ../runs/babilong/qa1_single-supporting-fact/gpt2/lr1e-04_linear_adamw_wd1e-03_2x128_mem10_bs16_bptt--1_vary/run_1 --from_pretrained gpt2 --model_type decoder --memory_cell_cls modeling_rmt.language_modeling:MemoryCell --recurrent_wrapper_cls modeling_rmt.language_modeling:RecurrentWrapper --model_cls transformers:AutoModelForCausalLM --segment_size 512 --sample_size 480 --num_mem_tokens 16 --max_n_segments 1 --batch_size 2 --gradient_accumulation_steps 1 --num_training_steps 4000 --iters 5000 --save_best --k2 -1 --optimizer AdamW --weight_decay 0.01 --lr 1e-05 --lr_scheduler linear --num_warmup_steps 500 --data_n_workers 1 --log_interval 50 --valid_interval 250 --optimize_metric exact_match --optimize_mode max --show_valid_examples 5 --early_stopping_patience 15 --seed 43 --clip_grad_norm 1.0 --model_cpt ../runs/server/babilong_checkpoints/qa2_test/run_4/model_best --vary_n_segments --use_generate_on_valid\"\n",
    "args = parser.parse_args(arg_string.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_cls = get_optimizer(args.optimizer)\n",
    "if optimizer_cls is None:\n",
    "    raise RuntimeError(f'{args.optimizer} was not found in optimizers, torch.optim, transformers.optimization')\n",
    "\n",
    "\n",
    "# todo: group optimizer params\n",
    "optimizer = optimizer_cls(rmt.parameters(), lr=args.lr, weight_decay=args.weight_decay) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    args, \n",
    "    accelerator, \n",
    "    rmt, \n",
    "    optimizer, \n",
    "    train_dataloader, \n",
    "    test_dataloader,\n",
    "    keep_for_metrics_fn=keep_for_metrics_fn, \n",
    "    metrics_fn=lambda d: metrics_fn(d, add_generation=False),\n",
    "    batch_metrics_fn=batch_metrics_fn,\n",
    "    generate_kwargs={\"pad_token_id\": id_pad_value, \"max_new_tokens\":10}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                                       | 0/999 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Validation: 100%|█████████████████████████████| 999/999 [01:09<00:00, 14.46it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.5959115713413048,\n",
       " 'exact_match': 0.6216216216216216,\n",
       " 'perplexity': 1.8146843893715991}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from accelerate.logging import get_logger\n",
    "logger = get_logger('')\n",
    "trainer.validate(test_dataloader, write_tb=True, split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
