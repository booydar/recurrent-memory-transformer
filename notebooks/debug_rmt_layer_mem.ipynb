{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "# from megatron.data.dataset_utils import get_indexed_dataset_\n",
    "\n",
    "# import horovod.torch as hvd\n",
    "# from dotenv import load_dotenv\n",
    "import torch\n",
    "import numpy as np\n",
    "import datasets\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from datasets import Dataset, load_dataset\n",
    "from huggingface_hub import hf_hub_download\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "sys.path.append('../')\n",
    "# from lm_experiments_tools import TrainerArgs\n",
    "from lm_experiments_tools.trainer import Trainer\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "# from lm_experiments_tools.lm_datasets import get_lm_datasets\n",
    "from task_utils.contract_nli import process_file\n",
    "import transformers  # noqa: E402\n",
    "from transformers import AutoConfig, AutoTokenizer, HfArgumentParser  # noqa: E402\n",
    "\n",
    "from lm_experiments_tools.utils import collect_run_configuration, get_cls_by_name, get_optimizer  # noqa: E402\n",
    "import lm_experiments_tools.optimizers as optimizers  # noqa: E402\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load_dotenv()\n",
    "\n",
    "# logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "#                     level=logging.INFO)\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "# # if CUDA_VISIBLE_DEVICES is not set make all gpus visible\n",
    "# if os.environ.get('CUDA_VISIBLE_DEVICES', None) is None:\n",
    "#     os.environ['CUDA_VISIBLE_DEVICES'] = ','.join([str(i) for i in range(torch.cuda.device_count())])\n",
    "\n",
    "# logger.info(f\"CUDA_VISIBLE_DEVICES: {os.environ['CUDA_VISIBLE_DEVICES']}\")\n",
    "# # first call to torch.cuda.device_count() sets visible gpus, following calls will not change the result\n",
    "# logger.info(f\"CUDA DEVICE COUNT: {torch.cuda.device_count()}\")\n",
    "\n",
    "# hvd.init()\n",
    "\n",
    "\n",
    "# # limit # of CPU threads to be used per pytorch worker, otherwise it might use all cpus and throttle gpus\n",
    "# # > 2 fails cause of https://github.com/pytorch/pytorch/issues/56615\n",
    "# # need to upgrade to torch>1.8.1\n",
    "# torch.set_num_threads(4)\n",
    "# # all gpus set with CUDA_VISIBLE_DEVICES are visible to process, indexing from 0 to ...\n",
    "# torch.cuda.set_device(hvd.local_rank())\n",
    "\n",
    "# parser = HfArgumentParser(TrainerArgs)\n",
    "# parser.add_argument('--task_name', type=str, help=\"Task name, wikitext, ...\")\n",
    "# parser.add_argument('--validate_only', action='store_true', default=False,\n",
    "#                     help='Skip training and run only validation. (default: False)')\n",
    "# parser.add_argument('--working_dir', type=str, default='.',\n",
    "#                     help='working dir, should be a dir with t5-experiments repo (default: .)')\n",
    "# parser.add_argument('--seed', type=int, default=42, help='random seed')\n",
    "# parser.add_argument('--show_valid_examples', type=int, default=0,\n",
    "#                     help='how many valid examples to show during training (default: 0)')\n",
    "# parser.add_argument('--input_seq_len', type=int, default=128, help='input sequnce length (default: 128).')\n",
    "# parser.add_argument('--target_seq_len', type=int, default=16, help='target sequnce length, should be set to '\n",
    "#                                                                    'max(len(target))+1 for EOS (default: 16).')\n",
    "# parser.add_argument('--data_n_workers', type=int, default=2, help='number of dataloader workers (default: 2)')\n",
    "\n",
    "# parser.add_argument('--input_prefix', type=str, default='', help='add task prefix to an input string (default: \"\")')\n",
    "# parser.add_argument('--sliding_window', action='store_true', help='use slinding window attentinon mask, '\n",
    "#                     'eval on last segment only', default=False)\n",
    "\n",
    "# # model args\n",
    "# parser.add_argument('--from_pretrained', type=str, help='model name in HF Model Hub (default: \"\")')\n",
    "# parser.add_argument('--model_cfg', type=str, help='path to model configuration file (default: \"\")')\n",
    "# parser.add_argument('--model_cls', type=str, default='transformers:BertForPreTraining',\n",
    "#                     help='model class name to use (default: transformers:BertForPreTraining)')\n",
    "# parser.add_argument('--memory_cell_cls', type=str, default=None, help='cell class for RMT')\n",
    "# parser.add_argument('--recurrent_wrapper_cls', type=str, default=None, help='recurrent wrapper class for RMT')\n",
    "# parser.add_argument('--model_cpt', type=str, default=None, help='pretrained model checkpoint path')\n",
    "# parser.add_argument('--backbone_cls', type=str, default=None,\n",
    "#                     help='backbone class name to use for RMT')\n",
    "# parser.add_argument('--model_type', type=str, default='encoder-decoder',\n",
    "#                     help='model type, encoder, encoder-decoder, decoder, affects preprocessing '\n",
    "#                          '(default: encoder-decoder)')\n",
    "\n",
    "\n",
    "# # Aydar # RMT args \n",
    "# parser.add_argument('--input_size', type=int, default=None, help='maximal input size of the backbone model')\n",
    "# parser.add_argument('--num_mem_tokens', type=int, default=None, help='number of memory tokens.')\n",
    "# parser.add_argument('--max_n_segments', type=int, default=1, help='maximal segment number')\n",
    "# # parser.add_argument('--sum_loss', action='store_true', default=False,\n",
    "# #                     help='with this flag task loss from all segments is summed')\n",
    "# # parser.add_argument('--bptt_depth', type=int, default=-1, help='max number of previous segments in gradient computation.')\n",
    "# # parser.add_argument('--segment_ordering', type=str, help='segment order', default='regular',\n",
    "# #                     choices=['regular', 'reversed', 'bidirectional', 'repeat_first', 'last_memory_only'])\n",
    "# # parser.add_argument('--memory_forward_func', type=str, help='path to memory forward fun—Åtion script', default=None)\n",
    "# # parser.add_argument('--memory_layers', type=str, help='memory-augmented layer inds or \"all\" for all layers', default=None)\n",
    "# # parser.add_argument('--share_memory_layers', action='store_true', help='share weights of memory layers', default=False)\n",
    "# # parser.add_argument('--reconstruction_loss_coef', type=float, default=None,\n",
    "# #                     help='reconstuction loss ratio in total loss')\n",
    "# # # parser.add_argument('--segment_ordering', type=str,help='????', default='regular',\n",
    "# # #                     choices=['regular', 'reversed', 'bidirectional', 'repeat_first', 'last_memory_only'])\n",
    "# # parser.add_argument('--retain_graph', action='store_true', help='Retain computation graph during backward pass', default=False)\n",
    "# # parser.add_argument('--use_truncated_backward', action='store_true', default=False,\n",
    "# #                     help='whether to use RMT truncated bptt method in backward')\n",
    "# # parser.add_argument('--k1', type=int, default=-1, help='(not implemented) If not -1, gradient update is done each k1 segments')\n",
    "# parser.add_argument('--k2', type=int, default=-1, help='number of last segments used by backward')\n",
    "# parser.add_argument('--freeze_model_weights', action='store_true', default=False,\n",
    "#                     help='Stop training all model weights except memory layers')\n",
    "# parser.add_argument('--backbone_cpt', type=str, default=None, help='backbone model checkpoint path')\n",
    "\n",
    "\n",
    "# # tokenizer\n",
    "# # todo: add wordpiece tokenizers support?\n",
    "# parser.add_argument('--tokenizer', type=str, default=None, help='path or name of pre-trained HF Tokenizer')\n",
    "\n",
    "# # optimizer args\n",
    "# parser.add_argument('--optimizer', type=str, default='AdamW', help='optimizer name: AdamW, Adafactor. (default: AdamW)')\n",
    "# parser.add_argument('--weight_decay', type=float, default=0.0, help='optimizer weight decay (default: 0.0)')\n",
    "# parser.add_argument('--scale_parameter', action='store_true', default=False,\n",
    "#                     help='Adafactor scale_parameter (default: False)')\n",
    "# parser.add_argument('--relative_step', action='store_true', default=False,\n",
    "#                     help='Adafactor relative_step (default: False)')\n",
    "# parser.add_argument('--warmup_init', action='store_true', default=False,\n",
    "#                     help='Adafactor warmup_init (default: False)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def download_metric():\n",
    "    scrolls_metric_path = hf_hub_download(repo_id=\"tau/scrolls\", filename=\"metrics/scrolls.py\", repo_type=\"dataset\")\n",
    "    updated_scrolls_metric_path = (\n",
    "        os.path.dirname(scrolls_metric_path) + os.path.basename(scrolls_metric_path).replace(\".\", \"_\") + \".py\"\n",
    "    )\n",
    "    shutil.copy(scrolls_metric_path, updated_scrolls_metric_path)\n",
    "    return updated_scrolls_metric_path\n",
    "\n",
    "\n",
    "scrolls_metric_path = download_metric()\n",
    "\n",
    "task_to_metric = {\n",
    "    'gov_report': ['rouge/rouge1', 'rouge/rouge2', 'rouge/rougeL', 'rouge/rougeLsum', 'rouge/geometric_mean'],\n",
    "    'summ_screen_fd': ['rouge/rouge1', 'rouge/rouge2', 'rouge/rougeL', 'rouge/rougeLsum', 'rouge/geometric_mean'],\n",
    "    'qmsum': ['rouge/rouge1', 'rouge/rouge2', 'rouge/rougeL', 'rouge/rougeLsum', 'rouge/geometric_mean'],\n",
    "    'narrative_qa': ['f1'],\n",
    "    'qasper': ['f1'],\n",
    "    'quality': ['exact_match'],\n",
    "    'contract_nli': ['exact_match']\n",
    "}\n",
    "\n",
    "tasks_with_duplicates = {'narrative_qa', 'qasper'}\n",
    "\n",
    "\n",
    "# https://github.com/tau-nlp/scrolls/blob/5bfb8dbaf3a0128ac8c65922096fd95a645f6ba2/baselines/src/utils/duplicates.py#L1\n",
    "# some tasks have multiple possible labels for single input, drop_duplicates_in_input will collect such labels\n",
    "def drop_duplicates_in_input(untokenized_dataset):\n",
    "    indices_to_keep = []\n",
    "    id_to_idx = {}\n",
    "    outputs = []\n",
    "    for i, (id_, output) in enumerate(zip(untokenized_dataset[\"id\"], untokenized_dataset[\"output\"])):\n",
    "        if id_ in id_to_idx:\n",
    "            outputs[id_to_idx[id_]].append(output)\n",
    "            continue\n",
    "        indices_to_keep.append(i)\n",
    "        id_to_idx[id_] = len(outputs)\n",
    "        outputs.append([output])\n",
    "    untokenized_dataset = untokenized_dataset.select(indices_to_keep).flatten_indices()\n",
    "    untokenized_dataset = untokenized_dataset.remove_columns(\"output\")\n",
    "    untokenized_dataset = untokenized_dataset.add_column(\"outputs\", outputs)\n",
    "    return untokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Holder:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            return super().__getattr__(name)\n",
    "        except AttributeError:\n",
    "            return None\n",
    "\n",
    "args = Holder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_cpt = '/cephfs/home/bulatov/bulatov/RMT_light/runs/contract_nli/gpt2/linear_adamw_wd1e-03_124-128-1x128_mem2_bs128_regular_bptt-1_from_cpt_0-1/run_1/'\n",
    "model_cpt = \"/cephfs/home/bulatov/bulatov/RMT_light/runs/contract_nli/gpt2/linear_adamw_wd1e-03_118-128-1x128_mem5_bs128_regular_bptt-1_from_cpt_0-1/run_1\"\n",
    "# model_cpt = '/cephfs/home/bulatov/bulatov/RMT_light/runs/contract_nli/gpt2/lr5e-05_linear_adamw_wd1e-03_236-128-2x128_mem5_bs128_iters1500_regular_bptt-2/run_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open(model_cpt + '/config.json', 'r') as f:\n",
    "#     config = json.load(f)\n",
    "\n",
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs = (\"--model_path\", \"/cephfs/home/bulatov/bulatov/RMT_light/runs/test/cnli\",\n",
    "# \"--from_pretrained\", \"gpt2\",\n",
    "\"--from_pretrained\", \"EleutherAI/pythia-70m-deduped\",\n",
    "\"--task_name\", \"contract_nli\",\n",
    "\"--model_type\", \"decoder\",\n",
    "\"--memory_cell_cls\", \"modeling_rmt.language_modeling:MemoryCell\",\n",
    "\"--recurrent_wrapper_cls\", \"modeling_rmt.language_modeling:RecurrentWrapper\",\n",
    "# \"--model_cls\",  \"transformers:AutoModelForCausalLM\",\n",
    "\"--model_cls\",  \"base_models.modeling_gpt_neox:GPTNeoXForCausalLM\",\n",
    "\"--segment_alignment\", \"right\",\n",
    "# \"--model_cpt\", model_cpt,\n",
    "\"--optimizer\", \"AdamW\",\n",
    "\"--weight_decay\", \"0.001\",\n",
    "\"--lr\", \"1e-03\", \n",
    "\"--lr_scheduler\", \"constant_with_warmup\",\n",
    "\"--clip_grad_value\", \"5.0\")\n",
    "\n",
    "int_attrs = (\n",
    "    \n",
    "# \"--input_seq_len\", \"118\",\n",
    "\"--input_seq_len\", \"177\",\n",
    "\"--input_size\", \"128\",\n",
    "\"--target_seq_len\", \"128\",\n",
    "\"--num_mem_tokens\", \"5\",\n",
    "\"--max_n_segments\" ,\"2\", \n",
    "\"--batch_size\", \"2\", \n",
    "\"--gradient_accumulation_steps\", \"1\",\n",
    "\"--iters\", \"100\",\n",
    "\"--num_warmup_steps\", \"100\",\n",
    "\"--data_n_workers\", \"2\",\n",
    "\"--log_interval\", \"10\",\n",
    "\"--show_valid_examples\", \"5\",\n",
    "\"--early_stopping_patience\", \"15\",\n",
    "\"--seed\", \"42\",\n",
    "\"--k2\", \"-1\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a, v in zip(int_attrs[::2], int_attrs[1::2]):\n",
    "    setattr(args, a.split('--')[1], int(v))\n",
    "\n",
    "for a, v in zip(attrs[::2], attrs[1::2]):\n",
    "    try:\n",
    "        setattr(args, a.split('--')[1], float(v))\n",
    "    except ValueError:\n",
    "        setattr(args, a.split('--')[1], v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at None\n",
      "loading file merges.txt from cache at None\n",
      "loading file tokenizer.json from cache at /home/jovyan/.cache/huggingface/hub/models--EleutherAI--pythia-70m-deduped/snapshots/e93a9faa9c77e5d09219f6c868bfc7a1bd65593c/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/jovyan/.cache/huggingface/hub/models--EleutherAI--pythia-70m-deduped/snapshots/e93a9faa9c77e5d09219f6c868bfc7a1bd65593c/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/jovyan/.cache/huggingface/hub/models--EleutherAI--pythia-70m-deduped/snapshots/e93a9faa9c77e5d09219f6c868bfc7a1bd65593c/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# args = parser.parse_args()\n",
    "# set current working dir\n",
    "# args.working_dir = str(Path(args.working_dir).expanduser().absolute())\n",
    "# os.chdir(args.working_dir)\n",
    "# if hvd.rank() == 0:\n",
    "#     logger.info(f'hvd size: {hvd.size()}')\n",
    "#     logger.info(f'FP16: {args.fp16}')\n",
    "\n",
    "# if hvd.rank() == 0 and args.model_path is None:\n",
    "#     logger.warning('model_path is not set: config, logs and checkpoints will not be saved.')\n",
    "\n",
    "# # create model path and save configuration\n",
    "# if hvd.rank() == 0 and args.model_path is not None:\n",
    "#     model_path = Path(args.model_path)\n",
    "#     if not model_path.exists():\n",
    "#         Path(model_path).mkdir(parents=True)\n",
    "#     args_dict = collect_run_configuration(args)\n",
    "#     # todo: if model path exists and there is config file, write new config file aside\n",
    "#     json.dump(args_dict, open(model_path/'config.json', 'w'), indent=4)\n",
    "\n",
    "if not args.from_pretrained:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.from_pretrained)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Assigning ['[GEN]', '[PAD]'] to the additional_special_tokens key of the tokenizer\n"
     ]
    }
   ],
   "source": [
    "if args.model_type == 'decoder':\n",
    "    from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "    tokenizer.add_special_tokens({'additional_special_tokens': ['[GEN]', '[PAD]']})\n",
    "    gen_token = tokenizer.encode('[GEN]')[0]\n",
    "    tokenizer.pad_token_id = tokenizer.encode('[PAD]')[0]\n",
    "    id_pad_value = tokenizer.pad_token_id\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        inputs = [b['input'][:args.input_seq_len * 10] for b in batch]\n",
    "        labels = [b['output'][:args.input_seq_len * 10] for b in batch]\n",
    "\n",
    "        collated = {}\n",
    "        inputs = tokenizer.batch_encode_plus(list(inputs), padding=False)\n",
    "        labels = tokenizer.batch_encode_plus(list(labels), padding=False)\n",
    "\n",
    "        full_inputs = [torch.tensor(i[:args.input_seq_len - len(l) - 1] + [gen_token] + l) for i, l in zip(inputs['input_ids'], labels['input_ids'])]\n",
    "        full_inputs = pad_sequence(full_inputs, padding_value=tokenizer.pad_token_id, batch_first=True)\n",
    "\n",
    "        # gen_inputs = [torch.tensor(i[:args.input_seq_len - 1] + [gen_token]) for i in inputs['input_ids']]\n",
    "        \n",
    "        input_ids_generate = torch.ones_like(full_inputs) * tokenizer.pad_token_id\n",
    "        gen_inputs = [torch.tensor(i[:args.input_seq_len - len(l) - 1] + [gen_token]) for i, l in zip(inputs['input_ids'], labels['input_ids'])]\n",
    "        gen_inputs = pad_sequence(gen_inputs, padding_value=tokenizer.pad_token_id, batch_first=True)\n",
    "        input_ids_generate[:, :gen_inputs.shape[1]] = gen_inputs\n",
    "\n",
    "        \n",
    "        labels_mask = torch.zeros_like(full_inputs).bool()\n",
    "        for i, l in enumerate(labels['input_ids']):\n",
    "            labels_mask[i, -len(l) -1:] = True\n",
    "\n",
    "        collated['input_ids'] = collated['labels'] = full_inputs\n",
    "        collated['input_ids_generate'] = input_ids_generate\n",
    "        collated['labels_mask'] = labels_mask\n",
    "        collated['attention_mask'] = (collated['input_ids'] != id_pad_value).bool()\n",
    "\n",
    "        collated['id'] = [b['id'] for b in batch]\n",
    "        collated['target_text'] = [b['output'] for b in batch]\n",
    "        return collated\n",
    "\n",
    "    # def collate_train(batch):\n",
    "    #     inputs = [b['input'][:args.input_seq_len * 10] for b in batch]\n",
    "    #     labels = [b['output'][:args.input_seq_len * 10] for b in batch]\n",
    "\n",
    "    #     collated = {}\n",
    "    #     inputs = tokenizer.batch_encode_plus(list(inputs), padding=False)\n",
    "    #     labels = tokenizer.batch_encode_plus(list(labels), padding=False)\n",
    "\n",
    "    #     full_inputs = [torch.tensor(i[:args.input_seq_len - len(l) - 1] + [gen_token] + l) for i, l in zip(inputs['input_ids'], labels['input_ids'])]\n",
    "    #     full_inputs = pad_sequence(full_inputs, padding_value=tokenizer.pad_token_id).T\n",
    "        \n",
    "    #     labels_mask = torch.zeros_like(full_inputs).bool()\n",
    "    #     for i, l in enumerate(labels['input_ids']):\n",
    "    #         labels_mask[i, -len(l) -1:] = True\n",
    "\n",
    "    #     collated['input_ids'] = collated['labels'] = full_inputs\n",
    "    #     collated['labels_mask'] = labels_mask\n",
    "    #     collated['attention_mask'] = (collated['input_ids'] != id_pad_value).bool()\n",
    "    #     return collated\n",
    "\n",
    "    # def collate_valid(batch):\n",
    "    #     inputs = [b['input'][:args.input_seq_len * 10] for b in batch]\n",
    "\n",
    "    #     collated = {}\n",
    "    #     inputs = tokenizer.batch_encode_plus(list(inputs), padding=False)\n",
    "    #     full_inputs = [torch.tensor(i[:args.input_seq_len - 1] + [gen_token]) for i in inputs['input_ids']]\n",
    "    #     full_inputs = pad_sequence(full_inputs, padding_value=tokenizer.pad_token_id).T\n",
    "        \n",
    "    #     collated['input_ids'] = full_inputs\n",
    "    #     collated['attention_mask'] = (collated['input_ids'] != id_pad_value).bool()\n",
    "    #     return collated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get train dataset\n",
    "# if hvd.rank() == 0:\n",
    "#     logger.info(f'preparing dataset for: {args.task_name}')\n",
    "dataset = datasets.load_dataset('tau/scrolls', args.task_name)\n",
    "train_dataset = dataset['train']\n",
    "valid_dataset = dataset['validation']\n",
    "# shuffle train data each epoch (one loop over train_dataset)\n",
    "# train_sampler = DistributedSampler(train_dataset, rank=hvd.rank(), num_replicas=hvd.size(), shuffle=True,\n",
    "#                                     drop_last=False, seed=args.seed)\n",
    "per_worker_batch_size = int(args.batch_size * args.gradient_accumulation_steps)\n",
    "global_batch_size = int(args.batch_size)\n",
    "kwargs = {'pin_memory': True, 'num_workers': args.data_n_workers}\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=per_worker_batch_size, \n",
    "# sampler=train_sampler,\n",
    "                                collate_fn=collate_fn, **kwargs)\n",
    "                                # collate_fn=collate_train, **kwargs)\n",
    "# get validation dataset\n",
    "valid_dataloader = None\n",
    "# if hvd.rank() == 0:\n",
    "#     logger.info(f'preparing validation data from: {args.task_name}')\n",
    "# valid_sampler = DistributedSampler(valid_dataset, rank=hvd.rank(), num_replicas=hvd.size(), shuffle=False)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=per_worker_batch_size, \n",
    "# sampler=valid_sampler,\n",
    "                                collate_fn=collate_fn, **kwargs)\n",
    "                                # collate_fn=collate_valid, **kwargs)\n",
    "\n",
    "# test_sampler = DistributedSampler(test_dataset, rank=hvd.rank(), num_replicas=hvd.size(), shuffle=False)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=per_worker_batch_size, \n",
    "# sampler=test_sampler,\n",
    "                                # collate_fn=collate_fn, **kwargs)\n",
    "# if args.valid_interval is None:\n",
    "#     args.valid_interval = args.log_interval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions\n",
    "\n",
    "class MemoryCell(torch.nn.Module):\n",
    "    def __init__(self, base_model, num_mem_tokens):\n",
    "        super().__init__()\n",
    "        self.model = base_model\n",
    "        self.create_memory(num_mem_tokens)\n",
    "\n",
    "    def create_memory(self, num_mem_tokens):\n",
    "        self.num_mem_tokens = num_mem_tokens\n",
    "        embeddings = self.model.get_input_embeddings()\n",
    "        memory_dim =  getattr(self.model.config, 'n_embd', self.model.config.hidden_size)\n",
    "        memory_weights = torch.randn((num_mem_tokens, memory_dim)) * embeddings.weight.data.std()\n",
    "        self.register_parameter('memory', torch.nn.Parameter(memory_weights, requires_grad=True))\n",
    "\n",
    "        self.read_memory_position = range(num_mem_tokens)\n",
    "        self.write_memory_position = range(-num_mem_tokens, 0)\n",
    "\n",
    "    def set_memory(self, input_shape):\n",
    "        memory = self.memory.repeat(input_shape[0], 1, 1)\n",
    "        return memory\n",
    "\n",
    "    def forward(self, input_ids, memory_state=None, **kwargs):\n",
    "        if memory_state is None:\n",
    "            memory_state = self.set_memory(input_ids.shape)\n",
    "\n",
    "        seg_kwargs = self.process_input(input_ids, memory_state, **kwargs)\n",
    "        out = self.model(**seg_kwargs)\n",
    "        out, new_memory_state = self.process_output(out, **kwargs)\n",
    "\n",
    "        return out, new_memory_state\n",
    "    \n",
    "    def generate(self, input_ids, memory_state, attention_mask, **generate_kwargs):\n",
    "        if memory_state is None:\n",
    "            memory_state = self.set_memory(input_ids.shape)\n",
    "\n",
    "        seg_kwargs = self.process_input(input_ids, memory_state, attention_mask=attention_mask)\n",
    "        out = self.model.generate(inputs_embeds=seg_kwargs['inputs_embeds'], attention_mask=seg_kwargs['attention_mask'], **generate_kwargs)\n",
    "        return out\n",
    "\n",
    "    def process_input(self, input_ids, memory_state, **kwargs):\n",
    "        seg_kwargs = dict(**kwargs)\n",
    "\n",
    "        inputs_embeds = kwargs.get('inputs_embeds')\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.model.get_input_embeddings()(input_ids)\n",
    "        inputs_embeds = torch.cat([memory_state, inputs_embeds, memory_state], dim=1)\n",
    "\n",
    "        seg_kwargs['input_ids'] = None\n",
    "        seg_kwargs['inputs_embeds'] = inputs_embeds\n",
    "        if kwargs.get('attention_mask') is not None:\n",
    "            seg_kwargs['attention_mask'] = self.pad_attention_mask(kwargs['attention_mask'], inputs_embeds.shape)\n",
    "        seg_kwargs['output_hidden_states'] = True\n",
    "        return seg_kwargs\n",
    "    \n",
    "    def pad_attention_mask(self, attention_mask, shape):\n",
    "        if self.num_mem_tokens in {0, None}:\n",
    "            return attention_mask\n",
    "        else:\n",
    "            mask = torch.ones(*shape[:2], dtype=torch.int64).to(attention_mask.device)\n",
    "            mask[:, self.num_mem_tokens:-self.num_mem_tokens] = attention_mask\n",
    "            return mask\n",
    "    \n",
    "    def process_output(self, model_outputs, **kwargs):\n",
    "        if self.num_mem_tokens not in {0, None}:\n",
    "            out = CausalLMOutputWithCrossAttentions()\n",
    "            memory_state = model_outputs.hidden_states[-1][:, -self.num_mem_tokens:]\n",
    "            out['logits'] = model_outputs.logits[:, self.num_mem_tokens:-self.num_mem_tokens]\n",
    "            \n",
    "            if kwargs.get('output_hidden_states'):\n",
    "                out['hidden_states'] = [lh[:, self.num_mem_tokens:-self.num_mem_tokens] for lh in model_outputs.hidden_states]\n",
    "            if kwargs.get('output_attentions'):\n",
    "                out['attentions'] = model_outputs['attentions']\n",
    "        else:\n",
    "            memory_state = None\n",
    "            out = model_outputs\n",
    "            \n",
    "        return out, memory_state \n",
    "\n",
    "\n",
    "import random\n",
    "class RecurrentWrapper(torch.nn.Module):\n",
    "    def __init__(self, memory_cell, **rmt_kwargs):\n",
    "        super().__init__()\n",
    "        self.memory_cell = memory_cell\n",
    "        self.rmt_config = rmt_kwargs\n",
    "\n",
    "    def forward(self, input_ids, labels=None, labels_mask=None, inputs_embeds=None, attention_mask=None, output_attentions=None, output_hidden_states=None):\n",
    "        memory_state = None\n",
    "        segmented = self.segment(input_ids=input_ids, inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
    "\n",
    "        cell_outputs = []\n",
    "        for seg_num, segment in enumerate(segmented):\n",
    "            cell_out, memory_state = self.memory_cell(**segment, memory_state=memory_state, output_hidden_states=True)\n",
    "            cell_outputs.append(cell_out)\n",
    "            self.manage_gradients(memory_state, seg_num)\n",
    "\n",
    "        out = self.process_outputs(cell_outputs, labels=labels, \n",
    "                                   labels_mask=labels_mask,\n",
    "                                   output_attentions=output_attentions, \n",
    "                                   output_hidden_states=output_hidden_states)\n",
    "        return out\n",
    "    \n",
    "    def generate(self, input_ids, attention_mask, **generate_kwargs):\n",
    "        memory_state = None\n",
    "        segmented = self.segment(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        for seg_num, segment in enumerate(segmented[:-1]):\n",
    "            cell_out, memory_state = self.memory_cell(**segment, memory_state=memory_state, output_hidden_states=True)\n",
    "\n",
    "        final_segment = segmented[-1]\n",
    "        out = self.memory_cell.generate(**final_segment, memory_state=memory_state, **generate_kwargs)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def segment(self, **kwargs):\n",
    "        segments = []\n",
    "        for k, tensor in kwargs.items():\n",
    "            if tensor is not None:\n",
    "                k_segments = self.split_tensor(tensor)\n",
    "                for s, k_seg in enumerate(k_segments):\n",
    "                    if s < len(segments):\n",
    "                        segments[s][k] = k_seg\n",
    "                    else:\n",
    "                        segments.append({k: k_seg})\n",
    "        \n",
    "        max_n_segments = self.rmt_config.get('max_n_segments', False)\n",
    "        if max_n_segments:\n",
    "            # if self.rmt_config.get('vary_n_segments', False):\n",
    "            #     max_n_segments = random.randint(1, max_n_segments+1)\n",
    "            segments = segments[:max_n_segments]\n",
    "\n",
    "        return segments\n",
    "    \n",
    "    def split_tensor(self, tensor):\n",
    "        align = self.rmt_config.get('segment_alignment')\n",
    "        segment_size = self.rmt_config.get('segment_size')\n",
    "        if align in {'left', None}:\n",
    "            split_inds = list(range(0, tensor.shape[1], segment_size)) + [tensor.shape[1]]\n",
    "            segments = [tensor[:, start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "        elif align in {'right', None}:\n",
    "            split_inds = (list(range(tensor.shape[1], 0, -segment_size)) + [0])[::-1]\n",
    "            segments = [tensor[:, start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "        elif align == 'center':\n",
    "            n_seg = math.ceil(tensor.shape[1] / segment_size)\n",
    "            segments = torch.chunk(tensor, n_seg, dim=1)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return segments\n",
    "\n",
    "    def process_outputs(self, cell_outputs, **kwargs):\n",
    "        out = CausalLMOutputWithCrossAttentions()\n",
    "        full_logits = torch.cat([o.logits for o in cell_outputs], dim=1)\n",
    "        full_hidden_states = tuple([torch.cat(layer_hs, dim=1) for layer_hs in zip(*[o.hidden_states for o in cell_outputs])])\n",
    "\n",
    "        labels = kwargs.get('labels')\n",
    "        if labels is not None:\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            shift_logits = full_logits[..., :-1, :].contiguous()\n",
    "            flat_labels = shift_labels.view(-1)\n",
    "            flat_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "            \n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            labels_mask = kwargs.get('labels_mask')\n",
    "            # print('\\n\\n\\n\\n\\n\\n\\n\\n')\n",
    "            # print('labels_mask', labels_mask.shape)\n",
    "            # print('shift_logits', shift_logits.shape)\n",
    "            # print('cell_outputs', len(cell_outputs))\n",
    "            if labels_mask is not None:\n",
    "                shift_mask = labels_mask[..., :-1].contiguous()\n",
    "\n",
    "                flat_labels = flat_labels[shift_mask.view(-1)]\n",
    "                flat_logits = flat_logits[shift_mask.view(-1)]\n",
    "                \n",
    "            out['loss'] = loss_fct(flat_logits, flat_labels)\n",
    "        else:\n",
    "            out['loss'] = 0\n",
    "\n",
    "        out['logits'] = full_logits\n",
    "        segment_keys = ['loss', 'logits']\n",
    "        if kwargs.get('output_attentions'):\n",
    "            segment_keys.append('attentions')\n",
    "        if kwargs.get('output_hidden_states'):\n",
    "            segment_keys.append('hidden_states')\n",
    "            out['hidden_states'] = full_hidden_states\n",
    "\n",
    "        for seg_num, o in enumerate(cell_outputs):\n",
    "            for key, value in o.items():\n",
    "                if any([sk in key for sk in segment_keys]):\n",
    "                    out[f'{key}_{seg_num}'] = value\n",
    "\n",
    "        return out \n",
    "        \n",
    "    def manage_gradients(self, memory_state, seg_num):\n",
    "        k2, max_n_segments = self.rmt_config.get('k2'), self.rmt_config.get('max_n_segments')\n",
    "        if seg_num == 0 \\\n",
    "            or k2 in {-1, None} \\\n",
    "            or seg_num + k2 > max_n_segments:\n",
    "                return True\n",
    "        \n",
    "        memory_state = memory_state.detach()\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from base_models.modeling_gpt_neox import GPTNeoXForCausalLM\n",
    "# base_model = GPTNeoXForCausalLM.from_pretrained(\"EleutherAI/pythia-70m-deduped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Union, Optional, Tuple\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPast\n",
    "\n",
    "def gpt_neox_horizontal_memory_forward(\n",
    "    self,\n",
    "    input_ids: Optional[torch.LongTensor] = None,\n",
    "    attention_mask: Optional[torch.FloatTensor] = None,\n",
    "    position_ids: Optional[torch.LongTensor] = None,\n",
    "    head_mask: Optional[torch.FloatTensor] = None,\n",
    "    inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "    use_cache: Optional[bool] = None,\n",
    "    output_attentions: Optional[bool] = None,\n",
    "    output_hidden_states: Optional[bool] = None,\n",
    "    return_dict: Optional[bool] = None,\n",
    "    rmt_parent=None\n",
    ") -> Union[Tuple, BaseModelOutputWithPast]:\n",
    "    r\"\"\"\n",
    "    past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
    "        Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
    "        If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n",
    "        don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n",
    "        `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
    "    use_cache (`bool`, *optional*):\n",
    "        If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
    "        `past_key_values`).\n",
    "    \"\"\"\n",
    "    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "    output_hidden_states = (\n",
    "        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "    )\n",
    "    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "    use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "\n",
    "    if input_ids is not None and inputs_embeds is not None:\n",
    "        raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "    elif input_ids is not None:\n",
    "        input_shape = input_ids.size()\n",
    "    elif inputs_embeds is not None:\n",
    "        input_shape = inputs_embeds.size()[:-1]\n",
    "    else:\n",
    "        raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "    batch_size, seq_length = input_shape\n",
    "\n",
    "    if past_key_values is None:\n",
    "        past_length = 0\n",
    "        past_key_values = tuple([None] * self.config.num_hidden_layers)\n",
    "    else:\n",
    "        past_length = past_key_values[0][0].size(-2)\n",
    "\n",
    "    if position_ids is None:\n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "        position_ids = torch.arange(past_length, seq_length + past_length, dtype=torch.long, device=device)\n",
    "        position_ids = position_ids.unsqueeze(0).view(-1, seq_length)\n",
    "    else:\n",
    "        position_ids = position_ids.view(-1, seq_length).long()\n",
    "\n",
    "    # Attention mask.\n",
    "    if attention_mask is not None:\n",
    "        assert batch_size > 0, \"batch_size has to be defined and > 0\"\n",
    "        attention_mask = attention_mask.view(batch_size, -1)\n",
    "        # We create a 3D attention mask from a 2D tensor mask.\n",
    "        # Sizes are [batch_size, 1, 1, to_seq_length]\n",
    "        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
    "        # this attention mask is more simple than the triangular masking of causal attention\n",
    "        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
    "        attention_mask = attention_mask[:, None, None, :]\n",
    "\n",
    "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
    "        # masked positions, this operation will create a tensor which is 0.0 for\n",
    "        # positions we want to attend and the dtype's smallest value for masked positions.\n",
    "        # Since we are adding it to the raw scores before the softmax, this is\n",
    "        # effectively the same as removing these entirely.\n",
    "        attention_mask = attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n",
    "        attention_mask = (1.0 - attention_mask) * torch.finfo(self.dtype).min\n",
    "\n",
    "    # Prepare head mask if needed\n",
    "    # 1.0 in head_mask indicate we keep the head\n",
    "    # attention_probs has shape bsz x n_heads x N x N\n",
    "    # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
    "    # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
    "    head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
    "\n",
    "    if inputs_embeds is None:\n",
    "        inputs_embeds = self.embed_in(input_ids)\n",
    "\n",
    "    hidden_states = self.emb_dropout(inputs_embeds)\n",
    "\n",
    "    if self.gradient_checkpointing and self.training:\n",
    "        if use_cache:\n",
    "            # logger.warning(\n",
    "            #     \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "            # )\n",
    "            use_cache = False\n",
    "\n",
    "    presents = () if use_cache else None\n",
    "    all_attentions = () if output_attentions else None\n",
    "    all_hidden_states = () if output_hidden_states else None\n",
    "    for i, (layer, layer_past) in enumerate(zip(self.layers, past_key_values)):\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if self.gradient_checkpointing and self.training:\n",
    "            raise NotImplementedError\n",
    "            # def create_custom_forward(module):\n",
    "            #     def custom_forward(*inputs):\n",
    "            #         # None for layer_past\n",
    "            #         return module(*inputs, use_cache, None, output_attentions)\n",
    "\n",
    "            #     return custom_forward\n",
    "\n",
    "            # outputs = torch.utils.checkpoint.checkpoint(\n",
    "            #     create_custom_forward(layer),\n",
    "            #     hidden_states,\n",
    "            #     attention_mask,\n",
    "            #     position_ids,\n",
    "            #     head_mask[i],\n",
    "            # )\n",
    "        else:\n",
    "            num_mem_tokens = rmt_parent.memory_cell.num_mem_tokens\n",
    "            if i in rmt_parent.memory_storage:\n",
    "                layer_memory = rmt_parent.memory_storage[i]\n",
    "                if layer_memory.shape[0] == 1:\n",
    "                    layer_memory = layer_memory.repeat(hidden_states.shape[0], 1, 1)\n",
    "\n",
    "                hidden_states[:, :num_mem_tokens] = layer_memory\n",
    "                    \n",
    "            outputs = layer(\n",
    "                hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                position_ids=position_ids,\n",
    "                head_mask=head_mask[i],\n",
    "                layer_past=layer_past,\n",
    "                use_cache=use_cache,\n",
    "                output_attentions=output_attentions,\n",
    "            )\n",
    "        hidden_states = outputs[0]\n",
    "        if use_cache is True:\n",
    "            presents = presents + (outputs[1],)\n",
    "        if output_attentions:\n",
    "            all_attentions = all_attentions + (outputs[2 if use_cache else 1],)\n",
    "\n",
    "\n",
    "    hidden_states = self.final_layer_norm(hidden_states)\n",
    "\n",
    "    ### set layer memory\n",
    "    rmt_parent.memory_storage[i] = hidden_states[:, -num_mem_tokens:].detach()\n",
    "\n",
    "    # Add last hidden state\n",
    "    if output_hidden_states:\n",
    "        all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "    if not return_dict:\n",
    "        return tuple(v for v in [hidden_states, presents, all_hidden_states, all_attentions] if v is not None)\n",
    "\n",
    "    return BaseModelOutputWithPast(\n",
    "        last_hidden_state=hidden_states,\n",
    "        past_key_values=presents,\n",
    "        hidden_states=all_hidden_states,\n",
    "        attentions=all_attentions,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Hello",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mHello\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Hello"
     ]
    }
   ],
   "source": [
    "raise ValueError(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_memory_layers(self):\n",
    "#     memory_layers, share_memory_layers = self.rmt_config.get('memory_layers'), self.rmt_config.get('share_memory_layers')\n",
    "#     if memory_layers is None:\n",
    "#         self.memory_layers = None\n",
    "#     else:\n",
    "#         if memory_layers == 'all':\n",
    "#             memory_layers = range(len(self.model.base_model.encoder.layer))\n",
    "#         else:\n",
    "#             raise NotImplementedError\n",
    "            \n",
    "#         if share_memory_layers:\n",
    "#             memory_layer = copy.deepcopy(self.model.base_model.encoder.layer[0])\n",
    "#             self.memory_layers = [memory_layer for _ in range(len(memory_layers))]\n",
    "#             for n, p in memory_layer.named_parameters():\n",
    "#                 param_name = re.sub('\\.', '_', f'memory_{n}')\n",
    "#                 self.register_parameter(param_name, p)\n",
    "#         else:\n",
    "#             self.memory_layers = [copy.deepcopy(self.model.base_model.encoder.layer[int(l)]) for l in memory_layers]\n",
    "#             for ln, layer in enumerate(self.memory_layers):\n",
    "#                 for n, p in layer.named_parameters():\n",
    "#                     param_name = re.sub('\\.', '_', f'{ln}_memory_{n}')\n",
    "#                     self.register_parameter(param_name, p)\n",
    "\n",
    "\n",
    "#                     add_memory_layers()\n",
    "    \n",
    "# # memory_forward_func = rmt_config.get('memory_forward_func')\n",
    "# # if memory_forward_func is None:\n",
    "# #     memory_forward_func = horizontal_memory_forward\n",
    "# # self.override_encoder_forward(memory_forward_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/jovyan/.cache/huggingface/hub/models--EleutherAI--pythia-70m-deduped/snapshots/e93a9faa9c77e5d09219f6c868bfc7a1bd65593c/config.json\n",
      "Model config GPTNeoXConfig {\n",
      "  \"architectures\": [\n",
      "    \"GPTNeoXForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"gpt_neox\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rotary_emb_base\": 10000,\n",
      "  \"rotary_pct\": 0.25,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_parallel_residual\": true,\n",
      "  \"vocab_size\": 50304\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/jovyan/.cache/huggingface/hub/models--EleutherAI--pythia-70m-deduped/snapshots/e93a9faa9c77e5d09219f6c868bfc7a1bd65593c/model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing GPTNeoXForCausalLM.\n",
      "\n",
      "All the weights of GPTNeoXForCausalLM were initialized from the model checkpoint at EleutherAI/pythia-70m-deduped.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTNeoXForCausalLM for predictions without further training.\n",
      "Generation config file not found, using a generation config created from the model config.\n"
     ]
    }
   ],
   "source": [
    "block_size=args.input_size-2*args.num_mem_tokens\n",
    "# define model\n",
    "model_cls = get_cls_by_name(args.model_cls)\n",
    "# if hvd.rank() == 0:\n",
    "#     logger.info(f'Using model class: {model_cls}')\n",
    "if not args.from_pretrained:\n",
    "    model_cfg = AutoConfig.from_pretrained(args.model_cfg)\n",
    "    model = model_cls(config=model_cfg)\n",
    "else:\n",
    "    # if hvd.rank() == 0:\n",
    "    #     logger.info(f'Loading pretrained model: {args.from_pretrained}')\n",
    "    model = model_cls.from_pretrained(args.from_pretrained)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "## load cpt of backbone model\n",
    "if args.backbone_cpt:\n",
    "    backbone_cpt = os.path.join(args.backbone_cpt, \"model_best.pth\")\n",
    "    cpt = torch.load(backbone_cpt, map_location='cpu')\n",
    "    model.load_state_dict(cpt['model_state_dict'], strict=False)\n",
    "    # if hvd.rank() == 0:\n",
    "    #     logger.info(f'Loaded baseline state dict from: {args.backbone_cpt}')\n",
    "\n",
    "# Pass memory settings to pretrained model\n",
    "if args.num_mem_tokens is not None:\n",
    "    # memory_cell_cls = get_cls_by_name(args.memory_cell_cls)\n",
    "    # recurrent_wrapper_cls = get_cls_by_name(args.recurrent_wrapper_cls)\n",
    "    memory_cell_cls = MemoryCell\n",
    "    recurrent_wrapper_cls = RecurrentWrapper\n",
    "    # if hvd.rank() == 0:\n",
    "    #     logger.info(f'Wrapping in: {memory_cell_cls} and {recurrent_wrapper_cls}')\n",
    "    \n",
    "    \n",
    "    cell = memory_cell_cls(model, args.num_mem_tokens)\n",
    "    model = recurrent_wrapper_cls(cell, \n",
    "                                    segment_size=block_size,\n",
    "                                    max_n_segments=args.max_n_segments, \n",
    "                                    k2=args.k2,\n",
    "                                    segment_alignment=args.segment_alignment\n",
    "    )\n",
    "                                \n",
    "\n",
    "    ## load cpt of rmt\n",
    "    if args.model_cpt:\n",
    "        model_cpt = os.path.join(args.model_cpt, \"model_best.pth\")\n",
    "        cpt = torch.load(model_cpt, map_location='cpu')\n",
    "        model.load_state_dict(cpt['model_state_dict'], strict=False)\n",
    "        # if hvd.rank() == 0:\n",
    "            # logger.info(f'Loaded RMT state dict from: {args.model_cpt}')\n",
    "        print(f'Loaded RMT state dict from: {args.model_cpt}')\n",
    "\n",
    "if args.freeze_model_weights:\n",
    "    for n, p in model.named_parameters():\n",
    "        # if 'memory' not in n and 'wte' not in n:\n",
    "        if 'memory' not in n and 'lora' not in n:\n",
    "            p.requires_grad = False\n",
    "    # if hvd.rank() == 0:\n",
    "    #     logger.info(f'Frozen moodel weights')\n",
    "    #     logger.info(f'Remaining parameters: {[n for n, p in model.named_parameters() if p.requires_grad]}')\n",
    "\n",
    "# # fix the not-contiguous error with loralib and horovod\n",
    "# def make_contiguous(module):\n",
    "#     with torch.no_grad():\n",
    "#         for param in module.parameters():\n",
    "#             param.set_(param.contiguous())\n",
    "# make_contiguous(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def override_encoder_forward(memory_forward_func)\n",
    "import types\n",
    "\n",
    "self = model\n",
    "new_forward = lambda *args, **kwargs: gpt_neox_horizontal_memory_forward(*args, **kwargs, rmt_parent=self)\n",
    "self.memory_cell.model.gpt_neox.forward = types.MethodType(new_forward, self.memory_cell.model.gpt_neox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define optimizer\n",
    "optimizer_cls = get_optimizer(args.optimizer)\n",
    "if optimizer_cls is None:\n",
    "    raise RuntimeError(f'{args.optimizer} was not found in optimizers, torch.optim, transformers.optimization')\n",
    "\n",
    "# if hvd.rank() == 0:\n",
    "#     logger.info(f'Using optimizer class: {optimizer_cls}')\n",
    "\n",
    "# todo: group optimizer params\n",
    "if optimizer_cls in [transformers.optimization.Adafactor, optimizers.Adafactor]:\n",
    "    # https://github.com/huggingface/transformers/pull/9751/files -> transformers 4.3.0\n",
    "    optimizer = optimizer_cls(model.parameters(), lr=args.lr,\n",
    "                                scale_parameter=args.scale_parameter,\n",
    "                                relative_step=args.relative_step,\n",
    "                                warmup_init=args.warmup_init,\n",
    "                                weight_decay=args.weight_decay)\n",
    "else:\n",
    "    optimizer = optimizer_cls(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "def keep_for_metrics_fn(batch, output):\n",
    "    data = {}\n",
    "    if 'generation_outputs' in output:\n",
    "            # data['labels'] = batch['answer']\n",
    "        data['generation_outputs'] = output['generation_outputs']\n",
    "        \n",
    "    data['labels'] = batch['labels']\n",
    "    for key in batch.keys():\n",
    "        if 'loss' in key: \n",
    "            data[key] = batch[key]\n",
    "    data['predictions'] = torch.argmax(output['logits'].detach(), dim=-1)\n",
    "    if 'labels_mask' in batch:\n",
    "        data['predictions'] = [data['predictions'][i, mask] for i, mask in enumerate(batch['labels_mask'])]\n",
    "    return data\n",
    "\n",
    "# HF datasets can compute metrics on each gpu process and then aggregate them on process with rank 0\n",
    "# synchronization is done by using temporay files on a shared filesystem\n",
    "# rank and number of workers is set by num_process and process_id params\n",
    "# BUT our Trainer aggregates all prediction from all gpus!\n",
    "#   this will lead to computing metrics for predictions repeated xN_GPUS times\n",
    "# need to try:\n",
    "# - keep_in_memory=True, may lead to OOM for large validation sets, after sync predictions and targets for the full\n",
    "#       validation set would be stored on each GPU -> xN_GPUs RAM\n",
    "#   - implemented currently\n",
    "# - compute metrics on batch lvl\n",
    "# - add support of HF metrics and turn off aggregation in case if metric has .add_batch method\n",
    "# scrolls_metric = datasets.load_metric(scrolls_metric_path, args.task_name, keep_in_memory=True)\n",
    "\n",
    "def metrics_fn(data):\n",
    "    metrics = {}\n",
    "    y, p = None, None\n",
    "    if 'generation_outputs' in data:\n",
    "        # replace -100 with pad token in labels\n",
    "        y = data['labels']\n",
    "        p = tokenizer.batch_decode(data['generation_outputs'], skip_special_tokens=True)\n",
    "        \n",
    "        if hvd.rank() == 0 and args.show_valid_examples > 0:\n",
    "            for i in range(min(args.show_valid_examples, len(y))):\n",
    "                logger.info(f'y: {y[i]}')\n",
    "                logger.info(f'p: {p[i]}')\n",
    "                logger.info(f'p ids: {data[\"generation_outputs\"][i]}')\n",
    "                logger.info('-' * 50)\n",
    "\n",
    "    if y is not None and p is not None:\n",
    "        metrics['exact_match'] = accuracy_score(y, p) * 100\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'labels', 'input_ids_generate', 'labels_mask', 'attention_mask', 'id', 'target_text'])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gen=iter(train_dataloader)\n",
    "train_batch = next(train_gen)\n",
    "train_batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1785, 32177,  7021,  3091,   417,  8107, 16518,   667,  5113,   534,\n",
       "           802,  2915,  5201, 49209,  7021,   434, 11204,  9440,  8339,    15,\n",
       "           187,   187, 34656,    14, 15857,  3207,  2697, 10860,  4889,  5836,\n",
       "            39, 25788, 13859,  7400, 13145,  1848, 17260,   187,  1552, 36429,\n",
       "            14, 15857,  3207,  2697, 10860,  4889,  5836,    39, 25788, 13859,\n",
       "          7400, 13145,  1848, 17260, 11126,  8903, 10742, 15440,   310,  1160,\n",
       "           407,   285,   875,    27,   187,     9,    74,    10,   253,  7454,\n",
       "           273,   253,  1986, 14726,  4855, 15605,   323, 44438,   265,    13,\n",
       "          1907,   697, 17929,  4441,   387, 11107, 43744,   372,  7812,  1288,\n",
       "           408,   386,    13,  7346,    19, 31955,    13, 18908,   313, 30350,\n",
       "           773,  4037,    41,  1311,   668,   390,   253,   773,  6744,   498,\n",
       "         14356,   668,   558,   285,   187,     9,  2886,    10, 27103,  1713,\n",
       "         15362,  1157,   247,  2567,  4232,   275,  8956,   342,   253,  5323,\n",
       "           273, 27103,  1713, 15362,   285,  1907,   697,  8624, 14145,  4441,\n",
       "           387, 45495, 27696, 15362,   313, 30350,   253,   773,    35,   301,\n",
       "           491,   668,   390,   253,   773, 49610,   850, 21372,   187,   510,\n",
       "          5201,   498, 14356,   285,  4568,   532,   850,   403,   671,  6289,\n",
       "           281, 26708,   347,   253, 50277, 50278, 50278],\n",
       "        [ 1785, 32177,  7021,  3091,  6909,   390,  1091,   690, 11204,  9440,\n",
       "          8339,  2220,   253, 15056,   273, 14297,    15,   187,   187, 34656,\n",
       "            14, 15857,  3207,  2697, 10860,  4889,  5836,    39, 25788, 13859,\n",
       "          7400, 13145,  1848, 17260,   187,  1552, 36429,    14, 15857,  3207,\n",
       "          2697, 10860,  4889,  5836,    39, 25788, 13859,  7400, 13145,  1848,\n",
       "         17260, 11126,  8903, 10742, 15440,   310,  1160,   407,   285,   875,\n",
       "            27,   187,     9,    74,    10,   253,  7454,   273,   253,  1986,\n",
       "         14726,  4855, 15605,   323, 44438,   265,    13,  1907,   697, 17929,\n",
       "          4441,   387, 11107, 43744,   372,  7812,  1288,   408,   386,    13,\n",
       "          7346,    19, 31955,    13, 18908,   313, 30350,   773,  4037,    41,\n",
       "          1311,   668,   390,   253,   773,  6744,   498, 14356,   668,   558,\n",
       "           285,   187,     9,  2886,    10, 27103,  1713, 15362,  1157,   247,\n",
       "          2567,  4232,   275,  8956,   342,   253,  5323,   273, 27103,  1713,\n",
       "         15362,   285,  1907,   697,  8624, 14145,  4441,   387, 45495, 27696,\n",
       "         15362,   313, 30350,   253,   773,    35,   301,   491,   668,   390,\n",
       "           253,   773, 49610,   850, 21372,   187,   510,  5201,   498, 14356,\n",
       "           285,  4568,   532,   850,   403,   671,  6289,   281, 26708,   347,\n",
       "           253,   773,  7834, 50277, 50278, 50278, 50278]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch.pop('id')\n",
    "train_batch.pop('target_text')\n",
    "train_batch.pop('input_ids_generate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.memory_storage = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entering custom forward\n",
      "entering custom forward\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(8.5803, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "out = model(**train_batch)\n",
    "out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 177])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch['attention_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.rmt_config['segment_alignment'] = 'center'\n",
    "segmented = model.segment(input_ids=train_batch['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'segment_size': 118,\n",
       " 'max_n_segments': 2,\n",
       " 'k2': -1,\n",
       " 'segment_alignment': 'center'}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.rmt_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 89]), torch.Size([2, 88]))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segmented[0]['input_ids'].shape, segmented[1]['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'labels', 'input_ids_generate', 'labels_mask', 'attention_mask', 'id', 'target_text'])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(train_gen)\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids_generate = batch['input_ids_generate']\n",
    "# attention_mask = batch['attention_mask']\n",
    "# attention_mask = input_ids_generate != tokenizer.pad_token_id\n",
    "# gen_out = model.generate(input_ids_generate, attention_mask=attention_mask, output_scores=True, return_dict_in_generate=True, max_new_tokens=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ef forward(self, input_ids, labels=None, labels_mask=None, inputs_embeds=None, attention_mask=None, output_attentions=None, output_hidden_states=None):\n",
    "self = model\n",
    "input_ids = batch['input_ids']\n",
    "input_ids_generate = batch['input_ids_generate']\n",
    "attention_mask = batch['attention_mask']\n",
    "labels = batch['labels']\n",
    "labels_mask = batch['labels_mask']\n",
    "\n",
    "inputs_embeds = None\n",
    "output_attentions = False\n",
    "output_hidden_states = False\n",
    "\n",
    "memory_state = None\n",
    "segmented = self.segment(input_ids=input_ids, inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
    "\n",
    "cell_outputs = []\n",
    "for seg_num, segment in enumerate(segmented):\n",
    "    cell_out, memory_state = self.memory_cell(**segment, memory_state=memory_state, output_hidden_states=True)\n",
    "    cell_outputs.append(cell_out)\n",
    "    self.manage_gradients(memory_state, seg_num)\n",
    "\n",
    "out = self.process_outputs(cell_outputs, labels=labels, \n",
    "                            labels_mask=labels_mask,\n",
    "                            output_attentions=output_attentions, \n",
    "                            output_hidden_states=output_hidden_states)\n",
    "# return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(segmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1700, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# def generate(self, input_ids, attention_mask, **generate_kwargs):\n",
    "# generate_kwargs = {'output_scores': True, 'return_dict_in_generate': True}\n",
    "generate_kwargs = {}\n",
    "\n",
    "memory_state = None\n",
    "segmented = self.segment(input_ids=input_ids_generate, attention_mask=attention_mask)\n",
    "\n",
    "# cell_outputs = []\n",
    "for seg_num, segment in enumerate(segmented[:-1]):\n",
    "    print('forwarding')\n",
    "    cell_out, memory_state = self.memory_cell(**segment, memory_state=memory_state, output_hidden_states=True)\n",
    "    # cell_outputs.append(cell_out)\n",
    "    # self.manage_gradients(memory_state, seg_num)\n",
    "\n",
    "final_segment = segmented[-1]\n",
    "gen_out = self.memory_cell.generate(**final_segment, memory_state=memory_state, **generate_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = tokenizer.batch_decode(gen_out[:, :], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = batch['target_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EntailmentNot mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned',\n",
       " 'Not mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Entailment', 'Entailment']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, False]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[l == p[:len(l)] for p, l in zip (predictions, labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Not mentioned mentioned mentioned mentionedNot mentioned mentioned mentioned mentionedNot mentioned mentioned mentioned mentioned mentionedNot mentioned mentioned',\n",
       " 'Entailment mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50256,  3673,  4750,  4750,  4750,  4750,  3673,  4750,  4750,  4750,\n",
       "          4750,  3673,  4750,  4750,  4750,  4750,  4750,  3673,  4750,  4750],\n",
       "        [50256, 14539,   603,   434,  4750,  4750,  4750,  4750,  4750,  4750,\n",
       "          4750,  4750,  4750,  4750,  4750,  4750,  4750,  4750,  4750,  4750]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_out.sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4750,  434])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "input_ids[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Receiving Party shall not reverse engineer any objects which embody Disclosing Party's Confidential Information.\\n\\nNON-DISCLOSURE AND CONFIDENTIALITY AGREEMENT\\nThis NON-DISCLOSURE AND CONFIDENTIALITY AGREEMENT (‚ÄúAgreement‚Äù) is made by and between:\\n(i) the Office of the United Nations High Commissioner for Refugees, having its headquarters located at 94 rue de Montbrillant, 1202 Geneva, Switzerland (hereinafter ‚ÄúUNHCR‚Äù or the ‚ÄúDiscloser[GEN]Not mentioned\",\n",
       " 'Receiving Party shall destroy or return some Confidential Information upon the termination of Agreement.\\n\\nNON-DISCLOSURE AND CONFIDENTIALITY AGREEMENT\\nThis NON-DISCLOSURE AND CONFIDENTIALITY AGREEMENT (‚ÄúAgreement‚Äù) is made by and between:\\n(i) the Office of the United Nations High Commissioner for Refugees, having its headquarters located at 94 rue de Montbrillant, 1202 Geneva, Switzerland (hereinafter ‚ÄúUNHCR‚Äù or the ‚ÄúDiscloser‚Äù[GEN]Entailment']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Receiving Party shall not reverse engineer any objects which embody Disclosing Party's Confidential Information.\\n\\nNON-DISCLOSURE AND CONFIDENTIALITY AGREEMENT\\nThis NON-DISCLOSURE AND CONFIDENTIALITY AGREEMENT (‚ÄúAgreement‚Äù) is made by and between:\\n(i) the Office of the United Nations High Commissioner for Refugees, having its headquarters located at 94 rue de Montbrillant, 1202 Geneva, Switzerland (hereinafter ‚ÄúUNHCR‚Äù or the ‚ÄúDiscloser[GEN]\"]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(input_ids_generate[:1, :-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.generate(input_ids_generate[:1, :-2], attention_mask=torch.ones_like(input_ids_generate[:1, :-2]).bool())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.batch_decode(input_ids_generate[0][attention_mask[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = out.logits.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NotNotÔøΩNotclosureosingmentNot mentioned mentioned',\n",
       " 'EntclosureailmentÔøΩEntEntailment mentioned']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(preds[:, -10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Not', ' mentioned', ' mentioned']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(preds[0][labels_mask[0]][-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[GEN]', 'Not', ' mentioned']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(labels[0][labels_mask[0]][-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[GEN]', 'Not', ' mentioned']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(labels[0][labels_mask[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'labels', 'input_ids_generate', 'labels_mask', 'attention_mask', 'id', 'target_text'])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "valid_gen=iter(train_dataloader)\n",
    "valid_batch = next(valid_gen)\n",
    "valid_batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = valid_batch['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(input_ids, labels=valid_batch['labels'], labels_mask=valid_batch['labels_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = out.logits.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Not', ' mentioned', ' mentioned'], ['Ent', 'ail', 'ment', ' mentioned']]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer.batch_decode(preds[i][valid_batch['labels_mask'][i]]) for i in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[GEN]', 'Not', ' mentioned'], ['[GEN]', 'Ent', 'ail', 'ment']]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer.batch_decode(input_ids[i][valid_batch['labels_mask'][i]]) for i in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|endoftext|>Not mentioned mentioned mentioned mentionedNot mentioned mentioned mentioned mentionedNot mentioned mentioned mentioned mentioned mentionedNot mentioned mentioned mentioned mentioned mentioned mentionedNot mentioned mentioned mentioned mentioned mentioned mentioned',\n",
       " '<|endoftext|>Entailment mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned mentioned']"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_text = tokenizer.batch_decode(gen_out.sequences)\n",
    "gen_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Receiving Party shall not reverse engineer any objects which embody Disclosing Party's Confidential Information.\\n\\nNON-DISCLOSURE AND CONFIDENTIALITY AGREEMENT\\nThis NON-DISCLOSURE AND CONFIDENTIALITY AGREEMENT (‚ÄúAgreement‚Äù) is made by and between:\\n(i) the Office of the United Nations High Commissioner for Refugees, having its headquarters located at 94 rue de Montbrillant, 1202 Geneva, Switzerland (hereinafter ‚ÄúUNHCR‚Äù or the ‚ÄúDiscloser[GEN]Not mentioned\",\n",
       " 'Receiving Party shall destroy or return some Confidential Information upon the termination of Agreement.\\n\\nNON-DISCLOSURE AND CONFIDENTIALITY AGREEMENT\\nThis NON-DISCLOSURE AND CONFIDENTIALITY AGREEMENT (‚ÄúAgreement‚Äù) is made by and between:\\n(i) the Office of the United Nations High Commissioner for Refugees, having its headquarters located at 94 rue de Montbrillant, 1202 Geneva, Switzerland (hereinafter ‚ÄúUNHCR‚Äù or the ‚ÄúDiscloser‚Äù[GEN]Entailment']"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(input_ids)tokenizer.batch_decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Receiving Party shall not reverse engineer any objects which embody Disclosing Party's Confidential Information.\\n\\nNON-DISCLOSURE AND CONFIDENTIALITY AGREEMENT\\nThis NON-DISCLOSURE AND CONFIDENTIALITY AGREEMENT (‚ÄúAgreement‚Äù) is made by and between:\\n(i) the Office of the United Nations High Commissioner for Refugees, having its headquarters located at 94 rue de Montbrillant, 1202 Geneva, Switzerland (hereinafter ‚ÄúUNHCR‚Äù or the ‚ÄúDiscloser[GEN][PAD][PAD]\",\n",
       " 'Receiving Party shall destroy or return some Confidential Information upon the termination of Agreement.\\n\\nNON-DISCLOSURE AND CONFIDENTIALITY AGREEMENT\\nThis NON-DISCLOSURE AND CONFIDENTIALITY AGREEMENT (‚ÄúAgreement‚Äù) is made by and between:\\n(i) the Office of the United Nations High Commissioner for Refugees, having its headquarters located at 94 rue de Montbrillant, 1202 Geneva, Switzerland (hereinafter ‚ÄúUNHCR‚Äù or the ‚ÄúDiscloser‚Äù[GEN][PAD][PAD][PAD]']"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(input_ids_generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[GEN][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]Not\\nNotailNotNotCLosingURENotNotFLIDENTIALNotNotRE[GEN]Not[PAD] mentioned-NotCLOSURE and CON[PAD][PAD][PAD]ITY Agreement[PAD]EMENT shallNotÔøΩNotreement mentionedÔøΩEntNot mentionedNotNotNotNotNot[GEN]Ent)NotNot of the Director States, Commissioner for[PAD]NotNot mentionedEnt located in:-ue[GEN][PAD][PAD][PAD][PAD],[PAD][PAD][PAD], FranceEntEnt mentionedfter mentionedÔøΩEntNotEntÔøΩEntNotNotÔøΩNotclosureosingmentNot mentioned mentioned'"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.load_state_dict(cpt['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model_state_dict', 'optimizer_state_dict', 'iteration', 'epoch', 'metrics', 'lr_scheduler_state_dict'])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpt.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([50256,  3673,  4750,  4750,  4750,  4750,  3673,  4750,  4750,  4750,\n",
       "         4750,  3673,  4750,  4750,  4750,  4750,  4750,  3673,  4750,  4750,\n",
       "         4750,  4750,  4750,  4750,  3673,  4750,  4750,  4750,  4750,  4750,\n",
       "         4750])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_out.sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3041,   344,  1412,  3615,  2236,   407,  9575, 11949,   597,  5563,\n",
       "          543, 48061,  3167,   565,  2752,  3615,   338,  7326, 35599,  6188,\n",
       "           13,   198,   198,    45,  1340,    12, 26288,  5097,  2640, 11335,\n",
       "         5357,  7102,    37, 25256, 12576,  9050, 13077,  2200, 12529,   198,\n",
       "         1212, 44521,    12, 26288,  5097,  2640, 11335,  5357,  7102,    37,\n",
       "        25256, 12576,  9050, 13077,  2200, 12529,   357,   447,   250, 10262,\n",
       "        10237,   447,   251,     8,   318,   925,   416,   290,  1022,    25,\n",
       "          198,     7,    72,     8,   262,  4452,   286,   262,  1578,  7973,\n",
       "         3334, 13270,   329, 38550,    11,  1719,   663, 10043,  5140,   379,\n",
       "        10048,   374,   518,   390,  5575,  1671,   359,   415,    11,  1105,\n",
       "         2999, 20552,    11, 14679,   357,  1456,  1437,   637,   564,   250,\n",
       "         4944, 43230,   447,   251,   393,   262,   564,   250,  7279,   565,\n",
       "        13416, 50257])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids_generate[0][attention_mask[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3041,   344,  1412,  3615,  2236,   407,  9575, 11949,   597,  5563,\n",
       "          543, 48061,  3167,   565,  2752,  3615,   338,  7326, 35599,  6188,\n",
       "           13,   198,   198,    45,  1340,    12, 26288,  5097,  2640, 11335,\n",
       "         5357,  7102,    37, 25256, 12576,  9050, 13077,  2200, 12529,   198,\n",
       "         1212, 44521,    12, 26288,  5097,  2640, 11335,  5357,  7102,    37,\n",
       "        25256, 12576,  9050, 13077,  2200, 12529,   357,   447,   250, 10262,\n",
       "        10237,   447,   251,     8,   318,   925,   416,   290,  1022,    25,\n",
       "          198,     7,    72,     8,   262,  4452,   286,   262,  1578,  7973,\n",
       "         3334, 13270,   329, 38550,    11,  1719,   663, 10043,  5140,   379,\n",
       "        10048,   374,   518,   390,  5575,  1671,   359,   415,    11,  1105,\n",
       "         2999, 20552,    11, 14679,   357,  1456,  1437,   637,   564,   250,\n",
       "         4944, 43230,   447,   251,   393,   262,   564,   250,  7279,   565,\n",
       "        13416, 50257])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[0][attention_mask[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, AutoTokenizer\n",
    "gpt = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "gpt_tok = AutoTokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_ids = gpt_tok.batch_encode_plus(['The meaning of life is'], return_tensors='pt')\n",
    "gpt_out = gpt.generate(**input_ids, output_scores=True, return_dict_in_generate=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 464, 3616,  286, 1204,  318]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_embeds = gpt.transformer.wte(input_ids['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "gpt_out = gpt.generate(inputs_embeds=inputs_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50256,   407,   262,   976,   355,   262,  3616,   286,  1918,    13,\n",
       "           198,   198,   464,  3616,   286,  1204,   318,   407,   262,   976]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|> not the same as the meaning of death.\\n\\nThe meaning of life is not the same'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(gpt_out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The meaning of life is not the same as the meaning of death.\\n\\nThe meaning of life'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer.decode(gpt_out.sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love cats. I love cats. I love cats. I love cats. I love cats.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer.decode([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, torch.Size([1, 50257]))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gpt_out.scores), gpt_out.scores[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 236])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "gen_out = model.generate(**valid_batch, output_scores=True, return_dict_in_generate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasattr(gen_out, 'sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([2, 236])\n",
      "labels torch.Size([2, 236])\n",
      "labels_mask torch.Size([2, 236])\n",
      "attention_mask torch.Size([2, 236])\n"
     ]
    }
   ],
   "source": [
    "for k in train_batch:\n",
    "    print(k, train_batch[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 118])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segs[0]['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segs = self.segment(input_ids=input_ids)\n",
    "len(segs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = model(**train_batch)\n",
    "# out.loss\n",
    "\n",
    "self = model\n",
    "input_ids = train_batch['input_ids']\n",
    "inputs_embeds = None\n",
    "attention_mask = train_batch['attention_mask']\n",
    "\n",
    "memory_state = None\n",
    "segmented = self.segment(input_ids=input_ids, inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
    "\n",
    "cell_outputs = []\n",
    "for seg_num, segment in enumerate(segmented):\n",
    "    cell_out, memory_state = self.memory_cell(**segment, memory_state=memory_state, output_hidden_states=True)\n",
    "    cell_outputs.append(cell_out)\n",
    "    self.manage_gradients(memory_state, seg_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(segmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 124])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segmented[0]['attention_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_scrolls_run.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_scrolls_run.ipynb#Y235sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m cell_outputs[\u001b[39m1\u001b[39;49m]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "cell_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(96.6092, grad_fn=<NllLossBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out = model(**train_batch)\n",
    "out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(84.5565, grad_fn=<NllLossBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out = model(**train_batch)\n",
    "out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(86.7867, grad_fn=<NllLossBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out = model(**train_batch)\n",
    "out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = tokenizer.batch_decode(gen_out, skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eos = tokenizer.eos_token\n",
    "[p[:p.index(eos)] if eos in p else p for p in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4]),\n",
       " tensor([1, 2, 3, 4, 0, 2, 3, 4, 0, 1, 3, 4, 0, 1, 2, 4, 0, 1, 2, 3]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.eye(5) == 0).nonzero(as_tuple=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [0, 2],\n",
       "        [0, 3],\n",
       "        [0, 4],\n",
       "        [1, 0],\n",
       "        [1, 2],\n",
       "        [1, 3],\n",
       "        [1, 4],\n",
       "        [2, 0],\n",
       "        [2, 1],\n",
       "        [2, 3],\n",
       "        [2, 4],\n",
       "        [3, 0],\n",
       "        [3, 1],\n",
       "        [3, 2],\n",
       "        [3, 4],\n",
       "        [4, 0],\n",
       "        [4, 1],\n",
       "        [4, 2],\n",
       "        [4, 3]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.eye(5) == 0).nonzero(as_tuple=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1]), tensor([0, 0]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(gen_out == tokenizer.eos_token_id).nonzero(as_tuple=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0],\n",
       "        [1, 0]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(gen_out == tokenizer.eos_token_id).nonzero(as_tuple=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen = model.generate(**valid_batch)\n",
    "self = model\n",
    "input_ids = valid_batch['input_ids']\n",
    "attention_mask = valid_batch['attention_mask']\n",
    "\n",
    "\n",
    "memory_state = None\n",
    "segmented = self.segment(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# cell_outputs = []\n",
    "for seg_num, segment in enumerate(segmented[:-1]):\n",
    "    cell_out, memory_state = self.memory_cell(**segment, memory_state=memory_state, output_hidden_states=True)\n",
    "    # cell_outputs.append(cell_out)\n",
    "    # self.manage_gradients(memory_state, seg_num)\n",
    "\n",
    "final_segment = segmented[-1]\n",
    "# out = self.memory_cell.generate(**final_segment, memory_state=memory_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "self = model.memory_cell\n",
    "input_ids = final_segment['input_ids']\n",
    "attention_mask = final_segment['attention_mask']\n",
    "generate_kwargs = {'max_new_tokens': 20}\n",
    "\n",
    "# def generate(self, input_ids, memory_state, attention_mask, **generate_kwargs)\n",
    "if memory_state is None:\n",
    "    memory_state = self.set_memory(input_ids.shape)\n",
    "\n",
    "seg_kwargs = self.process_input(input_ids, memory_state, attention_mask=attention_mask)\n",
    "out = self.model.generate(inputs_embeds=seg_kwargs['inputs_embeds'], attention_mask=seg_kwargs['attention_mask'], **generate_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 21])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|endoftext|>[GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN]',\n",
       " '<|endoftext|>[GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN]']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions\n",
    "\n",
    "class MemoryCell(torch.nn.Module):\n",
    "    def __init__(self, base_model, num_mem_tokens):\n",
    "        super().__init__()\n",
    "        self.model = base_model\n",
    "        self.create_memory(num_mem_tokens)\n",
    "\n",
    "    def create_memory(self, num_mem_tokens):\n",
    "        self.num_mem_tokens = num_mem_tokens\n",
    "        embeddings = self.model.get_input_embeddings()\n",
    "        memory_weights = torch.randn((num_mem_tokens, self.model.config.n_embd)) * embeddings.weight.data.std()\n",
    "        self.register_parameter('memory', torch.nn.Parameter(memory_weights, requires_grad=True))\n",
    "\n",
    "        self.read_memory_position = range(num_mem_tokens)\n",
    "        self.write_memory_position = range(-num_mem_tokens, 0)\n",
    "\n",
    "    def set_memory(self, input_shape):\n",
    "        memory = self.memory.repeat(input_shape[0], 1, 1)\n",
    "        return memory\n",
    "\n",
    "    def forward(self, input_ids, memory_state=None, **kwargs):\n",
    "        if memory_state is None:\n",
    "            memory_state = self.set_memory(input_ids.shape)\n",
    "\n",
    "        seg_kwargs = self.process_input(input_ids, memory_state, **kwargs)\n",
    "        out = self.model(**seg_kwargs)\n",
    "        out, new_memory_state = self.process_output(out, **kwargs)\n",
    "\n",
    "        return out, new_memory_state\n",
    "    \n",
    "    def generate(self, input_ids, memory_state, attention_mask, **generate_kwargs)\n",
    "        if memory_state is None:\n",
    "            memory_state = self.set_memory(input_ids.shape)\n",
    "\n",
    "        seg_kwargs = self.process_input(input_ids, memory_state, attention_mask=attention_mask)\n",
    "        out = self.model.generate(seg_kwargs.inputs_embeds, **generate_kwargs)\n",
    "        return out\n",
    "    \n",
    "    def process_input(self, input_ids, memory_state, **kwargs):\n",
    "        seg_kwargs = dict(**kwargs)\n",
    "\n",
    "        inputs_embeds = kwargs.get('inputs_embeds')\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.model.get_input_embeddings()(input_ids)\n",
    "        inputs_embeds = torch.cat([memory_state, inputs_embeds, memory_state], dim=1)\n",
    "\n",
    "        seg_kwargs['input_ids'] = None\n",
    "        seg_kwargs['inputs_embeds'] = inputs_embeds\n",
    "        if kwargs.get('attention_mask') is not None:\n",
    "            seg_kwargs['attention_mask'] = self.pad_attention_mask(kwargs['attention_mask'], inputs_embeds.shape)\n",
    "        seg_kwargs['output_hidden_states'] = True\n",
    "        return seg_kwargs\n",
    "    \n",
    "    def pad_attention_mask(self, attention_mask, shape):\n",
    "        if self.num_mem_tokens in {0, None}:\n",
    "            return attention_mask\n",
    "        else:\n",
    "            mask = torch.ones(*shape[:2], dtype=torch.int64).to(attention_mask.device)\n",
    "            mask[:, self.num_mem_tokens:-self.num_mem_tokens] = attention_mask\n",
    "            return mask\n",
    "    \n",
    "    def process_output(self, model_outputs, **kwargs):\n",
    "        if self.num_mem_tokens not in {0, None}:\n",
    "            out = CausalLMOutputWithCrossAttentions()\n",
    "            memory_state = model_outputs.hidden_states[-1][:, -self.num_mem_tokens:]\n",
    "            out['logits'] = model_outputs.logits[:, self.num_mem_tokens:-self.num_mem_tokens]\n",
    "            \n",
    "            if kwargs.get('output_hidden_states'):\n",
    "                out['hidden_states'] = [lh[:, self.num_mem_tokens:-self.num_mem_tokens] for lh in model_outputs.hidden_states]\n",
    "            if kwargs.get('output_attentions'):\n",
    "                out['attentions'] = model_outputs['attentions']\n",
    "        else:\n",
    "            memory_state = None\n",
    "            out = model_outputs\n",
    "            \n",
    "        return out, memory_state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Receiving Party shall not disclose the fact that Agreement was agreed or negotiated. Johns Hopkins University\\nNON-DISCLOSURE AGREEMENT For Bilateral Disclosure\\nThis Agreement is effective this of in the year ______ is by and between JHU and COMPANY, each defined below.\\nJHU: The Johns Hopkins University\\nAddress: 100 N. Charles St., 5th Floor\\nBaltimore, Maryland 21201\\nJHU Contact:\\nCOMPANY:\\nAddress:\\nCOMPANY Contact:\\nWHEREAS, each party has certain technical information described below which shall hereinafter be referred to as \"CONFIDENTIAL INFORMATION\";\\nCONFIDENTIAL INFORMATION:\\nWHEREAS, JHU and COMPANY are each interested in examining the CONFIDENTIAL INFORMATION of the other solely for the PURPOSE, defined below;\\nPURPOSE: To explore licensing, collaborative or sponsored research agreement opportunities related to the CONFIDENTIAL INFORMATION.\\nNOW, THEREFORE, in consideration of the premises and mutual covenants contained herein, the parties hereto agree as follows:\\n‚ÄùPROVIDER‚Äù shall mean the party hereto disclosing CONFIDENTIAL INFORMATION to the RECIPIENT party.\\n‚ÄúRECIPIENT‚Äù shall mean the party receiving CONFIDENTIAL INFORMATION from the PROVIDER party.\\n 1. PROVIDER, through its employee, the PROVIDER Contact, shall disclose CONFIDENTIAL INFORMATION to RECIPIENT, through its employee, the RECIPIENT Contact, to enable RECIPIENT to fully evaluate such disclosure solely for the PURPOSE. CONFIDENTIAL INFORMATION shall be indicated as confidential at the time of disclosure.\\n2. RECIPIENT agrees to accept the CONFIDENTIAL INFORMATION and to employ all reasonable efforts to maintain the CONFIDENTIAL INFORMATION as secret and confidential, such efforts to be no less than the degree of care employed by RECIPIENT to preserve and safeguard RECIPIENT\\'s own confidential information. The CONFIDENTIAL INFORMATION shall not be disclosed or revealed to anyone except employees of RECIPIENT who have a need to know the CONFIDENTIAL INFORMATION for the PURPOSE and who agree to be bound by the terms of this Agreement.\\n3. It is hereby acknowledged by PROVIDER that RECIPIENT shall incur no liability merely for examining and considering the CONFIDENTIAL INFORMATION. However, RECIPIENT agrees that it will not use the CONFIDENTIAL INFORMATION for any purpose other<|endoftext|>'"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(gen_inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = tokenizer.batch_decode([l[m] for l, m in zip(batch['labels'], batch['labels_mask'])], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Not mentioned', 'Entailment']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(pred, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NotMentioned', 'Contradiction']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer.batch_decode([l[m] for l, m in zip(batch['labels'], batch['labels_mask'])], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(62.5257, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(15):\n",
    "    out = model(**batch)\n",
    "    out.loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.8596, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = out.logits.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_text = tokenizer.batch_decode(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "generated = model.memory_cell.model.generate(batch['labels'][:, -5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([50257,  3673,    44,  1463,   276, 50256])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'][0][batch['labels_mask'][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([50257,  3673,    44,  1463,   276, 50256])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['labels'][0][batch['labels_mask'][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb Cell 34\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb#Y310sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m gen \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mmemory_cell\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(gen_inputs)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:26\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     25\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m():\n\u001b[0;32m---> 26\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/transformers/generation_utils.py:1084\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[39m# 2. Define model inputs\u001b[39;00m\n\u001b[1;32m   1079\u001b[0m \u001b[39m# inputs_tensor has to be defined\u001b[39;00m\n\u001b[1;32m   1080\u001b[0m \u001b[39m# model_input_name is defined if model-specific keyword input is passed\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m \u001b[39m# otherwise model_input_name is None\u001b[39;00m\n\u001b[1;32m   1082\u001b[0m \u001b[39m# all model-specific keyword inputs are removed from `model_kwargs`\u001b[39;00m\n\u001b[1;32m   1083\u001b[0m inputs_tensor, model_input_name, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_model_inputs(inputs, bos_token_id, model_kwargs)\n\u001b[0;32m-> 1084\u001b[0m batch_size \u001b[39m=\u001b[39m inputs_tensor\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1086\u001b[0m \u001b[39m# 3. Define other model kwargs\u001b[39;00m\n\u001b[1;32m   1087\u001b[0m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39moutput_attentions\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m output_attentions\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def keep_for_metrics_fn(batch, output):\n",
    "    data = {}\n",
    "    if isinstance(output, torch.Tensor):\n",
    "        data['labels'] = batch['labels']\n",
    "        \n",
    "    for key in batch.keys():\n",
    "        if 'loss' in key: \n",
    "            data[key] = batch[key]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[GEN]NotMentioned[GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN]',\n",
       " ' in[GEN]Contradiction[GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN]']"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(generated, add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  def keep_for_metrics_fn(batch, output):\n",
    "data = {}\n",
    "# if 'generation_outputs' in output:\n",
    "if isinstance(output, dict):\n",
    "    data['predictions'] = torch.argmax(output['logits'].detach(), dim=-1)\n",
    "    if 'labels_mask' in batch:\n",
    "        data['predictions'] = [data['predictions'][i, mask] for i, mask in enumerate(batch['labels_mask'])]\n",
    "else:\n",
    "    data['generation_outputs'] = output\n",
    "    \n",
    "data['labels'] = batch['labels']\n",
    "for key in batch.keys():\n",
    "    if 'loss' in key: \n",
    "        data[key] = batch[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = CausalLMOutputWithCrossAttentions()\n",
    "# full_logits = torch.cat([o.logits for o in cell_outputs], dim=1)\n",
    "# full_hidden_states = tuple([torch.cat(layer_hs, dim=1) for layer_hs in zip(*[o.hidden_states for o in cell_outputs])])\n",
    "\n",
    "full_logits = out.logits\n",
    "labels = kwargs.get('labels')\n",
    "if labels is not None:\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "    shift_logits = full_logits[..., :-1, :].contiguous()\n",
    "    flat_labels = shift_labels.view(-1)\n",
    "    flat_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "    \n",
    "    loss_fct = CrossEntropyLoss()\n",
    "    labels_mask = kwargs.get('labels_mask')\n",
    "    if labels_mask is not None:\n",
    "        shift_mask = labels_mask[..., :-1].contiguous()\n",
    "\n",
    "        flat_labels = flat_labels[shift_mask.view(-1)]\n",
    "        flat_logits = flat_logits[shift_mask.view(-1)]\n",
    "        \n",
    "    out['loss'] = loss_fct(flat_logits, flat_labels)\n",
    "\n",
    "out['logits'] = full_logits\n",
    "segment_keys = ['loss', 'logits']\n",
    "if kwargs.get('output_attentions'):\n",
    "    segment_keys.append('attentions')\n",
    "if kwargs.get('output_hidden_states'):\n",
    "    segment_keys.append('hidden_states')\n",
    "    out['hidden_states'] = full_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_sampler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb#Y221sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m### booydar\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb#Y221sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m batch_metrics_fn \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m _, y: {key: y[key] \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m y\u001b[39m.\u001b[39mkeys() \u001b[39mif\u001b[39;00m ((\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m key) \u001b[39mor\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39m!log\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m key))}\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb#Y221sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(args, model, optimizer, train_dataloader, valid_dataloader, train_sampler,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb#Y221sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m                     keep_for_metrics_fn\u001b[39m=\u001b[39mkeep_for_metrics_fn, metrics_fn\u001b[39m=\u001b[39mmetrics_fn,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb#Y221sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m                     \u001b[39m###booydar\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb#Y221sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m                     batch_metrics_fn\u001b[39m=\u001b[39mbatch_metrics_fn,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb#Y221sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m                     generate_kwargs\u001b[39m=\u001b[39m{})\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb#Y221sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m args\u001b[39m.\u001b[39mvalidate_only:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb#Y221sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# train loop\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu8/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_cnli_new_run.ipynb#Y221sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     trainer\u001b[39m.\u001b[39mtrain()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_sampler' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "### booydar\n",
    "batch_metrics_fn = lambda _, y: {key: y[key] for key in y.keys() if (('loss' in key) or ('!log' in key))}\n",
    "trainer = Trainer(args, model, optimizer, train_dataloader, valid_dataloader, train_sampler,\n",
    "                    keep_for_metrics_fn=keep_for_metrics_fn, metrics_fn=metrics_fn,\n",
    "                    ###booydar\n",
    "                    batch_metrics_fn=batch_metrics_fn,\n",
    "                    generate_kwargs={})\n",
    "\n",
    "if not args.validate_only:\n",
    "    # train loop\n",
    "    trainer.train()\n",
    "    # make sure all workers are done\n",
    "    hvd.barrier()\n",
    "    # run validation after training\n",
    "    if args.save_best:\n",
    "        best_model_path = str(Path(args.model_path) / 'model_best.pth')\n",
    "        if hvd.rank() == 0:\n",
    "            logger.info(f'Loading best saved model from {best_model_path}')\n",
    "        trainer.load(best_model_path)\n",
    "    if valid_dataloader is not None:\n",
    "        if hvd.rank() == 0:\n",
    "            logger.info('Runnning validation on valid data:')\n",
    "        trainer.validate(valid_dataloader, write_tb=False)\n",
    "else:\n",
    "    # run validation, do not write to tensorboard\n",
    "    if hvd.rank() == 0:\n",
    "        logger.info('Running validation on train set:')\n",
    "    trainer.validate(train_dataloader, split='train', write_tb=True)\n",
    "    if valid_dataloader is not None:\n",
    "        if hvd.rank() == 0:\n",
    "            logger.info('Running validation on valid data:')\n",
    "        trainer.validate(valid_dataloader, write_tb=True)\n",
    "    # if test_dataloader is not None:\n",
    "    #     if hvd.rank() == 0:\n",
    "    #         logger.info('Runnning validation on test data:')\n",
    "    #     trainer.validate(test_dataloader, write_tb=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Holder:\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq_len = 512\n",
    "target_seq_len = 512\n",
    "\n",
    "num_mem_tokens = 2\n",
    "input_size = 128\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "args = Holder\n",
    "args.target_seq_len = target_seq_len\n",
    "args.input_seq_len = input_seq_len\n",
    "args.num_mem_tokens = num_mem_tokens\n",
    "args.input_size = input_size\n",
    "args.input_prefix = ''\n",
    "args.block_size = None\n",
    "args.task_name = 'wikitext-2-v1'\n",
    "\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/cryptography/hazmat/backends/openssl/x509.py:14: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = 'gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/home/bulatov/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1360fde036a34b9292dfbe062f075a5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/bulatov/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-389b922bfc5fe729.arrow\n",
      "Loading cached processed dataset at /home/bulatov/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-6067a66e735cfbb1.arrow\n",
      "Loading cached processed dataset at /home/bulatov/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-941845a5470f2db7.arrow\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "raw_datasets = datasets.load_dataset('wikitext', args.task_name)\n",
    "column_names = raw_datasets[\"train\"].column_names\n",
    "text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[text_column_name])\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=column_names,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "block_size = args.input_size \n",
    "if args.num_mem_tokens is not None:\n",
    "    block_size -= 2 * args.num_mem_tokens\n",
    "history_size = args.input_seq_len - block_size\n",
    "\n",
    "def group_texts(examples, block_size, history_size=None):\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "\n",
    "    if history_size is None:\n",
    "        result = {\n",
    "            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "    else:\n",
    "        result = {\n",
    "            k: [t[max({0, i - history_size}) : i + block_size] for i in range(0, total_length, block_size)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_pad_value = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124, 388)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size, history_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/bulatov/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-1504f9373e317eca.arrow\n",
      "Loading cached processed dataset at /home/bulatov/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-c6da793e710ea6d8.arrow\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     input_ids = [torch.tensor(b['input_ids'][::-1]) for b in batch]\n",
    "#     labels = [torch.tensor(b['labels'][::-1]) for b in batch]\n",
    "#     attention_mask = [torch.tensor(b['attention_mask'][::-1]) for b in batch]\n",
    "#     input_ids = pad_sequence(input_ids, padding_value=id_pad_value).T.flip(1)\n",
    "#     labels = pad_sequence(labels, padding_value=-100).T.flip(1)\n",
    "#     attention_mask = pad_sequence(attention_mask, padding_value=0).T.flip(1)\n",
    "\n",
    "#     collated = {'input_ids': input_ids,\n",
    "#                 'labels': labels, \n",
    "#                 'attention_mask': attention_mask}\n",
    "\n",
    "#     if input_ids.shape[1] != block_size:\n",
    "#         labels_mask = torch.ones_like(input_ids, dtype=bool)\n",
    "#         labels_mask[:, :-block_size] = False\n",
    "#         collated['labels_mask'] = labels_mask\n",
    "\n",
    "#     return collated\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [torch.tensor(b['input_ids']) for b in batch]\n",
    "    labels = [torch.tensor(b['labels']) for b in batch]\n",
    "    labels_mask = [torch.ones_like(l, dtype=bool) for l in labels]\n",
    "    attention_mask = [torch.tensor(b['attention_mask']) for b in batch]\n",
    "\n",
    "    input_ids = pad_sequence(input_ids, padding_value=id_pad_value).T\n",
    "    labels = pad_sequence(labels, padding_value=-100).T\n",
    "    labels_mask = pad_sequence(labels_mask, padding_value=False).T\n",
    "    attention_mask = pad_sequence(attention_mask, padding_value=0).T\n",
    "\n",
    "    collated = {'input_ids': input_ids,\n",
    "                'labels': labels, \n",
    "                'labels_mask': labels_mask,\n",
    "                'attention_mask': attention_mask}\n",
    "\n",
    "    # if args.vary_n_segments:\n",
    "    #     n_segments = np.random.randint(1, args.max_n_segments + 1)\n",
    "    #     n_tokens = n_segments * block_size\n",
    "    #     for k in collated:\n",
    "    #         collated[k] = collated[k][:, -n_tokens:]\n",
    "\n",
    "    return collated\n",
    "\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"].map(lambda x: group_texts(x, block_size, history_size), \n",
    "                                        batched=True, desc=f\"Grouping train in chunks of {block_size} and history {history_size}\")\n",
    "valid_dataset = tokenized_datasets[\"validation\"].map(lambda x: group_texts(x, block_size), \n",
    "                                        batched=True, desc=f\"Grouping valid in chunks of {block_size}\")\n",
    "\n",
    "\n",
    "# shuffle train data each epoch (one loop over train_dataset)\n",
    "# train_sampler = DistributedStrain_dataset[i] for i in range(4)ampler(train_dataset, rank=hvd.rank(), num_replicas=hvd.size(), shuffle=True,\n",
    "#                                     drop_last=False, seed=args.seed)\n",
    "# per_worker_batch_size = args.batch_size * args.gradient_accumulation_steps\n",
    "# global_batch_size = per_worker_batch_size * hvd.size()\n",
    "\n",
    "# train_sampler = RandomSampler(train_dataset)\n",
    "kwargs = {'pin_memory': True}#, 'num_workers': args.data_n_workers}\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [train_dataset[i] for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in b[0]:\n",
    "    b[0][k] = b[0][k][:124]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = collate_fn(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = iter(train_dataloader)\n",
    "batch = next(gen)\n",
    "batch = next(gen)\n",
    "batch = next(gen)\n",
    "batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_datasets['train'][1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions\n",
    "\n",
    "class MemoryCell(torch.nn.Module):\n",
    "    def __init__(self, base_model, num_mem_tokens):\n",
    "        super().__init__()\n",
    "        self.model = base_model\n",
    "        self.create_memory(num_mem_tokens)\n",
    "\n",
    "    def create_memory(self, num_mem_tokens):\n",
    "        self.num_mem_tokens = num_mem_tokens\n",
    "        embeddings = self.model.get_input_embeddings()\n",
    "        memory_weights = torch.randn((num_mem_tokens, self.model.config.n_embd)) * embeddings.weight.data.std()\n",
    "        self.register_parameter('memory', torch.nn.Parameter(memory_weights, requires_grad=True))\n",
    "\n",
    "        self.read_memory_position = range(num_mem_tokens)\n",
    "        self.write_memory_position = range(-num_mem_tokens, 0)\n",
    "\n",
    "    def set_memory(self, input_shape):\n",
    "        memory = self.memory.repeat(input_shape[0], 1, 1)\n",
    "        return memory\n",
    "\n",
    "    def forward(self, input_ids, memory_state=None, **kwargs):\n",
    "        if memory_state is None:\n",
    "            memory_state = self.set_memory(input_ids.shape)\n",
    "\n",
    "        seg_kwargs = self.process_input(input_ids, memory_state, **kwargs)\n",
    "        out = self.model(**seg_kwargs)\n",
    "        out, new_memory_state = self.process_output(out, **kwargs)\n",
    "\n",
    "        return out, new_memory_state\n",
    "    \n",
    "    def process_input(self, input_ids, memory_state, **kwargs):\n",
    "        seg_kwargs = dict(**kwargs)\n",
    "\n",
    "        inputs_embeds = kwargs.get('inputs_embeds')\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.model.get_input_embeddings()(input_ids)\n",
    "        inputs_embeds = torch.cat([memory_state, inputs_embeds, memory_state], dim=1)\n",
    "\n",
    "        seg_kwargs['input_ids'] = None\n",
    "        seg_kwargs['inputs_embeds'] = inputs_embeds\n",
    "        if kwargs.get('attention_mask') is not None:\n",
    "            seg_kwargs['attention_mask'] = self.pad_attention_mask(kwargs['attention_mask'], inputs_embeds.shape)\n",
    "        seg_kwargs['output_hidden_states'] = True\n",
    "        return seg_kwargs\n",
    "    \n",
    "    def pad_attention_mask(self, attention_mask, shape):\n",
    "        if self.num_mem_tokens in {0, None}:\n",
    "            return attention_mask\n",
    "        else:\n",
    "            mask = torch.ones(*shape[:2], dtype=torch.int64).to(attention_mask.device)\n",
    "            mask[:, self.num_mem_tokens:-self.num_mem_tokens] = attention_mask\n",
    "            return mask\n",
    "    \n",
    "    def process_output(self, model_outputs, **kwargs):\n",
    "        if self.num_mem_tokens not in {0, None}:\n",
    "            out = CausalLMOutputWithCrossAttentions()\n",
    "            memory_state = model_outputs.hidden_states[-1][:, -self.num_mem_tokens:]\n",
    "            out['logits'] = model_outputs.logits[:, self.num_mem_tokens:-self.num_mem_tokens]\n",
    "            \n",
    "            if kwargs.get('output_hidden_states'):\n",
    "                out['hidden_states'] = [lh[:, self.num_mem_tokens:-self.num_mem_tokens] for lh in model_outputs.hidden_states]\n",
    "            if kwargs.get('output_attentions'):\n",
    "                out['attentions'] = model_outputs['attentions']\n",
    "        else:\n",
    "            memory_state = None\n",
    "            out = model_outputs\n",
    "            \n",
    "        return out, memory_state \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentWrapper(torch.nn.Module):\n",
    "    def __init__(self, memory_cell, **rmt_kwargs):\n",
    "        super().__init__()\n",
    "        self.memory_cell = memory_cell\n",
    "        self.rmt_config = rmt_kwargs\n",
    "\n",
    "    def forward(self, input_ids, labels=None, labels_mask=None, inputs_embeds=None, attention_mask=None, output_attentions=None, output_hidden_states=None):\n",
    "        memory_state = None\n",
    "        segmented = self.segment(input_ids=input_ids, inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
    "\n",
    "        cell_outputs = []\n",
    "        for seg_num, segment in enumerate(segmented):\n",
    "            cell_out, memory_state = self.memory_cell(**segment, memory_state=memory_state, output_hidden_states=True)\n",
    "            cell_outputs.append(cell_out)\n",
    "            self.manage_gradients(memory_state, seg_num)\n",
    "\n",
    "        out = self.process_outputs(cell_outputs, labels=labels, \n",
    "                                   labels_mask=labels_mask,\n",
    "                                   output_attentions=output_attentions, \n",
    "                                   output_hidden_states=output_hidden_states)\n",
    "        return out\n",
    "\n",
    "    def segment(self, **kwargs):\n",
    "        segments = []\n",
    "        for k, tensor in kwargs.items():\n",
    "            if tensor is not None:\n",
    "                k_segments = self.split_tensor(tensor)\n",
    "                for s, k_seg in enumerate(k_segments):\n",
    "                    if s < len(segments):\n",
    "                        segments[s][k] = k_seg\n",
    "                    else:\n",
    "                        segments.append({k: k_seg})\n",
    "\n",
    "        return segments\n",
    "    \n",
    "    def split_tensor(self, tensor):\n",
    "        align = self.rmt_config.get('segment_alignment')\n",
    "        segment_size = self.rmt_config.get('segment_size')\n",
    "        if align in {'left', None}:\n",
    "            split_inds = list(range(0, tensor.shape[1], segment_size)) + [tensor.shape[1]]\n",
    "            segments = [tensor[:, start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "        elif align in {'right', None}:\n",
    "            split_inds = (list(range(tensor.shape[1], 0, -segment_size)) + [0])[::-1]\n",
    "            segments = [tensor[:, start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "        elif align == 'center':\n",
    "            n_seg = math.ceil(tensor.shape[1] / segment_size)\n",
    "            segments = torch.chunk(tensor, n_seg, dim=1)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return segments\n",
    "\n",
    "    def process_outputs(self, cell_outputs, **kwargs):\n",
    "        out = CausalLMOutputWithCrossAttentions()\n",
    "        full_logits = torch.cat([o.logits for o in cell_outputs], dim=1)\n",
    "        full_hidden_states = tuple([torch.cat(layer_hs, dim=1) for layer_hs in zip(*[o.hidden_states for o in cell_outputs])])\n",
    "\n",
    "        labels = kwargs.get('labels')\n",
    "        if labels is not None:\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            shift_logits = full_logits[..., :-1, :].contiguous()\n",
    "            flat_labels = shift_labels.view(-1)\n",
    "            flat_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "            \n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            labels_mask = kwargs.get('labels_mask')\n",
    "            if labels_mask is not None:\n",
    "                shift_mask = labels_mask[..., :-1].contiguous()\n",
    "\n",
    "                flat_labels = flat_labels[shift_mask.view(-1)]\n",
    "                flat_logits = flat_logits[shift_mask.view(-1)]\n",
    "                \n",
    "            out['loss'] = loss_fct(flat_logits, flat_labels)\n",
    "\n",
    "        out['logits'] = full_logits\n",
    "        segment_keys = ['loss', 'logits']\n",
    "        if kwargs.get('output_attentions'):\n",
    "            segment_keys.append('attentions')\n",
    "        if kwargs.get('output_hidden_states'):\n",
    "            segment_keys.append('hidden_states')\n",
    "            out['hidden_states'] = full_hidden_states\n",
    "\n",
    "        # for seg_num, o in enumerate(cell_outputs):\n",
    "        #     for key, value in o.items():\n",
    "        #         if any([sk in key for sk in segment_keys]):\n",
    "        #             out[f'{key}_{seg_num}'] = value\n",
    "\n",
    "        return out \n",
    "        \n",
    "    def manage_gradients(self, memory_state, seg_num):\n",
    "        k2, max_n_segments = self.rmt_config.get('k2'), self.rmt_config.get('max_n_segments')\n",
    "        if seg_num == 0 \\\n",
    "            or k2 in {-1, None} \\\n",
    "            or seg_num + k2 > max_n_segments:\n",
    "                return True\n",
    "        \n",
    "        memory_state = memory_state.detach()\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/cryptography/hazmat/backends/openssl/x509.py:14: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_mem_tokens = 10\n",
    "# device = torch.device(3)\n",
    "device = 'cpu'\n",
    "\n",
    "rmt_config = {'num_mem_tokens': 10, \n",
    "               #  'max_n_segments': 1,\n",
    "               #  'segment_alignment': 'right',\n",
    "               #  'tokenizer': tokenizer,\n",
    "               #  'memory_layers': 'all', \n",
    "               #  'share_memory_layers': True,\n",
    "               #  'reconstruction_loss_coef': 0.1,\n",
    "               #  'k1': -1, 'k2': 3,\n",
    "               #  'segment_ordering': 'regular',\n",
    "               #  'input_size': 1024, \n",
    "               #  'bptt_depth': -1, \n",
    "               #  'sum_loss': False,\n",
    "             }\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "cell = MemoryCell(base_model, num_mem_tokens=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmt = RecurrentWrapper(cell, max_n_segments=5, segment_size=124, segment_alignment='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = iter(train_dataloader)\n",
    "batch = next(gen)\n",
    "batch = next(gen)\n",
    "batch = next(gen)\n",
    "batch.pop('labels_mask')\n",
    "# batch.pop('labels')\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmt_out = rmt(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['loss', 'logits'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmt_out.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contract-NLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "class CNLIDataset(Dataset):\n",
    "    def __init__(self, json_path):\n",
    "        with open(json_path, 'r') as f:\n",
    "            self.samples = json.load(f)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.samples[item]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done   1 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Batch computation too fast (0.0032s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=12)]: Done   8 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Batch computation too fast (0.0160s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=12)]: Done  28 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done  50 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 123 out of 123 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "dataset = CNLIDataset('/cephfs/home/bulatov/bulatov/datasets/contract_nli/contract-nli/test_processed.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.input_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_tokens('[GEN]', special_tokens=True)\n",
    "gen_token = tokenizer.encode('[GEN]')[0]\n",
    "id_pad_value = tokenizer.eos_token_id\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs = [b['context'][:args.input_seq_len * 10] for b in batch]\n",
    "    labels = [b['answer'][:args.input_seq_len * 10] for b in batch]\n",
    "\n",
    "    collated = {}\n",
    "    inputs = tokenizer.batch_encode_plus(list(inputs), padding=False)\n",
    "    labels = tokenizer.batch_encode_plus(list(labels), padding=False)\n",
    "\n",
    "    full_inputs, labels_mask = [], []\n",
    "    for inp, lab in zip(inputs['input_ids'], labels['input_ids']):\n",
    "        inp = inp[:args.input_seq_len - len(lab) - 1]\n",
    "        full_inputs.append(torch.tensor(inp + [gen_token] + lab))\n",
    "        labels_mask.append(torch.tensor([False] * len(inp) + [True] * (len(lab) + 1)))\n",
    "\n",
    "    full_inputs = pad_sequence(full_inputs, padding_value=id_pad_value).T\n",
    "    labels_mask = pad_sequence(labels_mask, padding_value=False).T\n",
    "\n",
    "    collated['input_ids'] = collated['labels'] = full_inputs\n",
    "    collated['labels_mask'] = labels_mask\n",
    "    collated['attention_mask'] = collated['input_ids'] != id_pad_value\n",
    "    return collated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = collate_fn([dataset[i] for i in range(4)])\n",
    "# batch = [dataset[i] for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented = rmt.segment(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([4, 103]),\n",
       " torch.Size([4, 103]),\n",
       " torch.Size([4, 103]),\n",
       " torch.Size([4, 103]),\n",
       " torch.Size([4, 100])]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s['input_ids'].shape for s in segmented]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = batch.pop('labels')\n",
    "rmt_out = rmt(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512, 50258])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmt_out.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 50258])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmt_out.logits[batch['labels_mask']].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[batch['labels_mask']].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NotMentioned<|endoftext|>'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([ 3673,    44,  1463,   276, 50256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Entailment'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([14539,   603,   434])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmt_out = rmt(**batch)\n",
    "\n",
    "self = rmt\n",
    "input_ids = batch['input_ids']\n",
    "\n",
    "memory_state = None\n",
    "segmented = self.segment(input_ids=input_ids, inputs_embeds=None, attention_mask=None)\n",
    "\n",
    "cell_outputs = []\n",
    "for seg_num, segment in enumerate(segmented):\n",
    "    cell_out, memory_state = self.memory_cell(**segment, memory_state=memory_state, output_hidden_states=True)#**batch)\n",
    "    cell_outputs.append(cell_out)\n",
    "    self.manage_gradients(memory_state, seg_num)\n",
    "\n",
    "# out = self.process_outputs(cell_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 248])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1.]), tensor([1., 1., 1., 1.]), tensor([1., 1.]))"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.chunk(torch.ones(10), 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50258, 768)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "# scrolls_metric_path = hf_hub_download(repo_id=\"datasets/tau/scrolls\", filename=\"metrics/scrolls.py\")tokenizer.add_tokens('[GEN]', special_tokens=True)\n",
    "gen_token = tokenizer.encode('[GEN]')[0]\n",
    "\n",
    "rmt.memory_cell.model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# gen_token = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<|endoftext|>',\n",
       " 'eos_token': '<|endoftext|>',\n",
       " 'unk_token': '<|endoftext|>'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_attention_first_token = False  # should be True for LED\n",
    "encode_plus_kwargs = {'truncation': True, 'padding': 'longest', 'pad_to_multiple_of': 1}\n",
    "# generate_kwargs = {'max_length': args.targettokenizer.add_tokens('[GEN]', special_tokens=True)\n",
    "# gen_token = tokenizer.encode('[GEN]')[0]\n",
    "\n",
    "# rmt.memory_cell.model.resize_token_embeddings(len(tokenizer))_seq_len, 'min_length': args.target_seq_len}\n",
    "generate_kwargs = {}\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs = [b['input'][:args.input_seq_len * 10] for b in batch]\n",
    "    labels = [b['output'][:args.input_seq_len * 10] for b in batch]\n",
    "\n",
    "    collated = {}\n",
    "    inputs = tokenizer.batch_encode_plus(list(inputs), padding=False)\n",
    "    labels = tokenizer.batch_encode_plus(list(labels), padding=False)\n",
    "\n",
    "    full_inputs = [torch.tensor(i[:input_size - len(l) - 1] + [gen_token] + l) for i, l in zip(inputs['input_ids'], labels['input_ids'])]\n",
    "    full_inputs = pad_sequence(full_inputs, padding_value=tokenizer.pad_token_id).T\n",
    "    \n",
    "    labels_mask = torch.zeros_like(full_inputs).bool()\n",
    "    for i, l in enumerate(labels['input_ids']):\n",
    "        labels_mask[i, -len(l) -1:] = True\n",
    "    collated['input_ids'] = collated['labels'] = full_inputs\n",
    "    collated['labels_mask'] = labels_mask\n",
    "    collated['attention_mask'] = collated['input_ids'] != tokenizer.pad_token_id\n",
    "\n",
    "    return collated\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset scrolls (/home/bulatov/.cache/huggingface/datasets/tau___scrolls/quality/1.0.0/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bb2ffcdd0604c9bb263db2f85f18d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq2seq_task_name = 'quality'\n",
    "dataset = datasets.load_dataset('tau/scrolls', seq2seq_task_name)\n",
    "train_dataset = dataset['train']\n",
    "# shuffle train data each epoch (one loop over train_dataset)\n",
    "# train_sampler = DistributedSampler(train_dataset, shuffle=True, drop_last=False, seed=args.seed)\n",
    "kwargs = {'pin_memory': True}\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2,\n",
    "                                collate_fn=collate_fn, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = iter(train_dataloader)\n",
    "batch = next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'labels', 'labels_mask', 'attention_mask'])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['labels'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Why is Si retirement so significant to the Space Exploration Team? \\n\\n (A) There aren‚Äôt enough working people in the world. They won‚Äôt be able to find a replacement.\\n (B) As one of two remaining spacemen, it would likely mean the defunding and shut down of the Space Exploration Team.\\n (C) Training new spacemen is costly and time consuming. They won‚Äôt have anyone else ready after him.\\n (D) His retirement may inspire others to[GEN]Training new spacemen is costly and time consuming. They won‚Äôt have anyone else ready after him.',\n",
       " 'What makes Gubelin an outlier in the present day?\\n\\n (A) He is much older than the rest of the population.\\n (B) He refuses new operations that could improve his health.\\n (C) His mind is still active, and he values hard work.\\n (D) He still wears glasses and value objects like the gold watch given to Si.\\n\\n\\nSPACEMAN ON A SPREE\\n\\n\\n\\n\\n   BY MACK REYNOLDS\\n\\n\\n\\n\\n   Illustrated by Nodel\\n\\n\\n\\n\\n[GEN]He still wears glasses and value objects like the gold watch given to Si.']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(batch['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[GEN]Training new spacemen is costly and time consuming. They won‚Äôt have anyone else ready after him.',\n",
       " '[GEN]He still wears glasses and value objects like the gold watch given to Si.']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode([c[i] for c, i in zip(batch['input_ids'], batch['labels_mask'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Why is Si retirement so significant to the Space Exploration Team? \\n\\n (A) There aren‚Äôt enough working people in the world. They won‚Äôt be able to find a replacement.\\n (B) As one of two remaining spacemen, it would likely mean the defunding and shut down of the Space Exploration Team.\\n (C) Training new spacemen is costly and time consuming. They won‚Äôt have anyone else ready after him.\\n (D) His retirement may inspire others toTraining new spacemen is costly and time consuming. They won‚Äôt have anyone else ready after him.',\n",
       " 'What makes Gubelin an outlier in the present day?\\n\\n (A) He is much older than the rest of the population.\\n (B) He refuses new operations that could improve his health.\\n (C) His mind is still active, and he values hard work.\\n (D) He still wears glasses and value objects like the gold watch given to Si.\\n\\n\\nSPACEMAN ON A SPREE\\n\\n\\n\\n\\n   BY MACK REYNOLDS\\n\\n\\n\\n\\n   Illustrated by Nodel\\n\\n\\n\\n\\nHe still wears glasses and value objects like the gold watch given to Si.']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode([c[i] for c, i in zip(batch['input_ids'], batch['attention_mask'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = rmt(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(60.2067, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(61.4377, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hvdenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
